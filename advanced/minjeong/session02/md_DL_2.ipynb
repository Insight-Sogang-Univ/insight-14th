{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bca1a69",
   "metadata": {},
   "source": [
    "# 딥러닝 (2) 사전과제\n",
    "## 과거를 기억하는 신경망 - RNN<br>\n",
    "#### <span style='background-color: #fff5b1'> 1. 순차 데이터란?</span>\n",
    "= 순서에 의미가 있고, 순서가 달라지면 의미가 손상되는 데이터   \n",
    "ex. 음성 및 오디오 데이터 / 자연어 / 생물학적 서열 데이터 / 비디오 데이터 등    \n",
    "\n",
    "**세부 분류**<br>\n",
    "(1) 순차 데이터 = 가장 넓은 범위의 개념    \n",
    "(2) 시간적 순차 데이터 : 순차 데이터 중, 순서가 <u>시간의 흐름</u>을 나타냄   \n",
    "(3) 시계열 데이터 : 시간적 순차 데이터 중, <u>데이터가 일정한 시간 간격으로 기록</u>됨    \n",
    "\n",
    "**정형 데이터와 차이점** <br>\n",
    "- 요소의 순서 의존성 : 데이터 구성 요소의 핵심 자체가 핵심 정보   \n",
    "- 자기 상관성 : 이전 시점의 데이터가 다음 시점 데이터에 영향  \n",
    "=> 과거 정보와 순서의 의미를 학습할 수 있는 신경망 필요함   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813aff87",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 2. RNN의 구조와 원리 </span> \n",
    "\n",
    "**1. RNN (Recurrent Neural Network) 정의**<br>\n",
    "= 순환 신경망   \n",
    "= 순환하는 구조를 가진 인공 신경망   \n",
    "-> 순차 데이터 처리에 특화    \n",
    "(1) 시퀀스 데이터를 입력 받아, 순서 정보를 유지하며 처리   \n",
    "(2) 순환하는 은닉층이 매 시점의 은닉 상태 업데이트   \n",
    "(3) 이전 시점 값을 현재 시점으로 넘김   \n",
    "\n",
    "\n",
    "**2. RNN 구조**<br>\n",
    "$x_t$ = 시간 스텝 t에서의 입력     \n",
    "$h_t$ = t에서의 은닉 상태    \n",
    "    - 은닉 상태: 은닉층을 통과한 결과값으로, 해당 시점까지의 정보를 요약한 메모리 or 문맥  \n",
    "\n",
    "$o_t$ = t에서의 출력    \n",
    "$U$ = $x_t$가 $h_t$에 영향을 주는 가중치 -> 현재 정보 처리    \n",
    "<u>$V$ = $h_{t-1}$이 $h_t$에 영향을 주는 가중치 -> 과거 정보를 다음 시점으로 전달</u>   \n",
    "    - 기존 신경망과의 차이점    \n",
    "\n",
    "$W$ = $h_t$가 $o_t$에 영향을 주는 가중치 -> 계산됨 메모리로 결과 만듦    \n",
    "<br>\n",
    "\n",
    "**정보 흐름**<br>\n",
    "- t-1 시점 : $x_{t-1}$이 들어가, $h_{t-1}$ 업데이트하고, $o_{t-1}$ 출력    \n",
    "- t 시점 : $h_{t-1}$ 이 다음 시점으로 전달      \n",
    "=> 새로운 입력 $x_t$와 $h_{t-1}$을 함께 사용해 $h_t$ 업데이트 후, $o_t$ 계산     \n",
    "\n",
    "\n",
    "**3. RNN의 핵심 원리 - 가중치 공유**<br>\n",
    "- 동일한 가중치를 시퀀스의 다양한 시점에서 반복적으로 적용      \n",
    "=> RNN이 시퀀스 데이터의 길이, 시점 위치에 상관없이 효과적으로 작동     \n",
    "\n",
    "(+)    \n",
    "- 학습 파라미터 수 감소 : 입력 시퀀스의 길이가 아무리 길어져도, 학습할 파라미터 수 일정하게 유지      \n",
    "- 일반화 능력 굿 : 이전 시점까지 정보 + 현재 시점의 입력이 주어지면 메모리를 어떻게 업데이트할 지에 대한 <u>일반적 규칙을 학습</u>      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201506d",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 3. 한계 및 장기 의존성 문제</span> \n",
    "- 전체 시퀀스를 모두 읽은 후 역전파가 이루어지는데, 이때 chain rule에 의해 미분값이 반복적으로 곱해짐     \n",
    "(1) 기울기 소실 : 시퀀스 뒤쪽의 오차가 앞쪽까지 전달되지 않아, 먼 과거의 정보를 학습하지 못함    \n",
    "(2) 기울기 폭주 : 시퀀스 뒤쪽의 오차가 역전파 과정에서 비정상적으로 커짐    \n",
    "(3) 느린 훈련 시간 : 계산 과정이 순차적으로 이루어져, 전체 시퀀스를 병렬적으로 처리할 수 없음    \n",
    "<br>\n",
    "\n",
    "**장기 의존성 문제**<br>\n",
    "= 시퀀스 앞 부분의 중요한 정보를 잊어, 맥락 파악 능력이 저하되는 현상   \n",
    "=> RNN 기억력 한계를 극복해, 더 길고 복잡한 시퀀스 데이터를 안정적으로 다루는 새로운 대안의 필요성 대두"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e8520",
   "metadata": {},
   "source": [
    "## 똑똑하게 기억하고 잊는 법 - LSTM & GPU<br>\n",
    "#### <span style='background-color: #fff5b1'> 1. LSTM</span> \n",
    "- 앞 정보가 뒤까지 충분히 전달되지 못하는 RNN의 기울기 소실 문제 해결    \n",
    "- 기억할 내용과 잊어버릴 내용을 선택해 중요한 정보를 오래 가져감 : Gate로 곱셈을 덧셈으로     \n",
    "- RNN과 달리, 2개의 순환되는 층 사용    \n",
    "    - <u>$c_t$ : 장기 기억</u>   \n",
    "    - $h_t$ : 단기 기억   \n",
    "\n",
    "- Gate로 필요한 정보만 통과   \n",
    "    - Forget gate : 정보를 얼마나 잊어버릴지 결정     \n",
    "    - Input gate : 현재 정보를 얼마나 사용할지 결정   \n",
    "    - Output gate : 다음 층으로 전달할 정보 결정    \n",
    "\n",
    "- Final memory cell : input gate + forget gate => 현재 정보를 얼마나 기억할지 계산    \n",
    "\n",
    "**LSTM의 장단점**<br>\n",
    "(+) 기울기 소실 문제 완화     \n",
    "(-) 구조 복잡하고, RNN보다 학습 파라미터 많음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614e6c7",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 2. GRU</span> \n",
    "- 별도의 메모리 셀 없이 gate 수를 줄여 구조 간소화    \n",
    "\n",
    "**LSTM과의 차이점**<br>\n",
    "(1) forget gate + input gate => update gate    \n",
    "(2) reset gate 사용    \n",
    "(3) gate 수가 2개로 줄어 학습 시간 줄어들지만, 성능은 LSTM과 비슷하거나 더 좋음   \n",
    "<br>\n",
    "\n",
    "**GRU 장단점** <br>\n",
    "(+) 성능과 효율성 사이의 균형 굿   \n",
    "(-) 긴 시퀀스 처리에서 한계 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b2b24",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 3. LSTM vs. GRU</span> \n",
    "\n",
    "**LSTM 적합한 경우**<br>\n",
    "- 긴 시퀀스에서 문맥 이해가 중요할 떄    \n",
    "- 데이터 양이 충분하고, 복잡한 패턴 학습할 떄   \n",
    "- 모델 성능 > 계산 효율 인 경우     \n",
    "- 기계 번역, 언어 모델링, 장기 시계열 예측 등     \n",
    "\n",
    "\n",
    "**GRU 적합한 경우**<br>\n",
    "- 자원이 제한적이거나, 학습 속도가 빨라야 할 떄    \n",
    "- 데이터 양이 적고 과적합 위험이 있을 때   \n",
    "- 실시간 예측이 필요할 때    \n",
    "ex. 음성 인식, 스트리밍 데이터, 실시간 비디오 분석, 짧은 시퀀스 기반의 텍스트 분류 등    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f06a6",
   "metadata": {},
   "source": [
    "## 문장 입력받아 문장 출력 - Seq2Seq<br>\n",
    "#### <span style='background-color: #fff5b1'> 1. Seq2Seq 기본 구조 - 인코더와 디코더</span> \n",
    "\n",
    "= 한 시퀀스를 다른 시퀀스로 변환하는 딥러닝 모델     \n",
    "\n",
    "(1) 아이템 입력 개수와 출력 개수 달라도 됨   \n",
    "(2) 인코더 - 디코더 모델    \n",
    "    - 인코더 : 입력된 시퀀스 읽고 압축해 문맥 정보 준비 -> 컨텍스트 벡터   \n",
    "    - 디코더 : 압축된 정보로 원하는 시퀀스 생성 -> 인코더가 보낸 컨텍스트 벡터는 디코더의 첫 번째 은닉 상태에 사용됨    \n",
    "    - 컨텍스트 벡터 : 인코더 중 마지막 시점의 은닉 상태를 의미하는 float 형의 벡터\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d107f7e7",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 2. 한계 - 병목 현상</span> \n",
    "- 고정된 컨텍스트 벡터에 소스 문장의 정보를 압축하면, 입력 시퀀스의 모든 정보를 담지 못하고 손실됨   \n",
    "=> 병목 현상   "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
