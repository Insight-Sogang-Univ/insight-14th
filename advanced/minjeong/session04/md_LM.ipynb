{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ec7ef0",
   "metadata": {},
   "source": [
    "# 언어 모델 & 생성 모델 사전과제\n",
    "## 언어 모델<br>\n",
    "#### <span style='background-color: #fff5b1'> 1. 언어 모델이란 </span>\n",
    "\n",
    "= 단어 시퀀스에 확률을 할당해, 가장 자연스러운 단어 시퀀스를 찾는 모델    \n",
    "\n",
    ". | 핵심 원리 | 계산 복잡도 | 예시      \n",
    "----- | ----- | ----- | -----     \n",
    "통계 기반 | 문장의 등장 빈도를 기반으로 확률 계산 | 낮음 | N-gram, Perplexity     \n",
    "인공 신경망 기반 | 단어를 벡터로 표현 & 신경망을 이용해 단어 예측 | 높음 | RNN, LSTM, GRU, 트랜스포머    \n",
    "\n",
    "#### <span style='background-color: #fff5b1'> 2. SLM (통계적 언어 모델) </span>\n",
    "\n",
    "**Statistic Language Model**<br>\n",
    "\n",
    "- 분포 가설에 근거      \n",
    "    - 한 단어의 주변에 자주 등장하는 단어들을 보면 해당 단어의 의미 짐작 가능하다는 가설   \n",
    "\n",
    "(1) 조건부 확률 [다음 단어의 예측 확률로 문장의 확률 구하기]     \n",
    "= A가 일어났을 때, B가 일어날 확률    \n",
    "=> $P(B|A) = P(A, B) / P(A)$    \n",
    "<br>\n",
    "- 다음 단어를 순차적으로 예측할 때, 각 부분의 확률을 개별적으로 구해 전체 문장의 최종 확률 계산    \n",
    "=> 이 값이 높을수록 해당 문장은 더 자연스러움   \n",
    "\n",
    "(2) 카운트 기반 [이전 단어로 다음 단어 확률 구하기]       \n",
    "- 이전 단어 시퀀스의 등장 빈도로 다음 단어의 확률 계산   \n",
    "=> 실제 단어의 등장 횟수를 세어 계산    \n",
    "<br>\n",
    "- 직관적이지만, 희소 문제 발생    \n",
    "    - 가진 데이터가 부족해 언어 모델링이 정확하지 않은 문제    \n",
    "    - 통계 기반 모델에서 인공 신경망 기반 모델로 넘어가는 계기    \n",
    "\n",
    "**N-gram**<br>\n",
    "= 연속된 n개의 단어 묶음 (시퀀스)   \n",
    "- 앞 단어 전체를 참고하지 않고, 일부 단어만 참고해 희소 문제 완화    \n",
    "    - 기존: 참고하는 단어가 많을수록 해당 시퀀스를 찾기 어려움 & 표현이 없는 경우에는 확률 계산 불가능   \n",
    "\n",
    "=> 참고하는 단어를 n개로 정하고 확률 계산    \n",
    "\n",
    "- 통계적 언어 모델 한계 일부 개선했지만, 희소 문제와 문맥 파악의 한계 극복하지 못함    \n",
    "\n",
    "**Perplexity(PPL)**<br>\n",
    "= 언어 모델이 특정 문장을 얼마나 혼란스러워하는지 나타내는 수치 (성능을 빠르고 정량적으로 평가하기 위한 지표)     \n",
    "- 문장의 희소성 : 문장이 희소하게 등장할수록 PPL 커짐    \n",
    "- 문장 길이 (단어 개수) : 커질수록 PPL 낮음    \n",
    "\n",
    "=> 통계적 언어 모델은 희소 문제를 가지기 때문에 훈련 데이터에 없는 문장이 나타나면 PPL 수치가 높게 나옴    \n",
    "=> 딥러닝 기반 모델은 단어와 문맥 간 관계를 일반화해 학습하기 때문에, 훈련 데이터에 없는 문장에도 유연하게 대처 가능   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200b593",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 3. 딥러닝 기반 언어모델 </span>\n",
    "\n",
    "**LLM(Large Language Mode)**<br>\n",
    "= 방대한 양의 데이터를 학습해, 인간 언어를 이해, 생성, 요약하는 작업을 수행하도록 설계된 인공지능 모델    \n",
    "\n",
    "(1) SLM (Small Language Model) : 제한적인 텍스트 데이터 학습 -> 가볍고 빠른 실행    \n",
    "(2) NLM (Nueral Language Model) : NLP 작업에 활용되는 언어 모델 -> 기존 통계 기반 언어 모델보다 높은 성능    \n",
    "(3) PLM (Pretrained Language Model) : 대규모 데이터셋으로 미리 학습된 언어 모델    \n",
    "(4) LLM    \n",
    "    - 대규모 학습 : 방대한 데이터 학습 -> 언어 패턴, 단어 간 관계, 문맥적 의미 등 스스로 학습    \n",
    "    - 예측 및 생성 : 학습 패턴 기반으로, 다음에 올 가장 확률 높은 단어를 순서대로 예측해 응답 생성    \n",
    "    - 텍스트 생성, 기계 번역, 질의 응답, 문서 요약, 감정 분석 등    \n",
    "\n",
    "LLM $\\subset$ Language Model $\\subset$ AI\n",
    "\n",
    "\n",
    "**BERT (Bidirectional Encoder Representation from Transformers)**<br>\n",
    "\n",
    "(1) 구조 및 처리 과정<br>\n",
    "- 트랜스포머의 인코더를 쌓아 올림    \n",
    "- input - 임베딩 - 트랜스포머 인코더 12개 / 24개 - output    \n",
    "    - 토큰 임베딩, 세그먼트 임베딩, 포지션 임베딩 합쳐서 하나의 입력 벡터 생성     \n",
    "\n",
    "(2) 사전학습 1 (MLM, Masked Language Model) <br>\n",
    "- 문장의 일부 단어를 가린 후 맞는지 아닌지 학습 => 문맥 이해 및 추론 능력 향상    \n",
    "- input의 15% 정도 단어를 랜덤 마스킹    \n",
    "- 이후 양방향 학습으로 마스크 되지 않은 부분도 학습    \n",
    "\n",
    "(3) 사전학습 2 (NSP, Next Sentence Prediction)<br>\n",
    "- 두 문장이 실제로 이어지는 문장인지 아닌지 학습 => 문장 간 관계 이해도 향상    \n",
    "\n",
    "(4) BERT 이후의 모델<br>\n",
    "- RoBERTa : BERT 사전학습 프로세스를 개선해 성능 향상 (레이어 간 파라미터 공유 & 임베딩 행렬 분해)      \n",
    "- ALBERT : BERT 경량화 버전     \n",
    "\n",
    "\n",
    "**GPT**<br>\n",
    "= Generative Pre-trained Transformer    \n",
    "- OpenAI의 LLM 모델    \n",
    "- BERT와 차이점 : 문장 전체를 보고 빈칸을 맞추지 않고, 앞 단어만 보고 다음 단어 생성 [생성 중심]    \n",
    "<br>\n",
    "\n",
    "**한계**<br>\n",
    "1. hallucination : 사실과 다른 내용을 그럴 듯하게 만들어냄    \n",
    "2. 업데이트 비용 : 새로운 정보 반영을 위해 전체 모델 재학습 필요 & 막대한 계산 자원 필요    \n",
    "\n",
    "**RAG**<br>\n",
    "= Retrieval Augmented Generation    \n",
    "\n",
    "(1) 필요성<br>\n",
    "- 관련 정보를 검색해 먼저 찾고, LLM에 넣어 최종 답변을 생성하는 하나의 파이프라인    \n",
    "\n",
    "(2) 구조<br>\n",
    "- 질의 인코더 : 사용자 질문 이해를 위한 언어 모델 -> 주어진 질문을 벡터 형태로 인코딩    \n",
    "- 지식 검색기 : 인코딩된 질문 바탕으로 외부 지식 베이스에서 관련 정보 검색    \n",
    "- 지식 증강 생성기 : 검색한 지식을 활용해 답변 생성하는 어어 모델   \n",
    "\n",
    "(3) 장점<br>\n",
    "- 풍부한 정보 제공    \n",
    "- 실시간 정보 반영    \n",
    "- 환각 방지    \n",
    "\n",
    "(4) CAG<br>\n",
    "= Credibility Aware Generation     \n",
    "- 검색 문서 중 신뢰성이 낮거나 잘못된 정보가 있을 경우, 답변의 품질과 정확도가 떨어지는 RAG의 문제를 완화    \n",
    "=> 모델이 문서의 신뢰도를 스스로 판단 & 활용하도록 훈련    \n",
    "\n",
    "**LangChain**<br>\n",
    "\n",
    "- LLM + RAG 모델의 파이프라인을 쉽게 구성하도록 돕는 프레임워크    \n",
    "= LLM 앱을 빠르게 조립할 수 이쓴 레고 블럭   \n",
    "\n",
    "**특징**<br>\n",
    "(1) 추상화 : 각종 작업을 간결하게 표현 & 간소화        \n",
    "(2) 표준화 : 비슷한 기능을 가진 요소들을 똑같은 형식인 컴포넌트로 표준화        \n",
    "(3) 체이닝 : 주요 컴포넌트를 쉽게 연결해 LLM 서비스의 로직 쉽게 파악          \n",
    "\n",
    "**한계**<br>\n",
    "- 이전 단계로 돌아가 다시 실행하는 복잡한 로직 구현 어려움    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58818a00",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 4. sLM(small Language Model) </span>\n",
    "\n",
    "**모델 압축**<br>\n",
    "(1) 가지치기 : 중요도가 낮거나, 중복되거나 불필요한 매개변수 제거    \n",
    "(2) 양자화 : 고정밀 데이터를 저정밀 데이터로 변환    \n",
    "(3) 지식 증류 : 사전학습된 교사 모델 내용을 학생 모델로 이전   \n",
    "\n",
    "**장점**<br>\n",
    "- 프라이버시 : 로컬에서 실행    \n",
    "- 비용 절감: 학습 / 추론에 필요한 비용 적음     \n",
    "- 효율성 & 맞춤화    \n",
    "\n",
    "**한계**<br>\n",
    "- 편향 : LLM에 존재하는 편향을 학습해 성능 저하 가능   \n",
    "- 제한된 일반화 : 광범위한 지식 기반이 LLM에 비해 부족   \n",
    "- 환각     \n",
    "- 성능, 용량 한계    \n",
    "\n",
    "**LLM과 sLM 결합 (결합 추론)**<br>\n",
    "- sLM이 먼저 풀어, 각 단계를 평가하고 점수를 매긴 뒤 어려운 부분만 LLM이 돕는 방식    \n",
    "=> LLM 수준의 정확도 확보 & LLM 토큰 사용량 절약   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca8a41",
   "metadata": {},
   "source": [
    "## 생성 모델<br>\n",
    "#### <span style='background-color: #fff5b1'> 1. 분류 모델 vs 생성 모델 </span>\n",
    "- 생성 모델 = 주어진 학습 데이터를 학습해, 학습 데이터의 분포를 따르는 유사한 데이터를 생성하는 모델    \n",
    "=> 입력 데이터의 확률 분포     \n",
    "(1) 명시적 확률밀도 모델 : 학습 데이터 분포 기반   \n",
    "(2) 암시적 확률밀도 모델 : 학습 데이터 분포와 무관하게 생성   \n",
    "\n",
    "#### <span style='background-color: #fff5b1'> 2. AE, VAE </span>\n",
    "\n",
    "**AE (Auto Encoder)**<br>\n",
    "= 입출력이 동일하게 만드는 것을 목적으로 하는 신경망     \n",
    "=> 데이터 복원 / 특성 학습에 많이 이용됨    \n",
    "\n",
    "- 인코더 : 고차원 데이터를 잠재 표현으로 변환 => 차원 축소 목적     \n",
    "- 디코더 : 잠재 표현을 풀어 입력을 재복원해 출력    \n",
    "- 잠재 표현 : 원본 데이터를 저차원으로 압축해 함축된 정보 저장     \n",
    "\n",
    "**VAE(Variational Auto Encoder)**<br>\n",
    "- 데이터를 잠재 공간으로 인코딩한 후, 그 공간에서 다시 데이터를 디코딩해 원본 데이터와 유사한 결과 생성    \n",
    "=> 데이터 생성에 많이 이용됨   \n",
    "\n",
    "- 인코더 : 고차원 입력 데이터를 분포로 변환     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afebb881",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 3. GAN (Generative Adversarial Network) </span>\n",
    "\n",
    "= 생성적 적대 신경망 : 생성자-판별자 신경망이 경쟁하며 훈련을 통해 작업의 정교함을 높이는 신경망 모델    \n",
    "    - 생성자 : 진짜 분포에 가까운 가짜 분포 생성    \n",
    "    - 판별자 : 표본이 가짜/진짜 분포 중 어느 곳에 속하는지 결정    \n",
    "\n",
    "=> 생성자는 설득력 있는 위조품을 생성하게 되고, 판별자의 느력도 함께 향상되며 두 모델 모두 각자의 작업에 전문화      \n",
    "\n",
    "**적용 사례**<br>\n",
    "\n",
    "- 엔비디아 가짜 이미지 생성 : 모델이 아주 서서히 학습하도록 하는 새로운 훈련 방법론 제시        \n",
    "- 워싱턴대 가짜 오바마 연설 영상    \n",
    "- 페이스북 Eye In-painting : 눈 감은 사진에서 뜬 사람의 모습 사진으로 바꿈    \n",
    "\n",
    "**장단점**<br>\n",
    "\n",
    "(+) 진짜 같은 가짜 생성 가능    \n",
    "(-) 불안정한 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ef5b8",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 4. 확산 모델 (Diffusion Model) </span>\n",
    "\n",
    "= 입력 이미지에 노이즈를 여러 단계에 걸쳐 추가하고 제거함으로써, 입력 이미지와 유사한 확률 분포를 가진 결과 이미지 생성하는 모델    \n",
    "\n",
    "**순확산과 역확산**<br>\n",
    "- 순확산 = 데이터에 점진적으로 노이즈 추가하는 과정    \n",
    "    - 원본 데이터에서 시작    \n",
    "    - 여러 번 반복되며 각 단계에서 데이터에 조금씩 노이즈 추가   \n",
    "    - 원본 데이터가 완전한 무작위 노이즈로 변함  \n",
    "\n",
    "- 역확산 = 노이즈 데이터에서 원본 데이터 재구성하는 과정    \n",
    "    - 노이즈 데이터에서 시작    \n",
    "    - 각 단계에서 점진적으로 노이즈 제거    \n",
    "    - 원본 데이터 복원 완료    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
