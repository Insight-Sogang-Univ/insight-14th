{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392c84eb",
   "metadata": {},
   "source": [
    "# 딥러닝 (1) 사전과제\n",
    "## 인공신경망의 시작: 퍼셉트론<br>\n",
    "#### <span style='background-color: #fff5b1'> 1. 인공신경망이란?</span> \n",
    "= 우리 뇌의 신경망을 모방한 모델    \n",
    "(1) 여러 input을 받아, 각각에 가중치 적용    \n",
    "(2) sum(input*가중치)이 일정 기준을 넘으면    \n",
    "(3) output 출력 <br>\n",
    "##### **퍼셉트론**\n",
    "= 초기 형태의 인공신경망  \n",
    "- 입력 -> 가중합 -> 활성화 함수 -> 출력<br>    \n",
    "\n",
    "(1) 가중치 : 입력값이 출력값에 주는 영향력 결정   \n",
    "(2) 활성화 함수 : 신호가 임계값을 넘으면 1, 넘지 못하면 0 출력하므로 활성화 함수로 계단 함수 사용   \n",
    "        - 이후 발전된 신경망은 sigmoid, Tanh, ReLU, leaky ReLU 등 사용   \n",
    "(3) 임계값 : 계단 함수에 사용된 임계치   \n",
    "(4) 편향 : 계단 함수를 구현한 식에서 임계값을 좌변으로 넘기면 편향 표현   \n",
    "=> 임계값과 편향은 표현 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6b3a5",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 2. 단층 퍼셉트론과 그 한계</span> \n",
    "\n",
    "(1) **단층 퍼셉트론**   \n",
    "= 입력 / 출력 두 개의 층으로만 이루어진 퍼셉트론   \n",
    "- AND, NAND, OR 논리 게이트 쉽게 구현 가능    \n",
    "    - 논리 게이트 = 디지털 회로에서 기본 논리 연산을 수행할 때 사용하는 구성 요소 (입력값 2개 & 출력값 1개)    \n",
    "    - AND : 입력값 2개가 모두 1인 경우에만, 1 출력   \n",
    "    - NAND : 입력값 2개가 모두 1인 경우에만, 0 출력   \n",
    "    - OR : 입력값 2개가 모두 0인 경우, 0 출력   \n",
    "\n",
    "(2) **단층 퍼셉트론의 한계**     \n",
    "- 선형 분류 문제만 구현 가능해서, 비선형 영역으로 분리해야 하는 XOR 게이트 구현 불가능   \n",
    "    - 입력값 2개의 값이 다른 경우에만, 1 출력   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5faca26",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 다층 퍼셉트론</span> \n",
    "- 두 개 이상의 은닉층   \n",
    "    - 딥러닝 시초 모델로 발전\n",
    "    - 심층 신경망 = 은닉층이 2개 이상인 신경망    \n",
    "    - 딥러닝 = 심층 신경망이 가중치를 스스로 찾도록 학습시킴   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1d1ee",
   "metadata": {},
   "source": [
    "## 딥러닝 모델의 학습 방법<br>\n",
    "#### <span style='background-color: #fff5b1'> 1. 순전파와 역전파</span> \n",
    "(1) 순전파 : 입력값으로부터 예측 수행     \n",
    "(2) 오차 계산 : 예측 정확도 측정   \n",
    "(3) 역전파 : 출력층에서 입력층 방향으로 오차 전달   \n",
    "(4) 가중치 업데이트   \n",
    "(5) 위의 과정 반복하며 오차값 0에 가까워지도록   \n",
    "<br>\n",
    "\n",
    "**연쇄 법칙**    \n",
    "= 서로 얽힌 변수 간 상관관계 계산 방법   \n",
    "= 각 은닉층마다 손실에 주는 영향을 곱해, 최종 손실에 대한 값을 나타낸 것   \n",
    "- 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있음   \n",
    "- 연쇄 법칙으로 구하는 기울기 = 손실 함수 줄이기 위해 가중치를 어떻게 바꿀지 알려주는 신호"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9902d6",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 2. 손실 함수</span> \n",
    "= 오차값과 실제 값을 수치로 표현하는 함수   \n",
    "\n",
    "**손실 함수 종류**     \n",
    "(1) 평균 제곱 오차 (MSE)    \n",
    "- 예측 값이 연속적 & 예측값과 실제값의 차이를 직접적으로 측정하는 회귀 문제에 적합   \n",
    "\n",
    "(2) 이진 크로스 엔트로피 (BCE)     \n",
    "- 낮은 확률로 예측해서 맞추거나, 높은 확률로 예측해서 틀리는 경우 loss가 더 큼    \n",
    "- 이진 분류 문제에서 사용 (로지스틱 회귀)\n",
    "    - 실제 정답이 1일 때, 예측 확률이 0에 가까우면 모델이 정답에 확신 없음   \n",
    "    - 실제 정답이 0일 때, 1에 가까우면 확신을 가지고 틀린 예측    \n",
    "\n",
    "(3) 크로스 엔트로피 오차 (CEE)    \n",
    "- 다중 클래스 분류 문제에서, 예측 확률과 실제 클래스의 일치 정도를 평가하는 데 적합   \n",
    "    - 두 확률 분포의 유사도 측정    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f62859",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 3. 활성화 함수</span> \n",
    "- 출력값을 다음 단계에서 쓸 수 있는 형태로 바꾸는 역할    \n",
    "    - 비선형성을 만들고, 이를 통해 복잡한 패턴 학습 가능   \n",
    "\n",
    "(1) 계단 함수    \n",
    "= 입력값과 가중치의 곱을 모두 더한 뒤, 기준을 넘으면 1 출력    \n",
    "    - 단층 퍼셉트론에서 사용    \n",
    "    - 입력 신호 = 입력값*가중치   \n",
    "    - 출력 신호 = 1 or 0     \n",
    "\n",
    "(2) 시그모이드    \n",
    "= 입력 신호를 0과 1 사이로 변환     \n",
    "- 계단 함수와 달리, x값에 따라 y값이 연속적으로 변화       \n",
    "\n",
    "(3) ReLU      \n",
    "= 입력 > 0이면 입력값 그대로 출력하고, 0 이하면 0 출력하는 비선형 함수     \n",
    "- 계산 간단 & 효율적    \n",
    "- 비선형성 제공     \n",
    "- 같은 신경망에서도 역전파 전달 굿 = 기울기 소실 문제 발생 X       \n",
    "\n",
    "(4) Tanh    \n",
    "= 시그모이드 함수 변형 (출력값이 -1~1)    \n",
    "\n",
    "(5) 소프트맥스    \n",
    "- 다중 클래스 분류 문제의 활성화 함수로 사용    \n",
    "- 출력값을 확률로 변환해 개별 클래스의 확률 알 수 있음    \n",
    "- 분류 문제에서 손실 함수와 연결 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa7833",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 4. 경사하강법과 옵티마이저</span> \n",
    "\n",
    "- 손실 함수 최소화해 모델 학습시키는 방법 = 오차를 가장 작게 만드는 가중치와 편향 찾기     \n",
    "\n",
    "**배치**     \n",
    "= 학습 과정에서 매개변수 조정에 사용하는 데이터의 묶음    \n",
    "- 배치 크기 = 하이퍼파라미터    \n",
    "    - 너무 크면, 학습이 느리고 메모리 부족    \n",
    "    - 너무 작으면, 가중치 업데이트가 잦아 학습 불안정   \n",
    "\n",
    "**이포크**    \n",
    "= 모든 데이터셋 학습 횟수     \n",
    "\n",
    "**이터레이션 / 스텝**    \n",
    "= 한 번의 에포크 끝내기 위해 필요한 배치 수    \n",
    "<br>\n",
    "\n",
    "**(1) 배치 경사 하강법**     \n",
    "- 전체 데이터가 한 묶음     \n",
    "    - 업데이트 횟수가 적음    \n",
    "    - 메모리 많이 필요함     \n",
    "    - 수렴이 안정적   \n",
    "\n",
    "**(2) 배치 크기가 1인 확률적 경사 하강법**   \n",
    "- 전체 데이터를 계산해 시간이 오래 걸리는 기존의 경사 하강법과 달리, 매개변수 값을 조정할 때 랜덤으로 선택한 하나의 데이터에 대해서만 계산 => 빠른 계산     \n",
    "    - 자원이 적은 컴퓨터에서도 쉽게 사용 가능   \n",
    "    - 매개변수의 변경폭 불안정하고, 정확도 낮을 수 있음    \n",
    "\n",
    "**(3) 미니 배치 경사 하강법**     \n",
    "= 배치 크기를 적절히 지정해, 해당 데이터 갯수만큼 매개변수 값 조정     \n",
    "    - (1)에 비해 빠르고, (2)보다 안정적   \n",
    "    - 배치 크기는 일반적으로 2^n 에 해당하는 숫자로 선택   \n",
    "\n",
    "<br>\n",
    "\n",
    "**옵티마이저**  \n",
    "- 경사 하강법을 효율적 & 안정적으로 수행    \n",
    "\n",
    "(1) 모멘텀     \n",
    "- 경사 하강법에서 계산한 접선 기울기에 바로 한 시점 전의 접선 기울기 값을 일정 비율만큼 반영    \n",
    "\n",
    "(2) RMSProp        \n",
    "- 학습률을 조정해, 안정적이고 빠르게 학습하도록 함    \n",
    "    - 최근 기울기를 참고해, 급한 경사에서 속도를 줄임    \n",
    "    - 완만한 경사에서 속도를 유지 or 증가    \n",
    "\n",
    "\n",
    "(3) 아담       \n",
    "= 모멘텀 + RMSProp     \n",
    "    - 방향과 학습률 모두 잡기 위한 방법      \n",
    "    - 각 파라미터의 1차 모멘트와 2차 모멘트를 사용해 학습률 조정   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ebef6",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 5. 데이터 증강과 전이 학습</span> \n",
    "\n",
    "**데이터 증강 장점**    \n",
    "(1) 모델 성능 향상 : 다양한 변형 데이터 학습해, 일반화된 패턴을 모델이 익힘    \n",
    "(2) 데이터 의존성 감소 : 데이터 증강으로, 과적합 완화 & 다양한 패턴 학습 가능    \n",
    "(3) 과적합 완화 : 증강된 데이터로 약간의 변형을 거친 데이터를 추가적으로 학습해, 다양한 입력값을 가짐       \n",
    "(4) 데이터 프라이버시 보호 : 원본 데이터 노출 최소화 & 성능 향상       \n",
    "\n",
    "<br>\n",
    "\n",
    "**전이 학습**    \n",
    "= 한 문제에서 학습한 지식을 다른 관련된 문제에 활용해 모델 성능 강화     \n",
    "(+) 계산 비용 절감, 데이터셋 크기 문제 완화, 일반화 가능성 향상, 성능 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12abda4",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c633eb",
   "metadata": {},
   "source": [
    "## 합성곱 신경망(CNN, Convolutional NN)<br>\n",
    "#### <span style='background-color: #fff5b1'> CNN 개념 설명</span> \n",
    "= 이미지의 공간 정보를 유지한 채로 학습하는 방식   \n",
    "- 특징 추출 영역 + 이미지 분류 영역     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e31b02",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 1. 이미지 처리에 CNN 사용하는 이유 </span> \n",
    "- 정형 데ㅔ이터와 다른 구조를 가지기 때문에, 이미지의 구조를 유지해야 함    \n",
    "    - 모델이 Convolution으로 특징을 추출해 학습하고, Pooling으로 차원 축소해 변화에 덜 민감하게 만듦    \n",
    "\n",
    "- 파라미터 폭발 : 이미지의 특정 피처에만 집중해 파라미터 효율적으로 사용        \n",
    "- 공간 정보 손실 : 특징을 어느 정도 추출해 flatten하기 때문에 손실 정도 낮음       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b0765",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 2. 합성곱과 풀링</span> \n",
    "- 이미지 : 공간적 관계 & 지역적 패턴이 중요    \n",
    "<br>\n",
    "\n",
    "**CNN 모델 구조**    \n",
    "1. 특징 추출 -> Convolution, Pooling    \n",
    "2. 클래스 분류 -> Fully Connected Layer   \n",
    "\n",
    "<br>\n",
    "\n",
    "**Convolution Layer**    \n",
    "= 전체 이미지를 일정 크기 필터로 훑으며 이미지 특징 학습   \n",
    "- 필터 / 커널이 이미지 위를 슬라이딩하며, 각 위치에서 요소별 곱셈 후 합계 계산하여 feature map에 저장   \n",
    "\n",
    "**Padding**    \n",
    "= 필터를 거치며 정보 손실 방지를 위해 이미지 바깥을 0으로 둘러쌈    \n",
    "\n",
    "**Stride**     \n",
    "= 이미지를 몇 칸씩 점프하며 훑을지 결정하는 값    \n",
    "- 계산량 감소 & 효율적 요약 가능   \n",
    "- stride가 클수록 피처 맵의 크기가 작아짐   \n",
    "\n",
    "**Pooling**     \n",
    "= 피처맵에서 특징 추출을 위해 이미지를 격자 형태로 분할한 뒤, 해당 영역 내의 가장 큰 값 (Max Pooling) / 평균값(Average Pooling)을 취함    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1bdc87",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 3. 대표적인 CNN 아키텍처</span> \n",
    "**AlexNet**    \n",
    "= 머신러닝을 벗어나, 최초로 GPU를 이용해 학습한 대규모 CNN 모델   \n",
    "- ReLU 함수로 기울기 소실 문제 해소    \n",
    "- 드롭아웃으로 과적합 방지    \n",
    "\n",
    "**VGGNet**    \n",
    "- 이미지 인식에 강한 깊은 층의 CNN 모델   \n",
    "- Convolution + max pooling 여러 번 반복   \n",
    "- 필터 크기 3*3 고정    \n",
    "- 전이 학습에 많이 사용하며, 초반 연산량 매우 많음     \n",
    "\n",
    "**ResNet**     \n",
    "= 잔차 연결 개념을 도입해 깊은 신경망의 소실 문제를 해결한 모델    \n",
    "- 잔차 연결 : 이전 레이어 출력을 몇 레이어 건너뛰어, 다음 레이어 입력으로 직접 추가    \n",
    "- 입출력 차원 맞춰야 함   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b62bae3",
   "metadata": {},
   "source": [
    "## 이미지 딥러닝 응용<br>\n",
    "#### <span style='background-color: #fff5b1'>1. 이미지 분류 </span> \n",
    "- 이미지를 픽셀 값 = 수치 데이터로 인식    \n",
    "- AlexNet, VGG, ResNet, VIT(Vision Transformer)   \n",
    "\n",
    "<br>\n",
    "\n",
    "#### <span style='background-color: #fff5b1'>2. 객체 탐지 </span> \n",
    "= 한 이미지에서 객채와 그를 둘러싸는 가장 작은 직사각형으로 정의되는 bounding box를 찾는 작업   \n",
    "- CNN로 피처맵을 얻고, 그 위에 Classification head (객체 클래스 예측), Regression head (객체 위치 좌표 예측)를 올림    \n",
    "- YOLO, Faster R-CNN, SSD 등   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf71ba",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'>3. 이미지 캡셔닝 </span> \n",
    "= 이미지 내의 객체에 대한 판단과 객체들 간 관계 파악 & 자연어 형태로 표현    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
