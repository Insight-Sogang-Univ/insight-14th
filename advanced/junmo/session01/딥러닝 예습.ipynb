{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c58dfc",
   "metadata": {},
   "source": [
    "목표 \n",
    "- 딥러닝의 기본 구조와 학습 원리를 이해한다.\n",
    "- 퍼셉트론에서 CNN으로 발전하는 과정을 통해 이미지 인식의 핵심 개념을 익힌다.\n",
    "\n",
    "***다음 내용이 필수적으로 들어가야 합니다.***\n",
    "\n",
    "- **단층 퍼셉트론/다층 퍼셉트론**\n",
    "- **딥러닝 모델의 학습 방법**\n",
    "    - 순전파와 역전파\n",
    "    - 손실함수\n",
    "    - 활성화함수\n",
    "    - 옵티마이저\n",
    "    - 증강과 전이학습\n",
    "- **CNN의 구조**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1777b",
   "metadata": {},
   "source": [
    "\n",
    "# 퍼셉트론\n",
    "\n",
    " \t- 인공신경망이라고도 하며, 인간의 뉴런을 본 따 만들었다. 전체 모습을 크게 4단계로 나눈다.\n",
    "\n",
    "\t1) 입력정보를 받는 부분들\n",
    "\n",
    "\t2) 입력정보들에 가중치를 곱해 총합을 구하는 가중합 단계\n",
    "\n",
    "\t3) 가중합 결과가 일정 기준을 넘으면 출력이 나오도록 만들어주는 활성화 함수 단계\n",
    "\n",
    "\t4) 출력정보가 나오는 단계\n",
    "\n",
    "\t- 위의 4단계가 거쳐 원하는 출력값이 나오도록 가중치를 조정해 나가는것을 ‘훈련’이라고 한다. \n",
    "\n",
    "\t- 단층 퍼셉트론\n",
    "\n",
    "\t\t1. 입력층,출력층 하나씩만 이루어진 퍼셉트론\n",
    "\n",
    "\t\t2. 논리게이트중 XOR게이트를 설명하지 못한다는 한계가 있다. \n",
    "\n",
    "\t\t\t- 왜냐하면 단층퍼셉트론은 선형결합이라 직선이기 때문에, xor게이트의 양 모서리를 묶어낼 재간이 없다. 단층으로 xor게이트를 풀려면 곡선이 필요하다. 곡선을 만들긴 어려우니 퍼셉트론을 여러개 만들어 직선을 추가하자!\n",
    "\n",
    "\n",
    "\t-다층 퍼셉트론 \n",
    "\n",
    "\t\t1.단층퍼셉트론의 한계를 뛰어넘기 위해 만들어짐\n",
    "\n",
    "\t\t2. 은닉층이 2개면 심층신경망, 심층신경망의 가중치를 찾도록 학습시키는건 딥러닝\n",
    "\n",
    "\n",
    "# 딥러닝\n",
    "\n",
    "\t- 기본구조\n",
    "\n",
    "\t\t: 순전파로 예측수행 >> 오차계산으로 정확도 측정 >> 역전파로 오차전달 >> 가중치 업데이트\t\n",
    "\n",
    "\t1) 연쇄법칙 \n",
    "\n",
    "\t\t- 서로 얽혀있는 변수간의 상관관계를 계산하는 방법!\n",
    "\n",
    "        - 즉 여러 은닉층이 있고 각 은닉층마다 손실에 영향을 주는데, 그 영향력이 얼마인가를 각 은닉층의 영향력의 곱으로 나타낸다.\n",
    "\n",
    "        - 그런데 은닉층이 엄청 많다면? 은닉층의 영향력을 곱할수록 기울기가 0에 가까워진다 이것이 바로 기울기 소실\n",
    "\n",
    "        - 기울기를 알아야하는 이유는 바로 오답노트를 해서 가중치를 어떻게 바꿔야할지 결정할 근거가 되기 때문. 여기서의 오답노트는 손실함수를 줄이는것\n",
    "\n",
    "    2) 손실함수\n",
    "\n",
    "        - 실제값과 예측값의 차이를 수치화하는 함수. 모델이 정확한지 오차를 계산하는 것이다\n",
    "\n",
    "        - 평균제곱오차\n",
    "\n",
    "            - 예측값이 연속적인 회귀문제에 적합\n",
    "\n",
    "        - 이진 크로스 엔트로피\n",
    "\n",
    "            - 이진분류문제에서 사용 \n",
    "\n",
    "            - 0이 정답일때와 1이 정답일때의 손실그래프가 교차하는 점이 최적이 된다\n",
    "\n",
    "        - 크로스 엔트로피 오차\n",
    "\n",
    "            - 다중 클래스 분류 문제, 두 확률분포의 유사도를 측정하는데 유용\n",
    "\n",
    "            - 예측확률이 실제클래스와 얼마나 일치하는가를 평가한다\n",
    "\n",
    "    3) 활성화 함수\n",
    "\n",
    "        - 출력값을 다음단계에 쓸수 있도록 형태를 바꿔줌!\n",
    "\n",
    "\t\t- 단층 퍼셉트론 : 계단함수\n",
    "\n",
    "\t\t\t입력값과 가중치의 곱의 총합을 비아스와 단순 비교\n",
    "\n",
    "\t\t- 다층 퍼셉트론 : 시그모이드, 렐루 등\n",
    "\n",
    "            시그모이드 : 비선형함수로서 0과1사이의 값으로 반환\n",
    "\n",
    "\t\t\t렐루함수 : 입력이 0초과면 그대로, 0이하면 0을 출력. 계산이 간단하며 기울기소실문제를 개선함! \n",
    "\t\t\t양수구간 미분값이 1이기 때문 \n",
    "\n",
    "\t\t\t탄함수 : 시그모이드의 변형으로 -1과 1사이 값으로 나옴\n",
    "\n",
    "\t\t\t소프트맥스 : 다중 클래스 분류문제에 활용, 출력값을 확률로 변환함. \n",
    "\n",
    "\t4) 경사하강법과 옵티마이저\n",
    "\n",
    "\t\t- 경사하강법 : 손실함수를 최소화하여 모델을 학습시키는 방법! \n",
    "\n",
    "\t\t\t- 배치를 나눠 학습시킨다!\n",
    "\n",
    "\t\t\t\t- 메모리와 시간을 절약하기위해\n",
    "\n",
    "\t\t\t\t- 배치란 데이터셋을 나누는 기준으로 학습과정에서 가중치와같은 배개변수를 조정하기위해 사용됨\n",
    "\n",
    "\t\t\t\t- 배치가 너무 크면 학습이 느려지고 메모리가 부족해짐. 너무 작아도 학습이 불안정해짐\n",
    "\n",
    "\t\t\t- 전체 데이터셋의 배치를 모두 돌리는게 1에폭이라고 함.\n",
    "\n",
    "\t\t\t- 1에폭에 배치의 개수를 이터레이션(스텝)이라고 함\n",
    "\n",
    "\t\t\t1. 배치경사하강법 \n",
    "\n",
    "\t\t\t\t- 배치사이즈가 전체 데이터사이즈이다. 따라서 메모리가 많이 필요하다. 하지만 통일성이 높아 수렴이 안정적이라는 장점이 있다. \n",
    "\n",
    "\t\t\t2. 배치크기가 1인 확률적 경사 하강법\n",
    "\n",
    "\t\t\t\t- 더 적은 데이터를 사용하므로 메모리도 아끼고 속도도 빠르다. 하지만 매개변수 변경폭이 불안정하고 정확도가 낮을 수 있다.\n",
    "\n",
    "\t\t\t3. 미니배치 경사하강법\n",
    "\n",
    "\t\t\t\t- 배치크기를 극단적이 아닌 적절히 설정해보자! \n",
    "\n",
    "\t\t\t\t- 적당히 빨라지고 적당히 안정적으로 바뀐다! 배치 사이즈는 2의 n승으로 한다. \n",
    "\n",
    "\t\t- 옵티마이저\n",
    "\n",
    "\t\t\t- 경사하강법에서 더 효과적으로 최솟값을 찾아갈 수 있다. \n",
    "\n",
    "\t\t\t1. 모멘텀\n",
    "\n",
    "\t\t\t\t- 변수가 가던 방향으로 계속 가도록 속도를 추가한다. 즉관성을 추가하는것\n",
    "\n",
    "\t\t\t\t- 작은 운덩이에 빠져도 관성으로 넘어갈수 있다\n",
    "\n",
    "\t\t\t2. 알엠에스프롭\n",
    "\n",
    "\t\t\t\t- 학습률을 조정해 안정적이고 빠르게 학습하도록 하자\n",
    "\n",
    "\t\t\t\t- 최근기울기를 참고해 급한경사에서는 천천히, 완만한경사에서는 속도를 유지하거나 즈가시킨다\n",
    "\n",
    "\t\t\t3. 아담\n",
    "\n",
    "\t\t\t\t- 알엠에스프롭과 모멘텀을 합침\n",
    "\n",
    "\t\t\t\t- 즉 방향과 학습률 두가지를 모두 잡기 위한 방법! 각 파라미터의 1차모멘트와 2차모멘트를 \n",
    "\t\t\t\t사용해서 학습률을 조정한다. \n",
    "\t\t-데이터 아규멘테이션\n",
    "\n",
    "\t\t\t- 다양한 변형데이터를 만들어 학습시킨다\n",
    "\n",
    "\t\t\t- 장점\n",
    "\n",
    "\t\t\t\t- 데이터 의존성이 감소하며 과적합이 완화된다. 데이터 프라이버시가 보호된다. \n",
    "\n",
    "\t\t- 전이학습\n",
    "\n",
    "\t\t\t- 한문제에서 학습한 지식을 다른관련 문제에 활용해보는것 !\n",
    "\n",
    "\t\t\t- 장점 \n",
    "\n",
    "\t\t\t\t- 계산비용 절감, 데이터셋크기 문제완화, 일반화가능성 향상, 성능의 향상 \n",
    "\t\t\t\t\n",
    "\t\t\t- 일반적 특징학습을 먼저 진행한다. \n",
    "\n",
    "\t\t\t- 이후 새로운 데이터에 학습을 진행한다. 이 과정에서 이전학습모델이 조정되고 이를 파인튜닝이라고 한다. \n",
    "\n",
    "# CNN\n",
    "\t- 이미지 공간정보를 유지한 상태로 학습이 가능한 방식\n",
    "\n",
    "\t- 다층퍼셉트론은 FC층을 활용해 분류를 진행하지만 CNN은 컨볼루션레이어와 풀링레이어를 사용해 이미지 특징을 유지한 후 FC층을 활용해 분류를 진행한다.\n",
    "\n",
    "\t- 특징 추출 영역\n",
    "\n",
    "\t\t- 컨볼루션 레이어와 풀링 레이어를 여러겹 쌓는 형태. 컨볼루션레이어는 입력데이터에 필터를 적용후 활성화 함수를 반영하는 필수요소. 풀링레이어는 이미지 공간크기를 줄여 중요 특징을 강조하는 역할. 모델 계산 효율성을 높이고 과적합 방지에 도움.\n",
    "\n",
    "\t- 클래스 분류 영역\n",
    "\n",
    "\t\t- FC레이어로 구성된다. \n",
    "\n",
    "\t1. MLP가 아닌 CNN인 이유\n",
    "\n",
    "\t\t1. 핵심은 이미지 데이터는 정형데이터와 다르니 이미지의 구조를 유지할 필요가 있음! 따라서 마구잡이 MLP가 아닌 Conv,Pulling이 추가된 CNN이 필요한것!\n",
    "\n",
    "\t\t2. MLP는 파라미터폭발 문제가 있는데 CNN에서는 부분연결구조와 Conv연산으로 특정피쳐에만 집중했다.\n",
    "\n",
    "\t\t3. MLP는 이미지를 1차원벡터로 바꾸는 과정에서 공간정보손실이 큰데, CNN은 특징을 추출하고 바꾸기때문에 손실정도가 낮다.\n",
    "\n",
    "\t2. Conv\n",
    "\n",
    "\t\t1. 전체 데이터를 커널 크기만큼만 훑으면서 특징을 학습한다. 커널은 가중치와 같은 역할이다\n",
    "\n",
    "\t\t2. 학습한 데이터는 요소별곱셈 후 합계를 계산함 이를 모아 피쳐맵에 저장해둔다!  \n",
    "\n",
    "\t3. Padding\n",
    "\n",
    "\t\t1. 커널을 거치며 정보를 잃어버리는것을 막기 위해 이미지 바깥을 0으로 둘러싸는 기법\n",
    "\n",
    "\t\t- 가장자리 특징까지 반영할 수 있다. \n",
    "\n",
    "\t4. Stride\n",
    "\n",
    "\t\t1. 필터를 몇칸씩 점프하면서 훑을 지 결정하는 값\n",
    "\n",
    "\t\t- 값이 커질수록 피쳐맵의 크기가 작아진다 \n",
    "\n",
    "\t5. Pooling\n",
    "\n",
    "\t\t1. 피처맵에서 특징을 추출하기 위해 이미지를 격자형태로 분할한뒤, 영역안에서 가장 큰 값을 고르는것\n",
    "\n",
    "\t\t2. 고르는 방식에 따라 맥스풀링, 에버리지풀링으로 나뉜다. \n",
    "\n",
    "\t\t3. 차원을 축소시키기 때문에 용량과 계산량이 감소하지만 정보소실위험이 존재한다. \n",
    "\n",
    "\t6. CNN종류\n",
    "\n",
    "\t\t1. Alexnet: 5개의 Conv층과 3개의 FC층 구조, 렐루함수를 사용함\n",
    "\n",
    "\t\t2. VGGnet:Conv+Pool을 여러번쌓음. 단순하지만 정확도가 높고 초반연산량이 매우 많음\n",
    "\n",
    "\t\t3. ResNet: 잔차연결개념을 도입함. 이전 레이어의 출력을 건너뛰고 다음 레이어 입력으로 추가함으로써 층을 깊게 쌓아도 기울기 소실문제가 없음. 단 입력과 출력 차원을 맞춰줘야함\n",
    "\n",
    "# CNN응용\n",
    "\n",
    "\t1. 이미지 분류\n",
    "\n",
    "\t\t-Yolo\n",
    "\n",
    "\t2. 객체탐지 : CNN을 통해 Feature Map을 얻고, 그 위에 두가지 head를 올림\n",
    "\n",
    "\t3. 이미지 캡셔닝 : 이미지 내의 객체에 대한 판단뿐아니라 객체들간 관계를 파악하고 단어를 추출하는것.\n",
    "\t\n",
    "\t\t- CNN으로 특징을 추출하고, attention network를 이용해 단어를 생성 rnn을 이용해 문장완성하는 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4659ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
