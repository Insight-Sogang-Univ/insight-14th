{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e30489",
   "metadata": {},
   "source": [
    "- RNN / LSTM의 한계\n",
    "- 트랜스포머의 핵심 원리\n",
    "- Self - Attention\n",
    "    - Query, Key, Value\n",
    "    - Cross Attention vs Self Attention\n",
    "    - Scaled Dot-Product Attention\n",
    "- 트랜스포머의 전처리 과정\n",
    "    - Positional Encoding\n",
    "- 트랜스포머의 인코더/디코더 구조\n",
    "    - Multi-Head Attention\n",
    "        - Multi-Head Attention의 핵심 원리 및 장점\n",
    "        - Masked Multi-Head Attention\n",
    "    - Encoder-Decoder Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba00816",
   "metadata": {},
   "source": [
    "# RNN의 한계와 어텐션의 등장\n",
    "\n",
    "## RNN의 한계\n",
    "\n",
    "- 장기의존성 문제가 있었고 이를 해결하기 위해 LSTM모델을 만들었다. \n",
    "- 장기의존성 문제가 발생했던 이유는 RNN의 구조가 과거 값들에 계속 가중치가 곱해짐에 따라 초반 데이터의 영향력이 너무 적어지기 때문이었다. \n",
    "- 그래서 LSTM은 잊을 정보와 가져갈 정보를 따로 처리했지만 그럼에도 긴 문장에선 기울기감소가 일어난ㄴ다. \n",
    "- Seq2Seq에서는 인코더에서 LSTM을 돌리고 나온 최종결과를 컨텍스트벡터에 저장 후 디코더로 넘겨 다음 단어를 예측하는 모델이다. \n",
    "- 고정된 길이의 벡터에 모든정보를 담다 보니 데이터 소실이 불가피했다. \n",
    "- 이를 해결코자 어텐션이 개발됐다.\n",
    "# Self-Attention\n",
    "## ateention\n",
    "- 어텐션은 중요한것에 집중하는 메커니즘. LSTM이 잊을 정보와 기억할 정보를 구분한다는 논리흐름에서 이어져 집중할 정보에 집중하자는 논리흐름이 나온 것이다.\n",
    "- 데이터소실을 막기 위해 출력시점마다 컨텍스트벡터를 만들어낸다. 그리고 그 컨텍스트 벡터는 현재 출력하는 단어와 인코더의 연관성을 계산하고 가중치를 조절해 가장 집중할 부분을 정한다.\n",
    "## self-attention\n",
    "- 순차적 구조를 제거하고 오직 어텐션만 사용해보자\n",
    "- 문장안에서 어떤 단어가 다른단어와 얼마나 중요한 관계인지 파악가능\n",
    "- 트랜스포머\n",
    "    - 인코더와 디코더 구조는 차용했지만 rnn의 순차적구조는 모두 제거하고 어텐션만 사용했다\n",
    "    - 셀프어텐션, 멀티헤드어텐션, 포지셔널인코딩을 사용했다.\n",
    "    - seq2seq은 인코더 디코더 사이에서만 어텐션을 사용한것에 비해 인코더, 디코더 안에서도 어텐션을 사용하게 됐다. \n",
    "- Q,K,V\n",
    "    - Q: 내가 알고자 하는 질문, 요청. 즉 물어보는 주체\n",
    "    - K: 검색대상이 되는 모든 정보들이 달고있는 이름표. 물어보는 대상\n",
    "    - V: 퀘리랑 같은거에서 시작해서 Q,K의 어텐션계산을 한 결과와 행렬곱을 통해 어텐션벨류를 담는 대상이 된다.\n",
    "- cross attention 과 다른점은 셀프어텐션은 q,k,v모두 동일한시퀀스에서 따온다. cross attention은 인코더의 시퀀스에서 k,v를 가져오고 디코더의 q를 이용해 가중합을 구하는 어텐션이다. \n",
    "- dot-product attetion\n",
    "    - 쿼리,키 벡터를 내적해 유사도를 구함. 이 유사도에 소프트맥스 함수를 적용해 어텐션 가중치를 구함. 이후 이 가중치를 밸류에 곱해 어텐션밸류를 구함.\n",
    "    - 그러나 이 방법은 벡터 차원이 커질수록, 내적의 결과가 너무 커지거나 작아지는 문제가 발생\n",
    "    - 따라서 스케일링과정을 추가해줌. 소프트맥스 안에 유사도를 키벡터의 차원으로 나눠줌으로써 스케일링을 진행함. \n",
    "\n",
    "# Multi_Head Attiention\n",
    "## single head attention\n",
    "\n",
    "- 가중치 행렬은 하나만 사용. 512차원의 입력벡터를 가중치분포를 계산한 후 새로운 512차원의 출력벡터를 생성.\n",
    "- 하지만 이 출력벡터 하나만으로는 문장 속 단어들의 관계를 모두 담아내기 어려움\n",
    "-따라서 싱글헤드 어텐션을 병렬적으로 배치해 학습하자는 아이디어가 나오게됨\n",
    "## Multi head attention\n",
    "- 분할 : 하나의 512벡터를 8개의 벡터 그룹으로 쪼갬. 각각의 그룹은 이제 다양한 관점을 가질 준비가 된 것.\n",
    "- 병렬어텐션 계산 : 각 헤드들이 서로 관여하지 않고 각자의 관점으로 내적을 진행함. 총 8개의 결과가 나올 것\n",
    "- 결합 및 최종투영: 이제 8개의 분석결과를 순서대로 이어붙임. 단순 나열상태인 512벡터를 또다른 가중치 행렬을 곱해줌으로써 최종 아웃풋벡터를 구함. 그런데 최종가중치행렬은 어떻게정하지? \n",
    "# Transformer 전체 아키텍처\n",
    "## 토큰화\n",
    "- 입력 텍스트를 모델이 처리할수 있는 단위로 나누는 단계.\n",
    "- 대부분은 단어에 해당하며 일부단어는 두개의 토큰으로도 나눌 수 있음.\n",
    "## 임베딩\n",
    "- 토큰들을 숫자벡터로 변환하는 단계.\n",
    "- 의미적으로 유사한 토큰들은 유사한 벡터로 표현해 임베딩공간에서 유사한 좌표로 표현가능하다. 그런데 임베딩 단계에서 이미 단어간 유사도를 측정 가능하다는건 단어간 연관성도 어느정도 알고들어가는것아닌가? 그럼 트랜스포머학습 이전에 임베딩을 미리 준비해야된다면, 그건 지도학습을 통해 어느정도 체계가 있는건가?\n",
    "## 포지셔널 인코딩\n",
    "- 단어들의 순서정보를 임베딩 벡터에 추가하는 단계.\n",
    "- 트랜스포머는 모든 단어를 병렬로 처리하므로 순서를 알지 못함. 따라서 문법 측면과 등장순서를 알려주기 위해 포지셔널 인코딩을 사용함.\n",
    "- 이 포지셔널인코딩은 셀프어텐션에서는 중요할것같은데 크로스어텐션을통해 디코더로 넘어갈떄 이 포지셔널인코딩값이 중요하게작용하나?\n",
    "## 인코더와 디코더\n",
    "### 인코더\n",
    "- 피드포워드 레이어\n",
    "    - 입력값을 렐루함수화 했다가 다시 선형화하는 멀티헤드퍼셉트론이다\n",
    "    - 렐루함수화하면 비선형성을 줄 수 있고 더 복자한 패턴을 학습하 수 있기 때문이다. \n",
    "    - 잔차연결 학습법은 학습하기 전의 정보와 학습 후 정보를 비교해 학습 시간을 단축해주는역할을 한다.\n",
    "### 디코더 \n",
    "- 셀프어텐션은 양방향 어텐션이지만 디코더에서는 일부러 단방향 어텐션을 만들었다.\n",
    "    - 왜냐하면 디코더는 다음 단어를 생성해야하는데 다음단어를 이미 알고있다면 그건 컷닝이기 때문이다\n",
    "    - 그래서 다음단어를 마스크로 가려 좌->우 단방향어텐션을 구현시켰다.\n",
    "    - 인코더에게 받은 키와밸류값을 활용해 디코더의 퀘리값과 비교해 다음 단어를 예측한다.\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a85e12",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
