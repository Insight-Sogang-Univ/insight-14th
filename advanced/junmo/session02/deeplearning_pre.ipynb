{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbcfcea",
   "metadata": {},
   "source": [
    " 과거를 기억하는 신경망: RNN (Recurrent Neural Network) \n",
    "\n",
    "\n",
    "1-1. 순차 데이터 (Sequence Data)의 이해\n",
    "\n",
    "순차 데이터는 순서에 의미가 있으며, 순서가 바뀌면 데이터의 의미가 손상되는 특징을 가짐.\n",
    "* 정의: 요소들의 순서 의존성이 핵심 정보이며, 이전 시점의 데이터가 다음 시점에 영향을 주는 자기상관성을 가짐.\n",
    "* 예시: 자연어 문장, 음성 파형, DNA 서열, 비디오 프레임 등.\n",
    "* 기존 데이터와의 차이: 일반 정형 데이터는 행(샘플)과 열(특성)의 순서가 독립적이지만, 순차 데이터는 요소의 순서 자체가 정보를 담고 있음.\n",
    "\n",
    "1-2. RNN의 구조와 원리\n",
    "\n",
    "RNN은 순차 데이터 처리에 특화된 인공 신경망으로, 순환하는 은닉층을 통해 과거 정보를 기억함.\n",
    "* 핵심 원리:\n",
    "    * 은닉 상태: 특정 시점 t까지의 중요 정보를 요약한 '메모리' 또는 '문맥' 벡터. 매 시점마다 업데이트되며 다음 시점으로 전달됨.\n",
    "    * 순환 가중치(V): 이전 시점의 은닉 상태h(t-1)가 현재 은닉 상태h(t)에 영향을 주는 핵심 가중치. 이 가중치를 통해 정보가 순환됨.\n",
    "* RNN 아키텍처 유형:\n",
    "    * One to Many: 이미지 캡셔닝 (입력 1개, 출력 시퀀스)\n",
    "    * Many to One: 감정 분석, 스팸 분류 (입력 시퀀스, 출력 1개)\n",
    "    * Many to Many: 기계 번역, 비디오 인식 (입력 시퀀스, 출력 시퀀스)\n",
    "* 가중치 공유의 이점:\n",
    "    * 입력 시퀀스의 길이에 상관없이 동일한 파라미터(가중치)를 반복적으로 사용함.\n",
    "    * 학습 파라미터 수 감소: 모델을 가볍고 빠르게 만들며, 긴 시퀀스에서도 효율적임.\n",
    "    * 일반화 능력 향상: 시점이나 길이에 상관없이 정보를 처리하는 하나의 일반화된 규칙을 학습함.\n",
    "\n",
    "1-3. RNN의 치명적인 한계: 장기 의존성 문제 \n",
    "\n",
    "RNN의 순차적 구조는 역전파 과정에서 심각한 문제들이 발생하며, 결국 장기적인 맥락 파악을 어렵게 만듦\n",
    "* 문제 1: 기울기 소실 (Vanishing Gradients):\n",
    "    * 시퀀스 뒤쪽의 오차 t시점이 앞쪽 t-n 시점으로 역전파될 때, 반복적인 미분값(기울기) 곱셈W와 활성화 함수 미분값의 곱으로 인해 기울기가 0으로 수렴하는 현상.\n",
    "    * 결과: 먼 과거의 중요한 정보(예: 문장의 주어)를 학습하지 못하고 잊어버리게 됨.\n",
    "* 문제 2: 기울기 폭주 (Exploding Gradients):\n",
    "    * 반대로 기울기가 1보다 클 경우, 오차가 비정상적으로 커져 가중치가 터무니없는 값으로 업데이트되는 현상.\n",
    "    * 해결책: Gradient Clipping (기울기의 상한선을 정해 폭주 방지)으로 완화 가능.\n",
    "* 문제 3: 느린 훈련 시간:\n",
    "    * h(t)를 계산하려면 반드시 h(t-1)이 필요하므로 병렬 처리가 불가능하며, 시퀀스가 길수록 훈련 시간이 증가함.\n",
    "\n",
    "\n",
    "2. 똑똑하게 기억하고 잊는 법: LSTM & GRU \n",
    "\n",
    "RNN의 장기 의존성 문제를 해결하기 위해 도입됨. 정보를 선택적으로 기억/삭제하는 게이트(Gate) 메커니즘을 가진 모델.\n",
    "\n",
    "2-1. LSTM (Long Short Term Memory)\n",
    "\n",
    "* 핵심 아이디어: 정보를 관리하는 '셀 상태’C(t)를 별도로 두어, 중요한 정보가 손실 없이 오랫동안 보존되도록 함.\n",
    "* RNN과의 차이: 은닉 상태 h(t), 단기 기억/출력) 외에 셀 상태 C(t)라는 두 개의 순환되는 층을 사용.\n",
    "* 3가지 게이트:\n",
    "    1. Forget Gate f(t): 이전 셀 상태C(t-1)에서 얼마나 잊을지 결정. (sigmoid 출력: 0~1)\n",
    "    2. Input Gate : 현재 입력에서 얼마나 새로운 정보를 저장할지 결정.\n",
    "    3. Output Gate: 업데이트된 셀 상태를 바탕으로, 다음 층으로 전달할 은닉 상태를 최종적으로 결정.\n",
    "* 장점: 게이트를 통해 기울기가 반복적으로 곱해지는 것을 완화하고 정보의 흐름을 제어하여, 기울기 소실 문제를 효과적으로 완화.\n",
    "* 단점: 구조가 복잡하며, RNN보다 학습 파라미터가 훨씬 많음.\n",
    "\n",
    "2-2. GRU (Gated Recurrent Unit)\n",
    "\n",
    "* 핵심 아이디어: LSTM의 복잡한 구조를 단순화하여 효율성을 높인 모델.\n",
    "* LSTM과의 차이:\n",
    "    * 게이트 수 감소: Forget Gate와 Input Gate를 Update Gate 하나로 통합.\n",
    "    * 셀 상태 통합: 셀 상태와 은닉 상태를 하나의 은닉 상태로 통합.\n",
    "    * Reset Gate: 이전 시점의 정보를 얼마나 무시할지 결정.\n",
    "* 장점: LSTM과 유사한 성능을 보이면서도 파라미터 수가 적어 학습 속도가 빠르고 효율적.\n",
    "* 선택 기준: 긴 시퀀스에서 정교함이 중요하면 LSTM, 계산 효율성과 빠른 학습이 중요하면 GRU가 유리.\n",
    "\n",
    "\n",
    "3.Seq2Seq (Sequence to Sequence) \n",
    "\n",
    "\n",
    "3-1. Seq2Seq의 기본 구조: 인코더와 디코더\n",
    "\n",
    "Seq2Seq는 기계 번역처럼 입력 시퀀스를 다른 출력 시퀀스로 변환하는 모델.\n",
    "* 특징: 입력 시퀀스와 출력 시퀀스의 길이가 달라도 처리가 가능.\n",
    "* 두 가지 핵심 모듈:\n",
    "    1. 인코더 (Encoder): 입력 시퀀스 전체를 순서대로 읽고, 그 의미를 하나의 고정된 크기의 벡터로 압축. RNN, LSTM, 또는 GRU가 주로 사용.\n",
    "    2. 디코더 (Decoder): 인코더로부터 전달받은 압축 정보를 바탕으로, 원하는 시퀀스를 한 단어씩 순차적으로 생성.\n",
    "* 컨텍스트 벡터 (Context Vector):\n",
    "    * 인코더의 마지막 은닉 상태를 의미.\n",
    "    * 입력 시퀀스(원문) 전체의 요약 정보처럼 작동하며, 인코더에서 디코더로 정보를 전달하는 유일한 다리 역할.\n",
    "\n",
    "3-2. Seq2Seq의 한계: 병목 현상 (Bottleneck) \n",
    "Seq2Seq 모델은 고정된 컨텍스트 벡터 구조로 인해 심각한 정보 손실 문제가 있음.\n",
    "* 문제점: 입력 시퀀스의 길이가 아무리 길고 복잡하더라도, 인코더는 모든 정보를 하나의 고정된 컨텍스트 벡터에 압축해야 함!\n",
    "* 결과: 특히 긴 문장의 경우, 앞쪽의 중요한 정보가 컨텍스트 벡터에 제대로 담기지 못하고 손실되어 모델의 성능 저하로 이어지게됨\n",
    "* 해결 방안의 필요성: 이 병목 현상과, 여전히 남아있는 긴 시퀀스에서의 문제(기울기 소실)를 극복하기 위해 어텐션(Attention) 메커니즘이 등장하게 됨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0f12b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
