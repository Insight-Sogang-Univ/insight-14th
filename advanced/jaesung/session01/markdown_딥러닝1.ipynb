{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3975ff",
   "metadata": {},
   "source": [
    "## 인공신경망, 퍼셉트론\n",
    "- 인공신경망: 뇌의 신경망을 모방한 모델\n",
    "    - 다수 입력, 가중치 적용 종합하여 활성함수 -> output\n",
    "\n",
    "- 퍼셉트론: 초기 형태의 인공신경망\n",
    "    - 입력 -> 가중합 -> 활성함수(계다함수) -> 출력\n",
    "    - Training: 원하는 출력값을 내보내도록 가중치를 조정\n",
    "\n",
    "- 가중치: 입력값이 출력값에 주는 영향력 결정. 클수록 해당 입력이 중요\n",
    "- 활성함수: 계단 함수, Sigmoid, Tanh, ReLU, leaky ReLU\n",
    "- 임계값: 가중합이 임계값을 넘으면 1, 0 \n",
    "- 편향: 임계값을 좌변으로 넘겨서 표현 가능. 딥러닝에서는 편향 개념 주로 사용\n",
    "\n",
    "### 단층 퍼셉트론\n",
    "- 두 개의 층(입력층, 출력층)으로만 이루어진 퍼셉트론\n",
    "- AND, NAND, OR 논리 게이트 구현 가능. \n",
    "- 활성함수가 계단 함수일 때, AND, NAND, OR 게이트 가중치, 편향 예시\n",
    "    - AND: [0.5, 0.5, -0.7], [0.5, 0.5, -0.8], [1.0, 1.0, -1.0] 등\n",
    "    - NAND: [-0.5, -0.5, 0.7]\n",
    "    - OR: [0.6, 0.6, -0.5]\n",
    "- XOR 게이트 구현 불가능 -> 직선이 아닌 비선형 영역으로 분리해야만 구현이 가능\n",
    "\n",
    "### 다층 퍼셉트론\n",
    "- XOR 게이트는 퍼셉트론의 층을 더 쌓으면 해결 가능\n",
    "- 중간에 층 추가 -> 입력층, 하나 이상의 은닉층, 출력층\n",
    "- DNN(Deep Neural ~, 심층 신경망): 은닉층이 2개 이상인 신경망\n",
    "- Deep Learning: 심층 신경망을 학습 (가중치를 스스로 찾도록)\n",
    "=> 퍼셉트론 ⊂ 다층 퍼셉트론(MLP) ⊂ 딥러닝 ⊂ 인공신경망(ANN)\n",
    "\n",
    "## 딥러닝 모델 학습 과정\n",
    "* 기본 구조\n",
    "    - 순전파(입력값 -> 출력)\n",
    "    - 오차 계산\n",
    "    - 역전파 (출력층 -> 입력층으로 오차 전달)\n",
    "    - 가중치 업데이트\n",
    "    - 반복 -> 오차값이 0에 가까워지게 됨\n",
    "\n",
    "* 연쇄법칙 (Chain Rule) : 서로 얽혀있는 변수 간의 상관관계를 계산\n",
    "- 여러 은닉층에서 각 은닉층마다 손실에 주는 영향이 곱해져서 최종 손실이 나옴\n",
    "- 각 층이 연결\n",
    "-> 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분 곱으로 나타낼 수 있음\n",
    "-> 기울기 소실 문제 발생 가능\n",
    "\n",
    "* 기울기(연쇄법칙을 통해 구함): 손실함수(오차)를 줄이기 위해 가중치를 어떻게 바꿀지에 대한 값\n",
    "\n",
    "* 손실함수: 실제값과 예측값 차이를 수치화\n",
    "    - 회귀: 평균 제곱 오차 (MSE) \n",
    "    - 이진분류: 이진 크로스 엔트로피 (BCE)\n",
    "        - Loss = -(ylog(y_hat) + (1-y)log(1-y_hat))\n",
    "        - 실제 정답이 1일 때, 예측 확률이 1에 가까워질수록 loss가 0에 가까워짐\n",
    "        - 실제 정답이 0일 때, 예측 확률이 0에 가까워질수록 loss가 0에 가까워짐\n",
    "    - 다중 클래스 분류: 크로스 엔트로피 오차 (CEE)\n",
    "        - -Sigma(y*log(y_hat))\n",
    "        - 예측 확률이 실제 클래스와 얼마나 일치? 실제 분포 / 예측 분포의 유사도 측정 시\n",
    "\n",
    "* 활성화 함수: 출력값을 다음 단계에서 쓸 수 있는 형태로 바꿔주는 역할\n",
    "    - 비선형성을 만듦. \n",
    "    - 출력값 = 활성함수f(가중치*입력값 결과) -> 비선형성 \n",
    "    - 단층 퍼셉트론: 계단함수 (임계값 x =0 기준 x <= 0 -> 0 or x > 0 -> 1)\n",
    "    - 다층 퍼셉트론\n",
    "        - 시그모이드 함수: 입력 신호를 0~1로 비선형적으로 변환\n",
    "            - 값이 연속적으로 변한다\n",
    "        - ReLU(중요!!!): x>0이면 y = x, x<= 0이면 0\n",
    "            - 딥러닝에서 가장 많이 사용\n",
    "            - 계산이 간단하고 효율적이며 비선형성 제공\n",
    "            - 같은 신경망에서도 역전파가 잘됨(기울기 소실 문제 해결)\n",
    "                - 양수 구간에서 미분값이 1이여서 역전파가 잘된다\n",
    "        - Tanh: 출력값 -1 ~ 1. 비선형성, 입력값의 중심을 0으로\n",
    "        - Softmax: Sigmoid랑 비슷하지만 다중 클래스 분류 문제. 출력값을 확률로 변환하여 개별 클래스의 확률을 알 수 있음 -> 출력값 합은 1\n",
    "            - Softax + 크로스 엔트로피 손실 조합 좋음\n",
    "\n",
    "* 경사하강법: 손실 함수를 최소화하여 모델 학습시키는 방법 \n",
    "    -> 오차를 가장 작게 만드는 가중치와 편향 찾기\n",
    "\n",
    "    * Batch: 학습 과정에서 가중치 같은 매개변수를 조정하기 위한 데이터 묶음\n",
    "        - 임의 조정 가능한 하이퍼파라미터\n",
    "        - 배치 크기 너무 크면 학습이 느려지고 메모리 부족.\n",
    "        - 너무 작으면 가중치가 너무 자주 업데이트 -> 학습 불안정\n",
    "\n",
    "    * Epoch: 모든 데이터셋을 학습하는 횟수. 1 epoch는 전체 데이터에 대해서 한번 학습\n",
    "        - 1 Epoch에서 다루는 데이터의 수 = 배치의 크기 * 이터레이션(step)\n",
    "        \n",
    "    - 배치 경사 하강법 / 배치 크기 1인 확률적 경사하강법 / 미니 배치 경사 하강법\n",
    "    - 배치 경사하강법 (Batch size = Data size)\n",
    "        - 전체 데이터를 통해 학습 -> 업데이트 횟수 적으나 메모리 많이 필요. 안정적\n",
    "    - Stochastic(배치 크기 1) \n",
    "        - 전체 데이터가 아니라 랜덤으로 선택 하나의 데이터에 대해서 계산\n",
    "        -> 더 빠르게 계산, 적은 자원 / 변경폭이 불안정, 낮은 정확도\n",
    "    - 미니 배치 경사하강법\n",
    "        - 배치 크기를 적절히 지정.\n",
    "        - 전체 데이터를 계산하는 것보다 빠르고, 배치 크기가 1인 경사 하강법보다 안정적\n",
    "        - 배치 크기는 일반적으로 2^n\n",
    "\n",
    "    - 옵티마이저: 경사하강법을 더 효율적이고 안정적으로\n",
    "        - 모멘텀, RMSProp, Adam\n",
    "\n",
    "### 데이터 증강, 전이 학습\n",
    "- 데이터 증강: 다양한 변형 데이터 학습하여 일반화된 패턴, 원본 데이터 의존성 감소, 과적합 완화, 원본 데이터 프라이버시 보호\n",
    "- 전이학습: 한 문제에서 학습한 지식을 재활용해서 새로운 문제를 더 빨리, 더 잘 해결\n",
    "    - 계산 비용 절감, 작은 데이터셋도 가능, 일반화 가능성, 성능 향상\n",
    "- Pre-trained model: 이미 큰 데이터셋으로 학습 -> 전이 학습\n",
    "\n",
    "- Fine Tuning: 새로운 데이터에 맞춰 일부/전체 층 재학습\n",
    "    - Pre-trained model을 새로운 문제에 맞춰 조정\n",
    "    - 초기 층은 그대로하고 후반층, 출력층만 재학습하여 모든 층을 재조정\n",
    "    - Pre-trained의 일반적 특징은 살리고, 새로운 문제에 특화된 정보 학습 \n",
    "\n",
    "## CNN\n",
    "- 컴퓨터에서 이미지는 픽셀 단위로 구분\n",
    "- 픽셀 정보를 그냥 학습하면 순서가 뒤섞인 채로 학습 -> 하나의 완전한 이미지 X, 원본 이미지의 특징이 사라짐\n",
    "- 사람: 이미지 전체의 모든 부분을 살피지 X, 특징이 존재하는 영역 위주로 살펴봄\n",
    "=> CNN: 이미지 공간정보를 유지한 상태로 학습하는 방식\n",
    "\n",
    "CNN: Feature Learning(위치별 특징 추출) -> Flatten (3차원 -> 1차원 배열) -> Classification (클래스 분류)\n",
    "- 특징 추출 영역\n",
    "    - Convolution Layer: 입력 데이터에 필터 적용 후 활성함수 반영\n",
    "    - Pooling Layer: convolution layer 다음에 이미지의 공간 크기 줄이고 중요한 특징 강조. \n",
    "- 클래스 분류: Full connected Layer로 구성 (회귀, 분류와 유사)\n",
    "\n",
    "=> 이미지 구조 유지 / 위치, 크기, 회전 등 변화에 강함(특징 파악) / 효율적인 파라미터 사용 / 공간 정보 보존 (특징 추출 후 flatten)\n",
    "\n",
    "### Convolution Layer\n",
    "- 전체 이미지를 특정 크기의 필터로 훑음 -> 필터 값이 가중치 역할\n",
    "- convolution 계산\n",
    "- 결과를 Feature Map에 저장\n",
    "\n",
    "### Padding\n",
    "- filter를 거치면서 정보 손실을 막기 위해 이미지 바깥은 0으로 padding. (가장자리 정보 손실 방지)\n",
    "\n",
    "### Stride\n",
    "- 이미지를 몇 칸씩 점프? \n",
    "\n",
    "### Pooling\n",
    "- Feature Map에서 특징 추출하기 위해 이미지 분할 후 영역에서 큰 값 or 평균값\n",
    "\n",
    "### AlexNet -> VGGNET -> ResNet -> Vision Transformer -> ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb1fb3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
