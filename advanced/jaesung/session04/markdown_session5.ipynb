{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23995b7",
   "metadata": {},
   "source": [
    "# 언어 모델 & 생성 모델\n",
    "\n",
    "## 언어모델\n",
    "- 단어 시퀀스에 확률 할당 -> 가장 자연스로운 단어 시퀀스를 찾아내는 모델\n",
    "- 이전 단어 주어졌을 때 다음 단어 예측하도록 훈련\n",
    "- 통계기반: N-gram, Perplexity \n",
    "- 인공 신경망 기반: RNN, LSTM, GRU, Transforemr 등\n",
    "    - 단어를 벡터로 표현하고 신경망을 이용해 다음 단어 예측\n",
    "\n",
    "### SLM (통계적 언어 모델)\n",
    "- 분포 가설: 비슷한 문맥에서 함께 나타나는 단어들은 비슷한 의미를 가진다는 가설\n",
    "-> 어떤 단어의 주변에서 자주 등장하는 단어들을 보면 그 단어의 의미를 짐작할 수 있다\n",
    "\n",
    "- 조건부 확률, 카운트기반 확률\n",
    "- 조건부 확률: A가 일어났다는 전제 하에 B가 일어날 확률\n",
    "    - \"오늘 날씨가 좋다\"의 문장 확률 계산\n",
    "    - 문장이 '오늘'이라는 단어로 시작할 확률 계산\n",
    "    -> 다음이 날씨가 일 확률 계산\n",
    "    -> 다음이 \"좋다\"일 확률 계산\n",
    "    -> P(오늘, 날씨가, 좋다) = P(오늘) x P(날씨가|오늘) x P(좋다|오늘, 날씨가)\n",
    "    => 이 확률값이 높을수록 더 자연스럽다고 판단\n",
    "\n",
    "- 카운트 기반: 이전 단어 시퀀스의 등장 빈도를 통해 다음 단어의 확률 계싼\n",
    "-> 학습 데이터에서 실제 등장 획수 카운트\n",
    "-> 직관적이지만 \"희소 문제\"\n",
    "\n",
    "* 희소 문제(Sparsity Problem)\n",
    "- 학습 데이터 양은 한정 But, 모델이 처리해야할 현실의 언어는 무한대에 가깝다.\n",
    "-> 데이터 부족으로 인해 언어를 정확하게 모델링하지 못하는 문제\n",
    "\n",
    "### N-gram\n",
    "- 카운트 기반의 SLM모델이지만, 앞 단어 전체 참고말고 일부 단어만 참고하여 희소 문제를 완화하려는 아이디어. 단어의 범위를 n개로 정해놓고 확률 계산\n",
    "\n",
    "- 정의: 연속된 n개의 단어 묶음. 문장을 정해진 n개의 단위로 잘라 토큰화\n",
    "- n-1개의 단어만 참고하여 다음 단어 예측\n",
    "- 한계 일부 개선 But, 희소 문제와 문맥 파악의 한계를 극복하지 못함.\n",
    "\n",
    "### Perplexity\n",
    "- 언어 모델 성능 비교를 위해 테스트 데이터를 이용해 빠르고 정량적으로 평가하기 위한 지표\n",
    "- 언어 모델이 특정 문장을 얼마나 혼란스러워하는지를 나타내는 수치\n",
    "- 낮을수록 좋다\n",
    "- 모델이 문장의 확률을 높게 예측할수록 PPL 값은 낮아짐\n",
    "- 영향 요인\n",
    "    - 문장(W)의 희소성: 문장 W가 희소하게 등장할수록 PPL이 커진다. \n",
    "        - 모델이 훈련 데이터에서 더 적게 봤으니깐\n",
    "    - 문장의 길이: 단어 개수 N이 커지면 PPL을 낮춰준다\n",
    "        - 희소한 부분이 있더라도 그 안에 예측이 쉬운 단어들도 같이 있어서\n",
    "<-> 딥러닝 기반의 언어모델: 단어와 문맥 간의 관계를 학습하여 일반화하는 능력이 뛰어남. \n",
    "\n",
    "## 딥러닝 기반 언어모델\n",
    "### LLM\n",
    "- 대규모 언어모델: 방대한 양의 데이터 학습하여 인간 언어 이해, 생성, 요약 등 다양한 작업 수행\n",
    "- 대규모 학습: 단어-단어 관계, 문맥적 의미, 문법 구조, 지식 등 언어의 패턴학습\n",
    "- 예측 및 생성: 학습한 패턴을 기반으로 다음에 올 가장 확률 높은 단어를 순서대로 예측하여 응답 생성\n",
    "- 사례\n",
    "    - 텍스트 생성, 기계 번역, 질의 응답, 문서 요약, 감정 분석\n",
    "\n",
    "### Bert\n",
    "- 트랜스포머의 인코더 부분만을 활용해 만든 양방향 언어 모델\n",
    "- 양방향: 문장을 읽을 때 앞뒤를 동시에 고려\n",
    "- 트랜스포머 기반, 사전학습 + 파인 튜닝 (큰 데이터로 먼저 일반적인 언어 능력을 배우고 나중에 특정 과제에 맞게 살짝 조정)\n",
    "\n",
    "- 트랜스포머의 인코더를 쌓아올린 구조\n",
    "- Embedding(단어 -> 숫자 벡터) \n",
    "    - Token(단어 자체 의미), Segment(두 개 이상 문장에서 어떤 문장인지 구분), Position(단어가 문장에서 몇 번째인지 위치 정보 포함)\n",
    "    => 다 더해서 하나의 입력 벡터 생성\n",
    "\n",
    "- 사전학습(Masked Language Model, Next Sentence Prediction)\n",
    "    - 문장 속 단어를 일부 가려 놓고, 가려진 단어가 뭔지 맞추는 방식의 학습 \n",
    "    - 자연스럽게 문맥을 이해하고 추론하는 능력 기르기\n",
    "    - Input 15% 단어 마스킹, 랜덤 단어 바꾸기 등 처리 \n",
    "    -> 언어의 문맥 학습\n",
    "\n",
    "    - 실제로 이어지는 문장인지도 학습\n",
    "\n",
    "이후 RoBerta, ALBERT\n",
    "\n",
    "### GPT\n",
    "- OPENAI LLM 모델. \n",
    "- 트랜스포머의 Decoder만 사용\n",
    "- Self-Attention은 Masked Attention으로 동작하여 현재 시점 단어 예측 시 앞의 단어들만 참고\n",
    "- BERT: 양방향, 이해 중심 (문장 전체를 봄)\n",
    "- GPT: 단방향, 생성 중심 (앞부분 단어만 보고 다음 단어 생성)\n",
    "\n",
    "한계: 환각, 업데이트 비용\n",
    "\n",
    "### RAG (Retrieval Augmented Generation)\n",
    "- 검색 모델 활용해 정보를 함께 제공 (관련된 정보를 검색하여 먼저 찾고 LLM에 넣어 최종 답변 생성)\n",
    "- 구조: 질의 인코더, 지식 검색기, 지식 증강 생성기\n",
    "- 장점: 풍부한 정보 제공, 실시간 정보 반영, 환각 방지\n",
    "** CAG(Credibility-Aware Generation)\n",
    "- 기존 RAG: 검색된 문서 중 일부 신뢰성이 낮거나 잘못된 정보면 품질, 정확도 떨어짐\n",
    "-> 잘못된 컨텍스트가 모델 성능에 미치는 부정적 영향 최소화, 문서를 단순히 활용하는 수준을 넘어 문서의 신뢰도를 스스로 판단하고 활용할 수 있도록 훈련시키기\n",
    "\n",
    "### LangChain\n",
    "- LLM + RAG를 직접 구현하는 것은 복잡하니깐, PipeLine을 쉽게 구성할 수 있도록 도와주는 프레임워크\n",
    "- LLM 조립 레고 블록\n",
    "- 추상화, 표준화(다양한 AI 모델, 원본 데이터들을 하나이 형식으로 통일, 표준화), 체이닝(자주 활용하는 컴포넌트 쉽게 연결해 LLM 서비스 로직 쉽게 파악)\n",
    "** LangGraph\n",
    "- Langchain은 선형적인 체인구조로 이전 단계로 다시 돌아가 실행하는 등의 복잡한 로직 구현 어려움\n",
    "-> 비선형적인 작업 형태\n",
    "    - Node, Edge... \n",
    "\n",
    "** LangSmith: LLM 기반 애플리케이션을 위한 통합 개발 및 운영 플랫폼\n",
    "- 디버깅, 추적, 테스트 및 평가, 모니터링, 프롬프트 허브(저장, 버전 관리)\n",
    "\n",
    "### sLM\n",
    "- 소규모 언어 모델: 규모와 범위가 작음(경량화, 빠른 학습/추론, 특정 작업 최적화, 적은 비용)\n",
    "- 모델 압축: 가지치기, 양자화, 사전 학습 내용 바탕으로 지식 증류\n",
    "- Llama, Phi-3, Gemma, Mixtral, OpenELM\n",
    "- 장점\n",
    "    - 프라이버시 (로컬 환경 실행 가능)\n",
    "    - 비용절감, 효율성 & 맞춤화\n",
    "- 한계\n",
    "    - 편향 및 성능 저하\n",
    "    - 제한된 일반화\n",
    "    - 환각, 성능(추론) 및 용량 한계\n",
    "-> Fine Tuning, 전문화, 지식 증류\n",
    "-> 추론 단계: 외부 도움으로 보완 (RAG, 결합 추론...), 모델 압축\n",
    "\n",
    "### 결합 추론\n",
    "- LLM + sLM 결합: 막히는 부분만 LLM에게 선택적으로 도움 받아 해결\n",
    "\n",
    "## 생성 모델\n",
    "- 주어진 학습 데이터를 학습하여 학습 데이터의 분포를 따르는 유사한 데이터를 생성하는 모델\n",
    "- 샘플에 레이블 지정 관심 X, 입력 데이터의 확률 부포에 관심\n",
    "<-> 분류 모델: 정답 레이블이 있다. \n",
    "\n",
    "- 명시적 확률밀도 모델: 학습 데이터의 분포를 기반으로 생성\n",
    "- 암시적 확률밀도 모델: 학습 데이터의 분포와 상관없이 생성\n",
    "\n",
    "AE: 입력과 동일한 출력을 만드는 것을 목적으로 하는 신경망\n",
    "    - 차원 축소, 특징 추출, 노이즈 제거, 이상 탐지 등 데이터 복원이나 특성 학습에 많이 사용\n",
    "\n",
    "VAE(확률적 오토인코더): 랜덤 노이즈로부터 원하는 영상을 얻기 -> 데이터를 잠재 공간으로 인코딩 -> 다시 디코딩하여 원본 데이터와 유사한 결과를 생성\n",
    "-> 이미지 생성, 텍스트 생성, 신호처리, 이미지 보간 등... \n",
    "-> Decoder 학습을 위해 Encoder를 붙임 \n",
    "\n",
    "### Gan\n",
    "- 생성적 적대 신경망: 생성자 신경망과 판별자 신경망이 서로 적대적으로 경쟁하면서, 훈련을 통해 자신이 작업을 점점 더 정교하게 수행하는 신경망 모델\n",
    "\n",
    "- 생성자: 진짜 분포에 가까운 가짜 분포 생성\n",
    "- 판별자: 표본이 가짜인지 진자인지 결정\n",
    "-> 실제 데이터의 분포에 가까운 새로운 데이터 생성. \n",
    "\n",
    "- GAN 장점: 진짜 같은 가짜를 생성할 수 있음\n",
    "- 한계점: 학습이 불안정함 (진동이 크다)\n",
    "\n",
    "### 확산 모델\n",
    "- 입력 이미지에 Noise를 여러 단계에 걸쳐 추가하고 여러 단계에 걸쳐 Noise를 제거함으로써 입력 이미지와 유사한 확률 분포를 가진 결과 이미지를 생성.\n",
    "- Diffusion Model의 목표: Forward -> Reverse 단계를 거친 결과 이미지를 입력 이미지의 확률 분포와 유사하게 만드는 것\n",
    "\n",
    "- 순확산: 데이터에 점진적으로 노이즈를 추가\n",
    "- 역확산: 노이즈 데이터에서 원본 데이터를 재구성하는 고정 (추가된 노이즈 제거 및 원래의 데이터를 복원)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
