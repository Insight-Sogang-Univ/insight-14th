{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7bb186",
   "metadata": {},
   "source": [
    "***다음 내용이 필수적으로 들어가야 합니다.*** \n",
    "### 넹~\n",
    "\n",
    "- 순차 데이터란?\n",
    "- RNN의 구조\n",
    "    - 가중치 공유\n",
    "    - 장기 의존성 문제\n",
    "- LSTM의 구조\n",
    "- GRU의 구조\n",
    "- Seq2Seq의 구조\n",
    "    - 병목현상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0100131",
   "metadata": {},
   "source": [
    "### 순차 데이터란\n",
    "- 순서에 의미, 순서가 달리지면 의미가 손상됨\n",
    "- 순차 데이터 -> 시간적 순차 데이터 (시간 흐름) -> 시계열 데이터 (일정한 시간간격)\n",
    "- 음성, 오디오 데이터, 자연어, 생물학적 서열 데이터, 비디오 데이터\n",
    "\n",
    "* 특징\n",
    "    - 데이터 구성 요소들의 순서 자체가 핵심 정보 (순서 의존성)\n",
    "    - 자기상관성: 이전 시점의 데이터가 다음 시점의 데이터에 영향을 미침\n",
    "\n",
    "=> 기존 머신 러닝 기법 한계 \n",
    "=> 과거 정보 기억 및 순서 의미 학습 필요\n",
    "\n",
    "## RNN\n",
    "- 순환하는 구조를 가진, 순차 데이터 처리 특화 인공 신경망\n",
    "- 시퀀스 데이터를 받아 순서 정보를 유지하며 처리\n",
    "- 순환하는 은닉층이 매 시점의 은닉층 상태를 업데이트\n",
    "- 이점 시점의 값을 현재 시점으로 넘겨 준다. \n",
    "\n",
    "### RNN 구조\n",
    "- 시간 스텝 t가 존재하는 데이터\n",
    "- 입력: 시간 스텝 t에서의 입력. 특정 시점의 입력\n",
    "- 은닉 상태: 시간 스텝 t에서의 은닉 상태. 이 시점의 메모리\n",
    "- 출력: 시간 스텝 t에서의 출력\n",
    "- 입력 가중치: 현재 시점 t의 입력이 은닉 상태에 영향을 주는 가중치\n",
    "- 순환 가중치: 이전 시점 t-1의 은닉 상태가 현재 은닉 상태에 영향을 주는 가중치\n",
    "    - 이점 시점의 정보를 전달 \n",
    "- 출력 가중치: 현재 시점 t의 은닉상태가 최종 출력에 영향을 주는 가중치\n",
    "    - 계산된 메모리를 바탕으로 결과를 만들어냄  \n",
    "\n",
    "- 정보 흐름\n",
    "    - t-1 시점에서 t-1 입력이 들어가서 은닉상태 업데이트 및 출력 내보냄\n",
    "    - t 시점: t-1 시점에서의 계산된 은닉 상태가 다음 시점에 \"그대로\" 전달\n",
    "    - 새로운 입력과 전달받은 과거 정보를 함께 사용하여 현재 시점의 은닉 상태 업데이트 및 출력 계산\n",
    "\n",
    "### 핵심 원리\n",
    "- 가중치 공유: \"동일한 가중치\"를 다양한 시점에서 반복적으로 사용\n",
    "- X1, X2... Xn 처리까지 모두 동일한 파라미터(입력 가중치 U, 순환 가중치 W, 편향 b)를 공유하는 구조\n",
    "- 각 시점 t마다 ht​=f(Uxt​+Wht−1​+b) 계산. U, W, b는 모든 시점 t에서 동일하게 공유\n",
    "    - U : 입력 → 은닉 가중치\n",
    "    - W : 이전 은닉 → 현재 은닉 가중치 (즉, \"메모리 연결\" 역할)\n",
    "    - b : 편향\n",
    "    - U,W,b 처음에는 무작위 초기화 -> 훈련 데이터를 통해 \"역전파\"로 최적화 -> 훈련이 끝난 후에는 고정 ~ \"고정된 규칙\"\n",
    "    - 새로운 입력 시퀀스에 대해서 고정된 규칙 기반으로 은닉 상태 업데이트 후 \"출력\" 계산\n",
    "    - 훈련 단계: U,W,b가 데이터 보고 배움(업데이트)\n",
    "    - 추론 단계: 학습된 U,W,b로 새로운 입력 처리\n",
    "\n",
    "### RNN 한계, 장기 의존성 문제\n",
    "- RNN 한계\n",
    "    - 구조 특성 상 전체 시퀀스를 \"모두 읽은 후\" 역전파가 이루어짐\n",
    "    - 기울기 소실 / 기울기 폭주\n",
    "    - 느린 훈련 시간: 순차적 계산 과정 t 시점 은닉층 계산하려면 t-1 시점 은닉 상태 필요 -> 병렬처리 불가\n",
    "    -> 장기 의존성 문제 발생\n",
    "        - 시퀀스 앞 부분의 중요한 정보를 잊어버려 맥락 파악 능력이 급격히 저하\n",
    "    \n",
    "    -> NEW! \"기억력 한계 극복\" \"더 길고 복잡한 시퀀스 데이터\" \"중요한 정보 오래, 불필요한 정보는 잊는 메모리 구조\"\n",
    "\n",
    "## LSTM, GRU\n",
    "- LSTM 철수: 겉과 속이 다른 정교하지만 고민이 많고 복잡한 모에피 러버 \n",
    "- GRU 짱구: 겉과 속이 같은 초코비, 액션가면, 예쁜 누나 등 좋아하는 새로운 자극 도파민 러버. 기억과 행동이 통합. 단순하고 즉각적\n",
    "\n",
    "### LSTM (Long Short Term Memory)\n",
    "- Gate로 정보 흐름 제어: Forget, input, output gate\n",
    "- 기억할 내용과 잊어버릴 내용 선택\n",
    "    - 순환 layer: c_t (장기 기억), h_t(단기 기억)\n",
    "    - Gate\n",
    "        - Forget gate: 정보를 얼마나 잊어버릴지\n",
    "        - Input gate: 현재 정보를 얼마나 사용할지\n",
    "        - Output gate: 다음 층으로 어떤 정보를 전달할지\n",
    "    - Final memory cell: input, forget gate 결합해 현재 정보를 얼마나 기억할지 계산\n",
    "    LSTM 수식 나중에 볼게요\n",
    "\n",
    "- 장점\n",
    "    - 기울기 소실 문제 효과적으로 완화\n",
    "    - Gate를 통해 기억할 정보와 잊을 정보를 나눠서 관리하여 이전 정보가 중요하면 보존하여 hidden state를 갱신 가능\n",
    "- 단점\n",
    "    - RNN보다 복잡하고 파라미터가 많아짐\n",
    "\n",
    "### GRU(Gated Recurrent Unit)\n",
    "- 메모리 셀 없이 게이트 수를 줄여 구조를 간소화\n",
    "- Update Gate: LSTM의 forget gate + input gate\n",
    "- Update gate, Reset gate 사용\n",
    "- gate 수가 3개 -> 2개\n",
    "- 수식 나중에 볼게요\n",
    "\n",
    "### LSTM vs GRU\n",
    "- LSTM: 게이트 3개, 복잡, 계산량 많음, 메모리 사용량 큼. 장기 의존성 처리에 강력함. 연구 사례가 많음. \n",
    "-> 데이터가 충분하고 문제 복잡성이 높을 때, 긴 시퀀스, 모델 성능이 중요할 때\n",
    "-> 기계 번역, 언어 모델링, 장기 시계열 예측 (금융, 기후 데이터)\n",
    "\n",
    "- GRU: 게이트 2개, 단순, 계산량 적음. 메모리 사용량 작음. 장기의존성 처리 다소 약함. \n",
    "-> 데이터 적거나 효율성이 중요할 때, 과적합 위험이 있을 때. 실시간 예측이 필요할 때 응용\n",
    "-> 음성 인식, 스트리밍 데이터, 실시간 비디오 분석, 짧은 시퀀스 기반의 텍스트 분류\n",
    "\n",
    "## Seq2Seq\n",
    "- 한 시퀀스를 다른 시퀀스로 변환 ~ 기계 번역에서 사용\n",
    "- 인코더 / 디코더\n",
    "- 인코더: 흰둥아 ~ 있잖아 ~ 그거 ~ 동그란 거 ~ 솜사탕 ~\n",
    "    - 순서대로 듣고 \"하나의 핵심의도\"로 \"요약\" (압축)\n",
    "    - 하나의 핵심의도 = \"컨텍스트 벡터\" \n",
    "\n",
    "순차 데이터 -> 인코더 -> 컨텍스트 벡터로 압축 -> 디코더\n",
    "\n",
    "- 디코더: 컨텍스트 벡터를 받아서 \"새로운 문장, 행동\"을 \"만듦\"\n",
    "    - 출력: 몸을 던져서 -> 데굴데굴 -> 털 부풀 -> 뿅\n",
    "\n",
    "* 특징\n",
    "    - 입,출력 길이가 달라도 됨\n",
    "    - 인코더, 디코더 분리\n",
    "    - 컨텍스트 벡터. \n",
    "\n",
    "### 기본 구조: 인코더, 디코더\n",
    "je suis etudiant -> 인코더 -> Context -> Decoder -> I am student\n",
    "- 인코더\n",
    "    - t시점의 임베딩된 단어의 입력과 t-1 시점의 은닉 상태가 RNN/LSTM의 입력으로 주어지며, 그 결과로 t시점의 은닉 상태가 나온다\n",
    "\n",
    "- Context Vector: \"인코더 중 마지막 시점\"의 은닉 상태\n",
    "    - 입력 시퀀스의 요약 정보처럼 작동\n",
    "\n",
    "- 디코더\n",
    "    - 인코더의 컨텍스트 벡터는 디코더의 첫 번째 은닉 상태에 사용됨. \n",
    "    - t시점의 출력값이 그 다음 시점의 입력값으로 넘어감\n",
    "\n",
    "- 인코더, 디코더: RNN, LSTM, GRU 여러개 조합 형태 \n",
    "\n",
    "### 한계: 병목 현상\n",
    "- 문장을 요약할 요약본의 크기에 제한을 걸면 정보 손실될 수 있음\n",
    "- 고정된 길이의 context vector 때문에 입력 시퀀스의 모든 정보를 담지 못하고 손실될 수 있음 -> 병목 현상\n",
    "- 기울기 소실, 병렬화 불가능 문제 있다.\n",
    "\n",
    "-> \"Attention\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
