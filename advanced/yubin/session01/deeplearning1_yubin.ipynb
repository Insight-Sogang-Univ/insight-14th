{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d67229",
   "metadata": {},
   "source": [
    "# 딥러닝1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45ab10",
   "metadata": {},
   "source": [
    "## 인공신경망의 시작: 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08243121",
   "metadata": {},
   "source": [
    "### 인공신경망이란?(Artificial Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e316310",
   "metadata": {},
   "source": [
    "**인공신경망과 퍼셉트론**\n",
    "1. 인공신경망: 우리 뇌의 신경망(뇌를 구성하는 신경세포 뉴런이 서로 연결되어 정보를 처리하는 구조)을 모방한 모델 <br>\n",
    "<br>\n",
    "<생물학적 뉴런이 정보를 처리하는 구조> <br>\n",
    "* 가지돌기를 통해 외부 전기신호를 받아들임\n",
    "* 모인 신호가 일정 이상의 크기를 가지면\n",
    "* 축삭돌기를 통해 신호를 전달\n",
    "<br>\n",
    "<인공신경망>\n",
    "* 다수의 입력을 받아 각각에 가중치 적용\n",
    "* 종합한 값이 일정 기준을 넘으면(활성화함수)\n",
    "* 아웃풋 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e55087",
   "metadata": {},
   "source": [
    "##### 페셉트론의 출력값을 구하는 절차<br>\n",
    "=> 입력 -> 가중합 -> 활성화 함수 -> 출력 <br>\n",
    "1. 입력값과 가중치를 곱합\n",
    "2. 곱한 값들의 총합을 구함\n",
    "3. 총합이 임계값응 넘으면 1, 넘지 않으면 0을 출력 \n",
    "<br>\n",
    "*Training: 원하는 출력값을 내보내도록 가중치를 조정해가는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2454bd",
   "metadata": {},
   "source": [
    "**용어정리**\n",
    "1. 가중치 = 입력값이 출력값에 주는 영향력(중요도)를 결정, *가중치의 값이 클수록 해당 입력값이 중요함을 의미*\n",
    "2. 활성화함수 = 신호가 임계값(0)을 넘으면 1, 넘지 못하면 0을 출력 (여기서는 계단함수 였음! ㄹ이 90도 돌아간 상태였으니까...)\n",
    "3. 임계값 = 계단 함수에 사용된 임계치로 보통 세타로 표현\n",
    "4. **편향** = 계단 함수에서 구한 식에서, 임계값을 좌변으로 넘기면 편향으로 표현 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89537ded",
   "metadata": {},
   "source": [
    "### 단층 퍼셉트론과 그 한계\n",
    "1. 단층 퍼셉트론 : 두 개의 층(입력층, 출력층)으로만 이루어진 퍼셉트론\n",
    "2. 논리 게이트 구현(andm nand, or등): 기본적이 논리 연산을 수행하는 데 사용되는 중요한 구성 요소\n",
    "* **두개의 입력값**과 **하나의 출력값**\n",
    "    1. AND  게이트 : 두 개의 입력값이 모두 1인 경우에만 출력값이 1이 나오는 구조\n",
    "    2. NAND 게이트 : 두 개의 입력값이 1인 경우에만 출력값이 0, 나머지 입력값의 쌍에 대해서는 모두 출력값이 1\n",
    "    3. OR 게이트 : 두개의 입력이 모두 0인 경우에 출력값이 0이고, 나머지 경우에는 모두 출력값이 1\n",
    "<br>\n",
    "3. 단층 퍼셉트론의 한계 : **XOR 게이트 구현 불가능**\n",
    "* XOR 게이트 : 입력값 두 개가 서로 다른 값을 갖고 있을 때에만 출력값이 1\n",
    "-> 이는 직선이 아닌 곡선, 비선형 영역으로 분리해야만 구현이 가능하다.\n",
    "* 퍼셉트론은 입력값과 가중치의 선형 결합으로 이루어진 구조이기 때문에, 단층 퍼셉트론은 직선 하나로 두 영역을 나눌 수 있는 **선형 분류** 문제에 대해서만 구현이 가능함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30869e",
   "metadata": {},
   "source": [
    "### 다층 퍼셉트론<br>\n",
    ": XOR 게이트는 기존의 AND, NAND, OR 게이트를 조합하면 만들 수 있음. 즉, 퍼셉트론의 층을 더 쌓으면 가능함\n",
    "<br>\n",
    "* 층을 더 추가하여 입력층 + 하나 이상의 은닉층(더 복잡한거 다루려면 이걸 추가) + 출력층으로 구성\n",
    "* 두 개 이상의 은닉층을 갖는 MLP 가 딥러닝의 시초 모델로 발전\n",
    "    * 심층 신경망 = 은닉층이 2개 이상인 신경만\n",
    "    * 딥러닝 = 심층 신경망을 학습(가중치를 스스로 찾도록)시킴\n",
    "        * 퍼셉트론 -> 다층 퍼셉트론 -> 딥러닝 -> 인공 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e0b91",
   "metadata": {},
   "source": [
    "## 딥러닝 모델은 어떻게 학습할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f666d1e",
   "metadata": {},
   "source": [
    "### 순전파와 역전피\n",
    "* 학습 알고리즘의 기본 구조\n",
    "    1. 순전파를 통해 입력값으로부터 예측을 수행 (문제풀이-예측)\n",
    "    2. 오차 계산을 통해 예측 정확도를 측정\n",
    "    3. 역전파를 통해 출력층에서 다시 입력층 방향으로 오차를 전달 (정답보고 다시)\n",
    "    4. 가중치를 업데이트\n",
    "    5. 1~4번 과정을 반복 => 연쇄법칙\n",
    "<br>\n",
    "* 연쇄법칙 : 서로 얽혀있는 변수 간의 상관관계를 계산하는 방법\n",
    "    : 딥러닝 모델은 은닉층이 여러 개 존재하는데, **각 은닉층마다 손실에 주는 영향을 곱해서 최종 손실에 대한 값을 나타냄**\n",
    "    * 결국 A가 C에 미치는 영향은, B가 C에 미치는 영향과 A가 B에 미치는 영향을 곱한 것\n",
    "        *그런데... 은닉층이 너무 많고 미분값이 1보다 작은 값이면 점점 0에 수렴! = 기울기가 0이 되는 문제 발생 \n",
    "        * => **기울기가 소실되는 문제** = 은닉층이 많아질 수록 은닉층의 영향력이 거의 사라지는 것\n",
    "    * 기울기는 왜 계싼할까?\n",
    "        * 순전파와 역전파를 활용해서 오차를 줄이는 것이 필요\n",
    "        * 손실함수 = 오차값과 실제값을 수치로 표현\n",
    "        * 연쇄법칙을 통해 구하는 기울기 = **손실함수를 줄이기 위해 가중치를 어떻게 바꿀가? 를 알려주는 신호**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c32d49",
   "metadata": {},
   "source": [
    "### 손실함수 <br>\n",
    "-> 출력층의 결과와 정답의 차이를 계산하는 데 사용되는 함수 <br>\n",
    "**손실 함수의 종류**\n",
    "1. 평균제곱오차 MSE : 예측값이 연속적이고, 예측값과 실제값 간의 차이를 직접적으로 측정하는 회귀 문제 일 때\n",
    "2. 이진 크로스 엔트로피: 이진 분류 문제(로지스틱 회귀)에서 사용\n",
    "    * 실제 정답이 1일 때\n",
    "        * 예측확률이1 -> loss값은 0에 가까워짐\n",
    "        * 예측확률이0 -> 손실값이 급격히 증가 => 모델에 확신이 없는 경우\n",
    "    * 실제 정답이 0일 때\n",
    "        * 예측 확률이 0 -> loss 값은 0\n",
    "        * 예측 확률이 1 -> loss 값 급격히 증가 => 모델이 확신을 가지고 틀린 예측\n",
    "3. 크로스 엔트로피 오차\n",
    "    * **다중 클래스 분류** 문제에서 예측 확률이 실제와 얼마나 일치하는 지 평가하는데에 적합\n",
    "    * 진짜 분포와 예측 분포의 유사도를 측정하는 데에 유용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b6c9e",
   "metadata": {},
   "source": [
    "### 활성화함수<br>\n",
    ": 출력값을 다음 단계에서 쓸 수 있는 형태로 적절히 바꿔주는 역할 <br>\n",
    "* 비선형성을 만들어주며, 이를 통해 복잡한 패턴도 학습이 가능함 <br>\n",
    "    * 단층 퍼셉트론\n",
    "        * 계단 함수\n",
    "            1. 입력 신호 : 입력값과 가중치의 곱\n",
    "            2. 출력 신호 : 1/0\n",
    "        * 시그모이드 함수 : 직선이 아니라 곡선형태로 0~1 사이로 반환\n",
    "            * y값이 연속적으로 변함\n",
    "        * **ReLU 함수** : 입력이 0을 넘으면 그 입력값을 그대로 출력하고, 0 이하면 0을 출력하는 비선형 함수 -> 딥러닝에서 가장 많이 사용\n",
    "            *왜?\n",
    "                1. 계산이 간단하고 효율적\n",
    "                2. 비선형성을 제공 (은닉층에 비선형성이 안주면 선형모델처럼 됨)\n",
    "                3. 같은 신경망에서도 역전파가 잘 전달됨 = 기울기 소실 문제가 적음\n",
    "        * Tanh(쌍곡탄젠트)함수 : 시그모이드 함수의 변형 -> -1~1까지의 출력값\n",
    "        * Softmax 함수 :  시그모이드랑 비슷하지만, **다중 클래스 분류 문제의 활성화 함수**로 사용됨 -> 출력값을 *확률*로 반환해 개별 클래스의 확률을 알 수 있음\n",
    "            * **Softmax 함수 + 크로스 엔트로피 손실 함수**?<br>\n",
    "            : 손실 함수가 예측의 확률(softmax 함수)과 정답 확률과의 차이(크로스 엔트로피 손실 함수)를 바로 계산할 수 있어 학습 효율이 높습니다.\n",
    "    * 다층 퍼셉트론: '정도' 표현 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48b75e",
   "metadata": {},
   "source": [
    "### 경사하강법과 옵티마이저 \n",
    "1. 경사하강법: 손실함수를 최소화하며 모델을 학습시키는 방법 = 오차를 가장 작게 만드는 가중치와 편향을 찾는 방법 <br>\n",
    "-> 그러나 데이터가 너무 방대한데 한번에 넣으면 시간이 많이 걸리니 ... => 배치\n",
    "2. 배치: 학습과정에서 가중치와 같은 매개변수를 조정하기 위해 사용하는 데이터의 묶음\n",
    "    * 배치 크기는 사용자가 임의로 정하는 하이퍼파라미터\n",
    "    * 배치 크기가 너무 크면 학습이 느려지고 메모리가 부족\n",
    "    * 배치 크기가 너무 작으면 가중치가 자주 업데이트 되어 학습이 불안정\n",
    "3. 이포크 : 모든 데이터 셋을 학습하는 횟수 (1이포크 = 모든걸 한번 풀어봄)\n",
    "4. 이터레이션 또는 스텝 : 한 번의 에포크를 끝내기 위해서 필요한 배치의 수\n",
    "    * **1이포크에서 다루는 데이터의 수 = 배치의 크기*이터레이션**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ce632",
   "metadata": {},
   "source": [
    "1. 배치 경사 하강법 : 가장 기본적인 경사 하강법, 오차를 구할 때 전체 데이터를 한 묶음으로 묶음\n",
    "    * 전체 데이터를 통해 학습-> 업데이트 획수가 적음\n",
    "    * 전테 데이터를 한 번에 해서 메모리가 많이 필요함\n",
    "    * 항상 같은 데이터에 대해서 경사를 구해 수렴이 안정적\n",
    "2. 배치 크기가 1인 확률적 경사 하강법 : 랜덤으로 선택된 하나의 데이터에 대해서만 계산 <br>\n",
    "-> 당연히 더 빠르게 계산 가능\n",
    "    * 하나의 데이터에 대해서만 메모리에 저장 -> 자원이 적은 컴퓨터에서도 쉽게 사용 가능\n",
    "    * 매개변수의 변경폭이 불안정, 정확도가 낮음\n",
    "3. 미니 배치 경사 하강법 : 배치가 크기가 1인 경사하강법은 정확도가 낮을 수 있기에!!!!<br>\n",
    "배치크기를 적절히 지정하여 해당 데이터 개수만큼에 대해 **매개변수의 값을 조정**\n",
    "    * 배치 사이즈는 보통 2의 n승에 해당하는 숫자로 선택\n",
    "    * model.fit()에서 배치크기를 별도로 지정해주지 않으면, 기본값은 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271fb02d",
   "metadata": {},
   "source": [
    "##### 옵티마이저: 경사하강법을 더 효율적이고 안정적으로 하기 위해선 옵티마이저가 필요\n",
    "1. 모멘텀 : 변수가 가던 방향으로 계속 가도록 속도를 추가한다. (관성) <br>\n",
    "* 경사 하강법에 계산된 접선의 기울기에 한 시점 전의 접선의 기울기값을 일정 비율 만큼 반영\n",
    "2. 알엠에스프롭 \n",
    "* 기존 경사하강법에서는! 기울기가 큰 방향에서는 너무 많이, 기울기가 작은 방향에서는 너무 작게 이동 => **학습률**을 조정해서 안정적으로 빠르게 학습\n",
    "* 기존 경사 하강법은 무조건 현재 기울기를 보고!\n",
    "* RMSProp에서는 최근 기울기를 참고해서, 급한 경사에서는 속도를 줄이고 완만한 경사에서는 속도를 유지하거나 증가 \n",
    "3. 아담 : RMSProp과 모멘텀을 합친 방향과 스텝 사이즈를 적절하게 합친 방법 => 방향과 학습률을 모두 잡기 위한 방법\n",
    "    * 각 파라미터의 1차 모멘트(기울기 평균) & 2차 모멘트(기울기의 제곱 평균)을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c0fd2",
   "metadata": {},
   "source": [
    "### 학습을 향상시키는 기법: 데이터 증강과 전이 학습\n",
    "1. 데이터 증강: 기존 데이터에서 다양한 변형을 만들어 학습 데이터를 늘리면 더욱 효과적인 학습이 가능함\n",
    "    * 어디에 도움될까?\n",
    "    1. 모델 성능의 향상: 다양한 변형 데이터를 학습함으로써 모델이 더욱 일반화된 패턴을 익히게 됨\n",
    "    2. 데이터 의존성 감소 : 원본 데이터와 매우 유사한 조건에서만 작동하는 것이 아니라, 다양한 패턴을 학습하도록 할 수 있음\n",
    "    3. 과접합 완화 : 적은 데이터 셋 -> 과적합 가능성이 큼! 따라서 약간 변형이 있어야 다양한 입력값을 가질 수 있음\n",
    "    4. 데이터 프라이버시 보호 -> 원본 데이터의 노출을 최소화하면서 성능은 향상\n",
    "2. 전이학습: 한 문제에서 학습한 지식을 다른 관련된 문제에 활용해서 모델의 성능을 높이는 방법 \n",
    "    1. 계산 비용 절감\n",
    "    2. 데이터셋 크기 문제 완화 -> 적은 데이터 셋도 성능 좋은 모델 가능\n",
    "    3. 일반화 가능성 향상\n",
    "    4. 성능의 향상\n",
    "    * 전이 학습의 핵심은 이미 트레이닝된 모델과 fine-tuning을 이용하는 것!\n",
    "3. 전체적인 워크 플로우<br>\n",
    "    1. Rre-trained-model: 일반적 특징 학습\n",
    "    2. fine-Tuning : 새로운 데이터에 맞춰 일부/전체 층 재학습\n",
    "        * 새로운 데이터셋과 문제에 맞춰 1을 조정하는 과정\n",
    "        * 초기 층은 그대로 두고, 출력층이나 후반 층만 재학습하거나, 모든 층을 새로운 데이터에 맞게 재조정\n",
    "        * **1의 일반적인 특징은 살리고, 새로운 문제에 특화된 정보를 학습하는 흐름**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e442c0",
   "metadata": {},
   "source": [
    "## 합성곱신경망\n",
    "1. 픽셀 \n",
    "    * 입력 이미지의 가장 기본적인 데이터 단위\n",
    "    * 이미지의 각 점에서 밝기/색상을 나타내는 값\n",
    "    * 작은 이미지를 직사각형 형태로 모은 것 \n",
    "    * 각 픽셀은 단색의 직사각형으로 전체 이미지 크기 = 세로 픽셀 수 X 가로 픽셀 수\n",
    "    * 이미지 데이터 저장시 2차원 배열로 표현\n",
    "2. 채널\n",
    "    * 색공간: 픽셀의 색을 숫자로 표현하는 방식\n",
    "    * 그레이 스케일 : 흑백 이미지, 채널값=1. 0=검은색, 커질수록 하얀색으로 명도를 나타내는 8비티의 부호없는 정수로 저장\n",
    "    * RGB : 컬러이미지, 채널값 =3,Red, Green, Blue 색의 명도를 뜻하는 숫자 3개가 합쳐진 벡터로 표현, 이미지 데이터는 3X 가로픽셀 수X 세로 픽셀수 형태의 3차원 배열로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1849c",
   "metadata": {},
   "source": [
    "### CNN <br>\n",
    ": 컴퓨터가 이미지 인식 어려우니 픽셀 단위로 구분해서, 각 픽셀에 대해 RGB 값이 적절히 조합괴어 하나의 픽셀의 색상 정보를 나타낸다. \n",
    "* 사람은 이미지 전체의 모든 부분을 하나하나 모지 않고, 물체가 존재하는 영역, 물체의 특징 위주로 본다 <br>\n",
    "* **이처럼 이미지 공간 정보를 유지한 상태로 학습이 가능한 방식이 CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315b796c",
   "metadata": {},
   "source": [
    "**CNN**\n",
    "* 위치별 특징을 추출하는 부분과 클래스를 분류하는 부분으로 나눌 수 있다.\n",
    "* 특징 추출영역은 \n",
    "    * Convolution Layer :입력 데이터에 필터를 적용 후 활성화 함수를 반영하는 필수 요소\n",
    "    * Pollung Layer :Convolution Layer 다음에 위치하며, 이미지의 공간 크기를 줄여 중요한 특징을 강조하는 역할. 모델의 계산 효율성을 높이고, 과적합을 방지하는 데 도움을 줌.\n",
    "* 이미지 분류를 하는 영역은 CNN 모델의 마지막 -> Fully Connected Layer로 구성\n",
    "* Feature Learning(Conv + Pool)과 Classification(FC Layer) 사이에 3차원을 1차원 배열 형태로 만드는 Flatten가 위치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57562d",
   "metadata": {},
   "source": [
    "즉, 이미지를 인식하기 위한 모델은 이미지의 **특징을 학습**하는 부분과 어떤 이미지인지 **분류하는 부분**으로 구성된다. \n",
    "\n",
    "* 이미지의 특징을 학습하는 부분\n",
    "\n",
    "    - CNN 모델에서 새로 추가된 부분.\n",
    "    - convolution과 pooling을 사용한다.\n",
    "\n",
    "* 이미지를 분류하는 부분\n",
    "\n",
    "    - 회귀, 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3ff62",
   "metadata": {},
   "source": [
    "#### 왜 이미지 처리에 MLP가 아닌 CNN을 쓸까?\n",
    "0. 이미지 데이터 구조 -> 픽셀 -> 따라서 이미지 구조를 유지해야 한다\n",
    "    * 픽셀은 0~255 사이의 개별 값으로 이루어져 있고, H X W 행렬로 나타낼 수 있음 \n",
    "1. 위치, 크기, 회전 등의 변화에 취약 \n",
    "    * MLP는 작은 움직임에도 입력값 변화가 크다 -> 가중치가 무력화됨 -> 위치변화에 따른 숫자 인식 능력이 크게 저하\n",
    "    * CNN은 Convolution으로 특징 추출(모델이 특징을 학습)-> polling 으로 차원 축소 -> 어떤 영역에 어떤 특징이 존재하는지를 보기 때문에 위치에 무관!\n",
    "2. 파라미터 폭발 -> 완전 연결 구조로 파라미터, 가중치 수 급증 -> 학습 느림\n",
    "    * CNN은 부분연결 & Convolution 연산 -> 이미지의 특정 피처에만 집중 -> 효율적\n",
    "3. 공간 정보 손실 -> MLP는 이미지를 1차원 벡터로 바로 변환하면서 이미지의 공간 정보가 많이 손실\n",
    "    * CNN 에서는 다차원 이미지를 그대로 입력받고 공간 정보를 보존하면서 학습!\n",
    "    * 특징을 추출하고 변환해서 손실 정도가 낮음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed72d30",
   "metadata": {},
   "source": [
    "### 합성곱과 풀링\n",
    ": 이미지 데이터는 공간적 관계와 지역적 패턴이 중요!\n",
    "1. 모델의 구조\n",
    "    1. 특징 추출: Convolution, Pooling\n",
    "    2. 클래스 분류: Fully Connected Layer가 담당\n",
    "2. 합성곱 : 전체 이미지를 일정 크기의 필터를 훑으면서 이미지의 특징을 학습\n",
    "    1. 필터 또는 커널이 이미지 위를 슬라이딩 (필터값은 가중치와 같은 역할)\n",
    "    2. 각 위치에서 요소별 곱하고 합계 계산\n",
    "    3. 결과를 피처 맵에 저장\n",
    "3. padding :필터를 거치면 정보를 잃어버리는 것을 막기위해 이미지 바깥을 0으로 둘러쌈\n",
    "4. stride : 이미지를 몇칸씩 점프하면서 훑을 것인지 결정하는 값 -> 보폭이 커질 수록 피처맵의 크기가 작아진다.\n",
    "5. pooling :피처 맵에서 특징을 추출하기 위해 이미지를 격자 형태로 분할한 뒤, 해당 영역 안에서 가장 큰 값 또는 평균값을 취하는것 (보통은 맥스 풀링 = 최댓값 선택)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25205ef6",
   "metadata": {},
   "source": [
    "**CNN 구조 정리**\n",
    "- Convolution과 Pooling을 이용해 특징 추출을 진행\n",
    "- 연산 과정에서 출력(feature map)의 크기와 정보 보존 정도를 조절하기 위한 하이퍼파라미터로 padding, stride가 활용됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17827c",
   "metadata": {},
   "source": [
    "### 이미지 딥러닝 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43acc5b2",
   "metadata": {},
   "source": [
    "#### 이미지 분류\n",
    "1. 아이디어 : 이미지를 수치 데이터로 인식 -> 사전 정의된 클래스 중 하나로 분류\n",
    "2. 모델별 정리\n",
    "    * AlexNet\n",
    "\n",
    "        * 특징: 비교적 단순한 구조의 초기 CNN 모델\n",
    "        * 장점: 작은 데이터셋이나 학습 개념 설명용으로 적절\n",
    "        * 주의사항: 최근 모델에 비해 정확도 낮음\n",
    "\n",
    "    * VGG\n",
    "        * 특징: 더 깊은 네트워크 구조와 블록 구조 적용\n",
    "        * 장점: 규칙적인 구조로 실험과 확장 용이\n",
    "        * 주의사항: 계산량 증가, 메모리 요구 높음\n",
    "\n",
    "    * ResNet\n",
    "        * 특징: Residual(잔차) 연결을 도입 → 매우 깊은 네트워크 학습 가능\n",
    "        * 장점: 깊은 구조에서도 기울기 소실 문제 완화\n",
    "        * 주의사항: 과적합 가능성 있음 → batch size, regularization 조절 필요\n",
    "\n",
    "    * Vision Transformer (ViT)\n",
    "        * 특징: Transformer 구조를 이미지에 적용, 이미지를 패치 단위로 입력\n",
    "        * 장점: 이미지와 순차 모델 결합, 대규모 데이터에서 높은 성능\n",
    "        * 주의사항: 데이터와 계산 자원 많이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2d824",
   "metadata": {},
   "source": [
    "### 객체 탐지\n",
    "1. 아이디어: 한 이미지에서 객체와 이 객체를 둘러싸는 가장 작은 직사각형으로 정의되는 bounding box를 찾는 작업<br>\n",
    "2. 모델 적용 방식\n",
    "    1. CNN을 통해 피처 맵을 얻고, 그 위에 두가지 헤드를 올림\n",
    "        * Classification Head: 객체의 클래스 예측\n",
    "        * Regression Head: 객체의 위치 (x, y, width, height) 좌표 예측\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db25ee",
   "metadata": {},
   "source": [
    "#### 이미지 캡셔닝\n",
    "1. 아이디어: 이미지 내에 있는 객체에 대한 판단 뿐만 아니라 객체들 간의 관계를 파악하고 자연어의 형태로 알맞게 표현\n",
    "2. 모델 적용방식: CNN(이미지 특징 추출)과 RNN(문장으로 나타냄)\n",
    "    * 방법\n",
    "    1. CNN(RGB 3채널의 인풋데이터를 통해)을 통한 특징 추출\n",
    "    2. Attention Network을 이용하여 단어 생성\n",
    "        * 중요한 부분에 가중치를 두어 단어를 추출\n",
    "        * 가중치가 높은 부분을 이용해 사진을 한 문장으로 설명할 수 있게 함\n",
    "    3. RNN을 이용해서 문장 완성 \n",
    "        * 가중치를 바탕으로 추출해낸 이미지의 주요 단어들을 RNN을 통해 조합"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
