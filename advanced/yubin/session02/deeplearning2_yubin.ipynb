{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df67b8f3",
   "metadata": {},
   "source": [
    "# 딥러닝2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ace11f",
   "metadata": {},
   "source": [
    "## 과거를 기억하는 신경망: RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b4e28d",
   "metadata": {},
   "source": [
    "### Sequence Data\n",
    "1. 순차 데이터란? : 순서에 의미가 있으며, 순서가 달라질 경우 의미가 손상되는 데이터\n",
    "    * 시간적 순차 데이터: 순차 데이터 중 순서가 '시간의 흐름'을 나타내는 경우\n",
    "    * 시계열 데이터: 시간적 순차 데이터 중에서 데이터가 '일정한 시간 간격'으로 기록되는 경우\n",
    "2. 순차 데이터의 예시\n",
    "    * 음성 및 오디오 데이터 (소리의 파형)\n",
    "    * 자연어 (문자,단락,문서 등 단어의 순서가 매우 중요한 대표적인 순차데이터)\n",
    "    * 생물학적 서열 데이터\n",
    "    * 비디오 데이터 (여러 프레임이 순서대로 나열된 형태)\n",
    "3. 기존 정형 데이터와의 차이점\n",
    "    * 일반적인 정형 데이터: 샘플(행)의 독립성(순서 바뀌어도 됨)| 특성(열) 순서의 독립성\n",
    "    * 순차 데이터의 특징: 순서 자체가 핵심 정보| 이전 시점 데이터가 다음 데이터에 영향을 미침\n",
    "\n",
    "##### 따라서, 기존 머신러닝 기법으로는 한계가 있기 때문에, 과거의 정보를 기억하고, 순서의 의미를 학습할 수 있는 특별한 신경망이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ba4d6",
   "metadata": {},
   "source": [
    "### RNN 구조와 원리\n",
    "1. RNN: 순환하는 구조를 가진, 순차 데이터 처리에 특화된 인공 신경망\n",
    "* 시퀀스 데이터를 입력받아 순서정보를 유지하며 처리한다.\n",
    "* 순환하는 은닉층이 매 시점의 은닉 상태를 업데이트 한다.\n",
    "    * 은닉층 : 신경망을 구성하는 물리적 요소로 정보를 어떻게 처리할 지에 대한 '규칙', '함수' 그 자체\n",
    "    * 은닉 상태: 특정 시점에 은닉층을 통과한 결과값, 숫자로 이루어진 벡터,  해당 시점까지의 중요 정보를 요약한 '메모리' 또는 '문맥'\n",
    "* 이전 시점의 값을 현재 시점으로 넘겨준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f97f6fc",
   "metadata": {},
   "source": [
    "2. RNN의 구조 <br>\n",
    ": 입력 -> 입력 가중치 -> 시간 스텝 t에서의 은닉 상태('메모리') -> 출력 가중치 -> 출력\n",
    "<br>\n",
    "* 기존 신경망과의 차이 : 순환 가중치의 유무  <br>\n",
    "입력층 -> 은닉층 -> 출력층의 구조였지만, RNN은 중간에 가중치가 들어감!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e73c47",
   "metadata": {},
   "source": [
    "=> 은닉 상태의 순환을 통해 과거의 정보를 다음 시점으로 계속해서 전달 ('정보를 저장하는 숫자 벡터를 전달한다는 점을 기억하자!)\n",
    "<br>\n",
    "<br>\n",
    "**<RNN 아키텍쳐>**\n",
    "1. One to One: 간단한 기계학습 문제\n",
    "2. One to Many: 이미지 캡셔닝\n",
    "3. Many to One: 감정 분석, 스팸 메일 분류, 시계열 데이터 예측\n",
    "4. Many to Many: 기계 번역\n",
    "5. Many to Many: Video recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6162e0f",
   "metadata": {},
   "source": [
    "3. RNN의 핵심 원리: 가중치 공유\n",
    "* 동일한 가중치를 시퀀스의 다양한 시점에서 반복적으로 적용\n",
    "* RNN이 시퀀스 데이터의 길이와 시점 위치에 상관없이 효과적으로 작동하게 하는 핵심 원리\n",
    "    * 가중치 공유의 이점은?\n",
    "    1. 학습 파라미터 수 감소 \n",
    "        : 공유하지 않으면 시퀀스 길이가 길어질 수록 학습해야 할 파라미터가 무한정 늘어나서 매우 비효율적인데, 학습할 파라미터 수가 일정하게 유지됨.\n",
    "    2. 일반화 능력 향상\n",
    "        : 아전 시점까지의 정보와 현재 시점의 입력이 주어졌을 때, 어떻게 메모리를 업데이트 할 것인가? 라는 **하나의 일반적 규칙을 학습**\n",
    "        <br> -> 훈련 데이터에 없던 새로운 길이의 시퀀스나 다양한 패턴에도 유연하게 대응 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee35c2",
   "metadata": {},
   "source": [
    "### RNN의 한계 및 장기 의존성 문제\n",
    "1. RNN의 한계점\n",
    "    * RNN 구조의 특성상 전체 시퀀스를 모두 읽은 후 역전파가 이루어짐\n",
    "    * 역전파를 구하는 과정에서 미분값이 반복적으로 곱해짐\n",
    "    * 가중치 관련...\n",
    "        * 기울기 소실: 시퀀스 뒤쪽의 오차가 앞쪽까지 제대로 전달되지 않을 수 있음. 또한 역전파과정에서 반복적으로 곱해지는 기울기의 크기가 1보다 작으면 기울기가 0으로 수렴\n",
    "        * 기울기 폭주: 시퀀스 뒤쪽의 오차가 역전파 과정에서 비정상적으로 커짐. 기울기의 크기가 1보다 크면 기울기가 무한대로 발산 -> 엉뚱한 예측을 하거나, 계산 불능 상태가 되는 것\n",
    "            * 해결 방법 -> Gradient Clipping : Gradient의 상한선을 정해서 발산을 피함\n",
    "        * 느린 훈련 시간: 계산 과정이 순차적으로 이루어져야만 하는 구조적 한계 -> 전체 시퀀스를 한 번에 병렬 처리 불가능\n",
    "            * CNN은 각 영역에 필터 연산을 '동시에' 적용해서 병렬로 빠르게 계산 가능\n",
    "    * 장기 의존성 문제: 시퀀스 앞 부분의 중요한 정보를 잊어버려 맥락 파악 능력이 급격히 저하되는 현상 | 단기 기억력은 준수했지만, '장기 기억력'에 문제 발생\n",
    "=> RNN의 기억력 한계를 극복하고 더 길고 복잡한 시퀀스 데이터를 안정적으로 다룰 수 있는 새로운 대안의 필요성! 즉, 중요한 정보는 오래 기억하고, 불필요한 정보는 잊는 똑똑한 메모리 구조가 필요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214fa07",
   "metadata": {},
   "source": [
    "## LSTM과 GRU\n",
    "1. LSTM : 단기기억과 장기기억을 모두 가지고 있음! \n",
    "    * 행동 방식\n",
    "        * Forget Gate: 과거를 잊어버리자!!\n",
    "        * Input Gate: 이건 기억하자!!\n",
    "        * Output Gate: 어떻게 행동할지는 보여주는 것\n",
    "    * 진짜 속마음과 겉모습을 따로 분리해서 관리함. 매우 정교하지만, 그만큼 고민이 복잡하다는 특징이 있음\n",
    "2. GRU : 속마음과 겉모습이 크게 다르지 ㅇ낳고, 하나의 상태로 통합되어 있음\n",
    "    * Update Gate: 과거 경험과 새로운 자극을 비율로 섞어서 행동\n",
    "    * Reset Gate: 새로운 자극이 강렬하면, 과거 기억은 잠시 무시함\n",
    "* **결국, GRU는 기억과 행동이 하나로 통합되어 있음. LSTM보다 훨씬 단순하고 즉각적인 반응이기에 대부분의 상황에서는 GRU가 효율적일 것임. 그러니, LSTM에 비해서 정교함은 떨어짐**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db771fe6",
   "metadata": {},
   "source": [
    "### LSTM : 게이트로 정보의 흐름을 제어함. <br>\n",
    "1. 앞의 정보가 뒤로 충분히 전달되지 못하는 RNN의 기울기 소실 문제를 해결하기 위한 모델 구조\n",
    "2. LSTM을 마스터 해보겠슨\n",
    "    * 핵심 아이디어: 기억할 내용과 잊어버릴 내용을 선택해서, 중요힌 정보를 오래 가져감 -> GATE를 통해 곱셈을 덧셈으로 바꾼다.\n",
    "    * RNN과 주요차이: c(장기기억)와 h(단기기억), 2개의 순환되는 층을 사용\n",
    "    * Gate를 통해 필요한 정보들만 통과\n",
    "         * Forget Gate: 얼마나 잊어버릴지 결정\n",
    "         * Input Gate: 현재 정보를 얼마나 사용할 지 결정\n",
    "         * Output Gate: 다음 층으로 어떤 정보를 전달할 지 결정\n",
    "    * Final memory cell : Forget, Input gate를 결합해서 *현재 정보를 얼마나 기억하지 계싼*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78233b42",
   "metadata": {},
   "source": [
    "1. LSTM의 장점: 기억할 정보와 잊을 정보를 나눠서 관히하면서, 이전의 정보가 중요하다다면 완전히 보존되는 형태로 은닉층을 갱신하기 때문에, vanishing gradient problem 효과적으로 완화\n",
    "2. LSTM의 단점: 매우 복잡하기 때문에, RNN보다 학습 파라미터가 많아짐!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36dd12",
   "metadata": {},
   "source": [
    "### GRU : 더 단순하고 효율적인 구조\n",
    "1. LSTM에서 조금 더 발전한 것으로 별도의 메모리 셀 없이 게이트 수를 줄여 구조를 간소화\n",
    "2. LSTM과 GRU의 차이\n",
    "    * foget gate와 input gate를 하나의 update gate로 합침\n",
    "    * Reset gate를 사용\n",
    "    * Gate의 개수가 3개에서 2개로 줄어 학습 시간이 줄어듦\n",
    "    -> 학습 시간은 줄어들지만 성능은 LSTM과 유사하거나 더 좋다\n",
    "        - **update gate**:  이전 상태와 현재 상태를 얼마만큼의 비율로 반영할지\n",
    "        - **reset gate** : 이전 상태를 얼마나 반영할지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c1c3cb",
   "metadata": {},
   "source": [
    "1. GRU의 장점: 성능과 효율성 사이의 균형이 좋다\n",
    "2. GRU의 단점: 긴 시퀀스 처리에서의 한계는 여전히 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70704e",
   "metadata": {},
   "source": [
    "### LSTM VS GRU: 언제 무엇을 선책할까?\n",
    "1. LSTM이 적합한 경우\n",
    "- 긴 시퀀스에서 문맥 이해가 중요한 작업\n",
    "- 데이터 양이 충분하고 복잡한 패턴을 학습해야 하는 경우\n",
    "- 모델 성능이 계산 효율보다 더 중요한 경우\n",
    "- 기계 번역, 언어 모델링, 장기 시계열 예측 (예: 금융, 기후 데이터)\n",
    "2. GRU가 적합한 경우 \n",
    "- 자원이 제한적이거나 학습 속도가 중요한 경우\n",
    "- 데이터 양이 적고 과적합 위험이 있는 경우\n",
    "- 실시간 예측이 필요한 응용\n",
    "- 사용 예시: 음성 인식, 스트리밍 데이터, 실시간 비디오 분석, 짧은 시퀀스 기반의 텍스트 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf7983",
   "metadata": {},
   "source": [
    "## Seq2Seq : 문장을 입력받아 문장을 출력하다\n",
    "### Seq2Seq 기본 구조\n",
    "1. 정의: 한 시퀀스를 다른 시퀀스로 변환하는 작업을 수행하는 딥러닝 모델로, 대표적으로 기계 번역에서 사용된다.\n",
    "1. 인코더: 입력 문장(input Sequenxe)을 받아서 그 의미를 **하나의 핵심 요약(Context vector)**으로 압축 \n",
    "        * 컨텍스트 벡터는 float형의 벡터 형태이며, 인코더 중 마지막 시점의 은닉 상태(hidden state)를 의미\n",
    "    *  t시점의 임베딩된 단어의 입력과 t-1 시점의 은닉 상태(hidden state)가 RNN/LSTM의 입력으로 주어지며, 그 결과로 t시점의 은닉 상태가 나온다.\n",
    "2. 디코더: 컨텍스트 벡터를 받아서 새로운 문장이나 행동을 만들어낸다. -> 출력\n",
    "    * 인코더가 보내준 컨텍스트 벡터는 디코더의 첫번째 은닉 상태에 사용\n",
    "    * t시점에 나온 출력값이 그 다음 시점의 입력값으로 넘어간다.\n",
    "3. 핵심 특징은?\n",
    "    * 입력과 출력의 길이가 달라도 된다\n",
    "    * 인코더와 디코더가 분리되어 있다\n",
    "    * 문맥 벡터라는 다리를 건넌다 -> 완전 알잘딱깔센 요약핑\n",
    "    * 인코더와 디코더 모두 그림과 같이 **RNN(or LSTM/GRU)를 여러개 조합한 형태**이다.\n",
    "    * 모든 단어는 워드 임베딩(word2vec) 과정을 거친다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5206f9",
   "metadata": {},
   "source": [
    "### Seq2Seq의 한계\n",
    "* 고정된 context Venctor에 소스 문장의 정보를 압축하면서 -> 더 구체적으로 말할 수 있는데 그런 정보들이 싹 다 사라짐 -> **병목현상 발생**\n",
    "* 기울기 소실 문제\n",
    "* 병렬화 불가능 문제(순서대로 입력해야함) <br>\n",
    "-> 이래서... 다음 시간 투 비 컨티뉴...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff24488",
   "metadata": {},
   "source": [
    "# 정리\n",
    "1. RNN : 순차 데이터 처리를 하기 위해 과거의 정보를 기억하지만, 정보가 길어질 수록 앞의 내용을 잊어버리는 장기 의존성 문제\n",
    "2. LSTM, GRU : 장기 의존성, 기울기 소실 문제 해결하려고 등장! Gate를 사용해서 C, h를 나눠서 정보를 기억하는 선택적인 방식 사용!\n",
    "3. Seq2Seq: 인코더-디코더인데 긴문장-> 요약(요약은 인코더 마지막에 있는 컨텍스트 벡터가 하지롱) 시 정보가 소실되는 병목현상 발생"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
