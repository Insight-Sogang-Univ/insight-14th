{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f1ae36",
   "metadata": {},
   "source": [
    "# 언어 모델 & 생성 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed281a",
   "metadata": {},
   "source": [
    "## 1. 언어모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8c99f",
   "metadata": {},
   "source": [
    "### 1-1. 언어 모델이란?\n",
    "1. 언어모델: 언어 모델은 단어 시퀀스(문장)에 **확률**을 할당하여, 가장 자연스러운 단어 시퀀스를 찾아내는 모델 \n",
    "    * 자연스러움은 어떻게 판단?: 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b3900",
   "metadata": {},
   "source": [
    "### 1-2. SLM : 통계적 언어 모델\n",
    "1. Statistic Language Model 이란? : 분포 가설이라는 아주 중요한 아이디어에 근거한다\n",
    "* 분포가설이란? \"비슷한 문맥에서 나타나는 단어들은 비슷한 의미를 가진다\"\n",
    "* 이러한 분포가설을 설명해주는 기본 개념 = **조건부 확률**, **카운트기반 확률**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a7eeb",
   "metadata": {},
   "source": [
    "1. 조건부 확률 : A가 일어나는 전제하에 B가 일어날 확률 = P(B|A)\n",
    "* 예: P(오늘, 날씨가, 좋다) = P(오늘) X P(날씨가|오늘) X P(좋다|오늘, 날씨가)\n",
    "    * 이 확률값이 높을 수록 모델은 해당 문장을 더 자연스럽다고 판단한다.\n",
    "2. 카운트 기반 확률 : 이전 단어 시퀀스의 등장 빈도를 통해 다음 단어의 확률을 계산\n",
    "* 예: P(좋다|오늘 날씨가) = ('오늘 닐씨가 좋다'가 함께 등장한 횟수)/('오늘 날씨가'가 등장한 총 횟수)\n",
    "    * 직관적이지만 \"희소 문제\"라는 명확한 한계를 가짐\n",
    "    * 희소문제란?: 모델이 처리하는 데이터의 양은 한정되어 있지만, 모델이 처리해야 할 현실의 언어는 거의 무한대에 가까움 -> **가지고 있는 데이터가 부족해서 언어를 정확하게 모델링 하지 못하는 문제**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70b84a",
   "metadata": {},
   "source": [
    "#### 따라서!!! 언어 모델의 패러다임이 통계 기반 모델에서 인공 신경망 기반 모델로 넘어가는 결정적인 계기가 되었다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aba9a5",
   "metadata": {},
   "source": [
    "2.  N-Gram\n",
    "    1. 배경:통게적 언어모델이지만, 바로 앞 단어를 전체를 참고하던 기존 방식과 달리, 일부 단어만 참고하여 희소 문제를 완화하려는 아이디어에서 출발\n",
    "        * 일부 단어만 참고하면 희소 문제거 완화된다. -> 참고하는 단어의 수를 줄이기 때문\n",
    "        * 즉 단어의 범위를 n개로 정해놓고 확률을 계산하는 방식\n",
    "    2. 정의: 연속된 n개의 단어 묶음(시퀀스) -> n개로 잘라 **토큰화**\n",
    "    3. N-gram을 이용한 확률 계산 : **n-1개의 단어만 참고하여 예측**\n",
    "    4. 한계\n",
    "        1. 여전한 희소 문제\n",
    "        2. n을 선택하는 것의 trade-off 문제\n",
    "            * n을 크게 하면?: 더 많은 문맥을 고려하므로 정확도가 높아질 수 있지만, 희소 문제가 심해짐\n",
    "            * n을 작게 하면?: 희소 문제는 줄어들지만, 너무 적은 문맥만 고려하여 예측의 정확도가 떨어짐\n",
    "            * 즉 무작정 n을 늘린다고 성능이 좋아지진 않음 (일반적으로 5를 넘지 않아야 함)\n",
    "        3. 문맥 파악의 한계: n의 범위를 벗어난 단어는 전혀 고려하지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbfb09e",
   "metadata": {},
   "source": [
    "3. Perplexity(PPL) -> **테스트 데이터를 이용해 언어 모델의 성능을 빠르고 정량적으로 파악하기 위한 지표**\n",
    "1. PPL이란?: 언어 모델이 특정 문장을 얼마나 **혼란스러워 하는지**를 나타내는 수치, 즉 **헷갈리는 정도**를 의미함 => 낮을 수록 좋다 = 언어 모델의 성능이 좋다\n",
    "2. 수식: 문장의 확률에 역수를 취한지, 문장의 단어 수(N)로 N제곱근을 한 값\n",
    "3. 영향을 주는 요인들\n",
    "    1. 문장의 희소성 : W가 희소하게 등장 -> PPL이 커짐\n",
    "    2. N의 개수: N이 커지면 -> PPL이 낮아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba76f2",
   "metadata": {},
   "source": [
    "### 1-3. 딥러닝 기반 언어모델\n",
    "1. LLM : 대규모 언어 모델이 방대한 양의 데이터를 학습하여, 인간의 언어를 이해하고 생성하며 요약하는 등 다양한 작업을 수행하도록 설계된 인공지능 모델\n",
    "    * LLM의 발전과정 : SLM(국소 텍스트 데이터) -> NLM(단어, 문장 완성 등) -> PLM(대규모 데이터 셋) -> LLM\n",
    "    * AI 서비스란?: LLM을 핵심 두뇌로 사용하고, 여기에 여러 도구를 결합한 더 큰 시스템을 의미\n",
    "    * LLM의 작동 방식: 학습, 예측\n",
    "        1. 대규모 학습: 방대한 양의 데이터를 학습한다(언어의 패턴까지 스스로 습득)\n",
    "        2. 예측 및 생성: 사용자의 질문이나 지시를 받으면, 학습한 패턴을 기반으로 다음에 올 확률이 가장 높은 단어를 순서대로 예측하며 응답을 생성\n",
    "    * LLM 사용 사례 : 텍스트 생성, 기계 번역, 질의응답, 문서요약, 감정분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711fea6",
   "metadata": {},
   "source": [
    "2. BERT : 텍스트의 앞뒤 문맥을 동시에 고려해서 입력값을 벡터로 바꿔주는 딥러닝 모델\n",
    "* **트랜스포머의 인코더 부분을 활용해 만든 양방향 언어 모델**\n",
    "* 특징\n",
    "    1. 양방향: 문장을 읽을 때 앞 뒤를 동시에 고려\n",
    "    2. 트랜스포머 기반: 인코더 부분만 사용\n",
    "    3. 사전 학습 + 파인 튜닝 : 큰 데이터로 먼저 일반적인 언어능력을 배우고, 나중에 특정 과제에 맞게 살짝 조정 = MLM과 NSP를 통해 사전 학습\n",
    "* 구조: 트랜스포머의 인코더를 쌓아올린 구조\n",
    "* 처리 과정: input -> 임베딩 -> 인코더(12개 혹은 24개) -> output\n",
    "    * 임베딩: 입력된 단어들을 숫자 벡터로 바꿔서 모델을 이해하게 만들어주는 것\n",
    "        * 토큰 임베딩 : 단어 자체 의미\n",
    "        * 세그먼트 임베딩: 두 개이상의 문장이 들어왔을 때 어떤 문장인지 구분\n",
    "        * 포지션 임베딩: 몇번째 위치인지 위치정보 포함\n",
    "        * **세 개의 정보를 더해서 하나의 입력벡터 생성**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418019a2",
   "metadata": {},
   "source": [
    "사전학습 1: MLM = 문장 속 단어를 mask 해놓고 그게 뭔지 맞추는 방식의 학습\n",
    "* 이 과정을 통해 자연스럽게 문맥을 이해하고 추론하는 능력이 길러짐\n",
    "* 마스킹 비율과 방식\n",
    "    * 사전 훈련을 위해 들어가는 input 텍스트의 15%정도의 단어를 랜덤으로 마스킹\n",
    "    * 마스킹하도록 선택된 단어에 대해 세 가지 방식을 섞어 사용\n",
    "        * MASK 로 바꾸기\n",
    "        * 랜덤 단어로 바꾸기\n",
    "        * 그대로 두기 -> 정답 단어를 보는 경우도 훈련에 포함\n",
    "            * 이렇게 다양한 상황을 줘서 모델이 MASK에만 너무 의존하지 않도록 일반화 유도\n",
    "            * 이후 양방향 학스블 통해 mask안된 부분도 학습하며 언어의 문맥을 이해하게"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d2a7c",
   "metadata": {},
   "source": [
    "사전학습 2: NSP = 두 문장이 실제로 이어지는 문장인지 아닌지 맞히는 학습\n",
    "* 문장 간 관계 이해 능력을 기르기 위함\n",
    "<br>\n",
    "**MSM는 단어를 가리고 문맥으로 맞추는 방식, NSP는 두 문장이 실제로 이어지는지 판단하는 방식**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d86c98d",
   "metadata": {},
   "source": [
    "BERT 이후 모델은 뭐가 있을까?\n",
    "1. RoBERTa : BERT의 사전 훈련 프로세스를 개선하여 성능을 향상시킨 모델\n",
    "* 더 거대한 데이터셋으로 더 긴시간 훈련 가능! NSP제거! 마스킹 패턴을 동적으로 변경해서 다양한 패턴 학습\n",
    "2. ALMERT : BERT의 경량화 버전\n",
    "* 파라미터 공유 | 임베딩 행렬 분해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89724d8",
   "metadata": {},
   "source": [
    "3. GPT \n",
    "* 트랜스포머 기반으로 Open AI라는 기업에서 만든 LLM 모델\n",
    "* 지피티 아키텍쳐\n",
    "    * 지피티 2 아키텍쳐가 사실상 표준이 되었으며, 3부터는 기본 구조 동일\n",
    "    * 지피티는 트랜스포머의 디코더만 사용\n",
    "    * 셀프 어텐션은 마스크 어텐션으로만 동작 \n",
    "        * 미래 단어를 볼 수 없도록 마스킹처리(왼쪽 컨텍스트)만 참고하게 만듦\n",
    "    * 동작 원리: 앞 부분 단어들을 기반으로, 다음에 올 단어의 확룰 분포를 계싼\n",
    "    * 확률이 가장 높은 단어를 선택하면서 문장을 순차적으로 생성\n",
    "    * BERT와의 차이 \n",
    "        * 버트가 문장 전체를 보고 빈칸을 채우는 양방향, 이해 중심 이라면\n",
    "        * GPT는 앞부분 단어만 보고 다음 단어를 생성하는 방식\n",
    "* LLM의 한계\n",
    "    * 환각: 사실과 다른 내용을 그럴 듯 하게 만들어내는 현상\n",
    "    * 업데이트 비용이 막대함: 파라미터도 많고... GPU도 많이 필요함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be3dbc",
   "metadata": {},
   "source": [
    "3. RAG : LLM 모델을 매번 새로운 데이터로 재학습하는 것은 매우 비효율적임 + 환각 문제\n",
    "    1. 정의: 관련된 정보를 검색하여 먼저 찾고, 이를 LLM에 같이 넣어 최종 답변을 생성하는 하나의 파이프라인\n",
    "    2. RAG의 구조\n",
    "        1. 질의 인코더\n",
    "        * 사용자의 질문을 이해하기 위한 언어모델\n",
    "        * 주어진 질문을 벡터형태로 인코딩\n",
    "        2. 지식 검색기\n",
    "        * 인코딩된 질문을 바탕으로 외부지식 베이스에서 관련 정보를 검색\n",
    "        3. 지식 증강 생성기\n",
    "        * 검색된 지식을 활용하여 질문에 대한 답변을 생성하는 언어모델\n",
    "        * 기존의 LLM과 유사하지만, 검색된 지식을 추가 입력으로 받아 보다 정확하고 풍부한 답변 생성 가능\n",
    "    3. 장점\n",
    "    * 풍부한 정보 제공\n",
    "    * 실시간 정보 반영: 최신 데이터를 검색하여 반여람\n",
    "    * 환각방지 : 실제 데이터에 기반한 답변을 생성함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fcccc1",
   "metadata": {},
   "source": [
    "4. CAG \n",
    "    * RAG는 외부 문서를 검색해 모델에 제공함으로써 지식 부족 문제와 환강을 어느 정도 완화할 수 있지만, 검색된 문서 중 일부가 신뢰성이 낮거나 잘못된 정보일 경우 답변의 품질이 떨어질 수 있음\n",
    "    * CAG는 잘못된 컨텍스트가 모델 성능에 미치는 부정적 영향을 최소화하고 모델이 문서를 단순히 활용하는 수준을 넘어 **문서의 신뢰도를 스스로 판단하고 활용할 수 있도록 훈련시킴**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af257239",
   "metadata": {},
   "source": [
    "5. LANGCHAIN\n",
    "* LLM +RAG 직접 구현이 어려우니까 모델 파이프라인 쉽게 구성하도록 도와주는 역할을 하는 것이 랭체인 프레임 워크\n",
    "    * LLM 기반으로 앱 만들 때 필요한 구성 요소를 쉽게 연결\n",
    "    * 다양한 라이브러리 활용 가능\n",
    "* 랭체인 특징\n",
    "    1. 추상화 : 필요한 각종 작업을 간결하게 표현해주고 간소화 해줌\n",
    "    2. 표준화 : 비슷한 기능을 갖추고 있는 요소들을 똑같은 형식을 갖춘 컴포넌트로 표준화\n",
    "        * 다양한 AI 모델을 하나의 형식으로 통일\n",
    "        * 다양한 원본 데이터를 표준화된 컴포넌트로 관리\n",
    "    3. 체이닝\n",
    "        * 자주 활용하는 주요 컨포넌트를 쉽게 연결해서 LLM 서비스의 로직을 쉽게 파악\n",
    "        * |로 쉽게 연결 가능\n",
    "\n",
    "* 랭그래프\n",
    "1. 랭체인 한계 : 중간에 조건에 따라 다른걸 하거나 이전 단계로 돌아가기가 어려움\n",
    "2. 랭그래프: 비선형적인 작업형태로 사이클, 루프를 만들 수 있음\n",
    "    * 노드: 작업 수행 단위 -> 랭체인의 컴포넌트를 그대로 가져와 노드로 사용\n",
    "    * 엣지: 노드를 연결\n",
    "    * 반복/순환 로직이 필요할 때 랭그래프를 사용\n",
    "3. 랭스미스: LLM은 사용자의 질문이 어떤 프롬포트를 거쳐 LLM에게 전달되는지 파악이 어려움\n",
    "    * 핵심기능\n",
    "        1. 디버깅 및 추적: 모든 과정을 단계별로 시각화\n",
    "        2. 테스트 및 평가: 미리 준비된 질문과 정답 데이터셋을 만들어 LLM 성능을 평가\n",
    "        3. 모니터링: 사용자들이 어떻게 사용하는지 모니터링\n",
    "        3. 프롬프트 허브: 다양한 프롬프트를 저장하고, 버전을 관리하며, 쉽게 테스트할 수 있는 공간 제공\n",
    "4. 외부 API활요하기\n",
    "* LLM 내부 지식만으로 해결할 수 없는 질문에 대해 외부 API를 연동해서 답변\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d9ac81",
   "metadata": {},
   "source": [
    "### sLM \n",
    ": 소규모 언어모델 = 자연어 콘텐츠를 처리, 이해, 생성할 수 있느 AI모델로 규모와 범위가 작음\n",
    "* 장점:  경량화, 빠른 작업 속도, 적은 비용, 프라이버시에 유용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a3bcc",
   "metadata": {},
   "source": [
    "1. 모델 압축\n",
    "    1. 가지치기: 신경망에서 중요도가 낮거나, 중복되거나, 불필요한 매개변수를 제거하는 방법 -> 뉴런 자체 또는 신경망 계층의 매개변수를 가지치기\n",
    "    2. 양자화: **고정밀 데이터를 저정밀 데이터로 변환** -> 계산 부하를 줄이고, 추론 속도를 높임\n",
    "    3. 지식 증류: 사전 학습된 \"교사 모델\"의 학습 내용을 \"학생 모델\"로 이전시키는 방법\n",
    "        * 학생 모델의 예측을 교사 모델의 예측과 일치시키고, 기본 추론 과정을 모방하도록 학습!\n",
    "2. 장점\n",
    "    * 프라이버시 : 로컬에서 함\n",
    "    * 비용절감 : 필요 자원이 적다\n",
    "    * 효율성 및 맞춤화 \n",
    "3.한계\n",
    "    * 편향으로부터 학습 -> 성능 저하\n",
    "    * 제한된 일반화: 광범위한 지식 기반이 부족해 특정작업에 더 적합하고, 포괄적이거나 복잡한 지식이 필요한 작업에는 성능이 떨어질 수 있음\n",
    "    * 환각\n",
    "    * 성능과 용량이 부족 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd94bd",
   "metadata": {},
   "source": [
    "LLM과 sLM의 결합: sLM이 어려운 문제를 풀 때, 막히는 부분만 LLM에게 선택적으로 도움 받아 해결하는 방식\n",
    "* 작동원리 \n",
    "    1. sLM이 먼저 풀고\n",
    "    2. 각 단계 쳥가하고 점수 매기고\n",
    "    3. 어려운 부분만 큰 모델이 도와준다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5604a5f6",
   "metadata": {},
   "source": [
    "## 생성 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec774b",
   "metadata": {},
   "source": [
    "### 1. 생성모델 \n",
    ": 주어진 학습 데이터를 학습하여, 학습 데이터의 분포를 따르는 유사한 데이터를 생성하는 모델\n",
    "* 대부분 분류 모델 (신경망모델 - 회귀, 분류, 로지스틱 회귀분석 등)을 공부했음\n",
    "* 분류 모델을 학습시킬 때, 훈련 데이터의 각 샘플에는 정답 레이블이 있음\n",
    "* 반면 생성 모델은 샘플에 레이블을 지정하는 데는 관심이 없음\n",
    "* 생성 모델은 입력 데이터의 확률 분포를 알고자 하는 것\n",
    "1. 생성모델의 분류\n",
    "`   * 명시적 확률 밀도 모델: 분포 기반으로 생성\n",
    "        * 정확히 계싼가능하면 fully visible belief networks 등\n",
    "        * 정확히 계산 불가능하면 근사적 likehood 기반\n",
    "    * 암시적 확률 밀도 모델: 분포와 상관없이 생성\n",
    "        * Markov Chain\n",
    "        * Direct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7873d72",
   "metadata": {},
   "source": [
    "### 2-2. AE, VAE\n",
    "1. AE : 오토인코더는 입력과 동일한 출력을 만드는 것을 목적으로 하는 신경망\n",
    "    * 차원 축소, 특징 추출, 노이즈 제거, 이상탐지\n",
    "    * 구조\n",
    "        * 앞단: 인코더 : 원래의 고차원 입력 데이터를 잠재 표현으로 변환하는 네트워크\n",
    "            * 인코더가 압축한 z를 잠재벡터라고 부름. => 중요한 정보가 압축되어있음\n",
    "        * 뒷단: 디코더: 잠재표현을 풀어서 입력을 재복원\n",
    "            * 차원 축소가 잘 되었는지 검증\n",
    "        * z: 원본 데이터를 저차원으로 압축하여 함축된 정보를 저장\n",
    "    * 결국 인코더로 들어가는 입력데이터와 디코더가 출력한 데이터가 같아지도록 AE를 훈련해야 한다.\n",
    "2. VAE : 랜덤 노이즈로부터 원하는 영상을 얻을 수 없을까?라는 의문에서 시작 \n",
    "    * 데이터를 잠재 공간으로 인코딩한 다음 그곳에서 다시 데이터를 디코딩하여 원본 데이터와 유사한 결과를 생성하는 방식\n",
    "    * 이미지 생성, 텍스트 새엉, 신호 처리 등 새로운 이미지 생성 작업에 많이 사용\n",
    "    * 구조\n",
    "        * 인코더: 고차원의 입력 데이터를 단일 벡터가 아닌 분포로 변환하는 것\n",
    "            * 잠재 변수의 평균과 표준편차를 예측함\n",
    "        * 잠재벡터: 원본 데이터를 저차원으로 압축하여 함축된 정보를 저장하고 있는 벡터\n",
    "        * 디코더: 샘플링된 잠재 표현을 풀어서 입력을 재복원\n",
    "3. 비교\n",
    "    * AE의 목적은 인코더\n",
    "    * VAE의 목적은 디코더 -> 잠재코드의 값을 하나의 숫자로 나타내는 것이 아니라 *가우시안 확률분포에 기반한 확률값*으로 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea7f0a",
   "metadata": {},
   "source": [
    "### 2-3. GAN\n",
    "* 생성자 신경망과 판별자 신경망이 적대적으로 경쟁하면서, 훈련을 통해 자신의 작업을 점점 더 정교하게 수행하는 신경망 모델\n",
    "    * 생성자: 진짜 분포에 가까운 가짜 분포를 생성하는 것\n",
    "    * 판별자: 가짜분포에 속하는지 진짜 분포에 속하는지 결정\n",
    "    * 궁극적 목적: **실제 데이터의 분포에 가까운 데이터를 생성하고, 경계를 최적 솔루션으로 간주**\n",
    "* 구조 : 서로 대립하는 2개의 신경망으로 나뉨\n",
    "    * 생성자의 목적: 품질이 높은 위조품을 만드는 것\n",
    "    * 판별자의 목적: 생성자가 만든 위조품을 정확하게 판별\n",
    "* 장점: 진짜 같은 가짜를 생성할 수 있음\n",
    "* 한계\n",
    "    * 학습이 불안정함 -> 에포크가 커질 수록 진동이 엄청나게 일어나서 안정화가 되지 않음\n",
    "    * 끝없는 숨바꼭질 : 생성자가 좋아지면 판별자도 좋아지면서 global optimim에 수렴 X\n",
    "    * mode collasping:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd0d7b",
   "metadata": {},
   "source": [
    "### 2-4 확산 모델\n",
    ": 입력 이미지에 노이즈를 여러 단계에 걸쳐 추가하고, 여러 단계에 걸쳐 노이즈를 제거함으로써 입력 이미지와 유사한 확률 분포를 가진 결과 이미지를 생성하는 것\n",
    "1. 순확산 : 데이터에 점진적으로 노이즈를 추가\n",
    "    * 원본데이터 -> 노이즈 추가 -> 완전한 노이즈 데이터(까매진당...)\n",
    "2. 역확산 : 노이즈 데이터에서 원본 데이터를 재구성하는 과정\n",
    "*   * 노이즈 데이터 -> 노이즈 제거 -> 원본 데이터 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e17ef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
