{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/Insight-Sogang-Univ/insight-14th/blob/main/advanced/template/session05/assignment_languagemodel.ipynb\" target=\"_parent\">\n",
        "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "8BzWQexmpCCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ” ì´ë¡  ë¬¸ì œ"
      ],
      "metadata": {
        "id": "oaPYWHM5Y0Qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. SLM(í†µê³„ì  ì–¸ì–´ëª¨ë¸)ì˜ í•œê³„ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”."
      ],
      "metadata": {
        "id": "dafmjcvzoMh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[1ë²ˆ ë¬¸ì œ ì •ë‹µ]**\n",
        "\n",
        "***ì—¬ê¸°ì— ì •ë‹µì„ ì¨ì£¼ì„¸ìš”!*** \\\n",
        "[ì •ë‹µ]\n"
      ],
      "metadata": {
        "id": "YdXgXdCqoQgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. BERTì— ëŒ€í•œ ì„¤ëª…ì´ ì•„ë‹Œ ê²ƒì€?\n",
        "\n",
        "a) ì‚¬ì „ í•™ìŠµ ëª¨ë¸\n",
        "\n",
        "b) ì–‘ë°©í–¥ ë¬¸ë§¥ ì´í•´\n",
        "\n",
        "c) íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë””ì½”ë”ë¥¼ ìŒ“ì•„ ì˜¬ë¦° êµ¬ì¡°\n",
        "\n",
        "d) ì´í›„ RoBERTaì™€ ALBERTë¡œ ë°œì „"
      ],
      "metadata": {
        "id": "U8cvXUjfoY0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[2ë²ˆ ë¬¸ì œ ì •ë‹µ]**\n",
        "\n",
        "***ì—¬ê¸°ì— ì •ë‹µì„ ì¨ì£¼ì„¸ìš”!*** \\\n",
        "[ì •ë‹µ]"
      ],
      "metadata": {
        "id": "MuSdngcrob_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. BERTì™€ GPTì˜ í•™ìŠµ ë°©ì‹ ì°¨ì´ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”."
      ],
      "metadata": {
        "id": "AavVEOYMoiXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[3ë²ˆ ë¬¸ì œ ì •ë‹µ]**\n",
        "\n",
        "***ì—¬ê¸°ì— ì •ë‹µì„ ì¨ì£¼ì„¸ìš”!*** \\\n",
        "[ì •ë‹µ]"
      ],
      "metadata": {
        "id": "fkhYXD6jolM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. LLMì´ ì§€ë‹Œ í•œê³„ì™€ RAGì˜ í•„ìš”ì„±ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„œìˆ í•˜ì„¸ìš”."
      ],
      "metadata": {
        "id": "F36oD9dOouGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[4ë²ˆ ë¬¸ì œ ì •ë‹µ]**\n",
        "\n",
        "***ì—¬ê¸°ì— ì •ë‹µì„ ì¨ì£¼ì„¸ìš”!*** \\\n",
        "[ì •ë‹µ]"
      ],
      "metadata": {
        "id": "Ra1jp2PfoxSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. LLMì„ ê²½ëŸ‰í™”í•œ ëª¨ë¸ì¸ sLMì„ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ëŒ€í‘œì ì¸ ê¸°ìˆ  3ê°€ì§€ë¥¼ ì ìœ¼ì„¸ìš”.(ë‹¨ë‹µí˜•)"
      ],
      "metadata": {
        "id": "ci0-WuuQo2pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[5ë²ˆ ë¬¸ì œ ì •ë‹µ]**\n",
        "\n",
        "***ì—¬ê¸°ì— ì •ë‹µì„ ì¨ì£¼ì„¸ìš”!*** \\\n",
        "[ì •ë‹µ]"
      ],
      "metadata": {
        "id": "ZCb-JWJVo5mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ ì–¸ì–´ ëª¨ë¸ ê³¼ì œ"
      ],
      "metadata": {
        "id": "6InYiHirZzVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ§® GPU ì‚¬ìš©í•˜ê¸°"
      ],
      "metadata": {
        "id": "XFWRcK_cZ9sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "# PyTorch GPU ìƒíƒœ\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"âœ… [PyTorch] Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "\n",
        "# TensorFlow GPU ìƒíƒœ\n",
        "print(\"\\nâœ… [TensorFlow] GPU Devices:\")\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "YJaw3Bh7aBhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsVU927lLs2Z"
      },
      "source": [
        "# ğŸ” 1. BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMZ40xeWMFIu"
      },
      "source": [
        "**BERT (Bidirectional Encoder Representations from Transformers)**\n",
        "- ë¬¸ì¥ ë¶„ë¥˜(ìŠ¤íŒ¸ ë©”ì¼ íƒì§€), ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ(ì±—ë´‡, ê²€ìƒ‰ ì—”ì§„), ë²ˆì—­(ë‹¤êµ­ì–´ ì§€ì›), í…ìŠ¤íŠ¸ ìš”ì•½(ë‰´ìŠ¤ ìš”ì•½) ë“± ë‹¤ì–‘í•œ NLP ì‘ì—…ì— ì‚¬ìš©\n",
        "- ë¬¸ë§¥ì„ ì–‘ë°©í–¥ìœ¼ë¡œ ì´í•´í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ì •ë°€í•˜ê²Œ íŒŒì•…í•˜ë©°, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì‘ìš© ê°€ëŠ¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inLp1BtKM_e7"
      },
      "source": [
        "### 1-1. ëª¨ë¸ê³¼ tokenizer ì´ˆê¸°í™”\n",
        "- tokenizerë¥¼ í†µí•´ ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ì´ë¥¼ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395,
          "referenced_widgets": [
            "42b0bcb700f2460a94f5800d1218a22d",
            "0cc7c05a920f47dfaadf55d9973981ef",
            "dc2602d4839e4cdb8a4dd20c724e1db9",
            "364b9ea810c04c43a2f716dd48efa9be",
            "be12d7ac737844caa4be746ec3f1a5ca",
            "b3625ba95e3a442ab106411a1b07f049",
            "e2ec395a808d42a899d455744ace92e4",
            "454e0834edd441cd996eda6e560aa720",
            "5ec6da07169e4c309ae8168cf9ae70f7",
            "f2b306c5ccc34ef88aa7af9e73302b6b",
            "7b18e39ce214460cbe11f11986669bda",
            "c81815ab8f164e4e903e5332c2d50ba1",
            "9a2428ab48e64ced90756ab351338bb5",
            "823985f041b346279d673d7e34a92c12",
            "ae9889a61a3846078030e49341c0b448",
            "8616c8dbae9e4b37971d0de9951ede72",
            "ac510128afef494eb5206c1eb90a351d",
            "c028f5c523634596ba6829cbb0db5a1f",
            "9de05579152949239cadca8e524ffb6d",
            "f48125420fa8429597845dc8514031f0",
            "bf992114f6344255bf357870bc4d233a",
            "cc33b059201841c2b199ce5640fcf240",
            "0a84a4705d004fd888d71b402fdcb7be",
            "1e60d25ef74344589de134be16e0374e",
            "c57fa90b77f84315a26f48f3ef722fb9",
            "ef78c12b1275411a9eaf27548da6e893",
            "acaebf06bcc44a68b426d77104bfea21",
            "8ca25889ef0c42baaeca0dbf4c2c1d46",
            "0629570475dc48d4ae2ed4f6fd355388",
            "7ae8ee7ff6d94cd692c85c3c99ed8bd1",
            "089e7f17915e485aa62ced39a25a7f39",
            "7bbec307c21f48d9a92278141233a22e",
            "adcdbb5d02ad4f3ea4c5b815c9de136a",
            "83188b92f3f541c5b7b59c8b80c977ba",
            "dbe528cd68ca4228af3123f637fd5dd6",
            "ae6a14765a9a43a0beec43d64dea9117",
            "f4481c205c4a4a0399454a0dd4e8cb22",
            "fe992e4f0a1044e09cf3a9147fd0a549",
            "782cdd4a145743d6b8ef76ac31a82e72",
            "e47add3a617c4cc6a4f07132e336375b",
            "3554ba8aae7841cf8430d5235adbd96b",
            "59c2d85bfa36403fa80ad7e3b248250a",
            "ca579bdf3432479fabb9b38ad73e700b",
            "f87c5b08f021471e94a2b7cd35588e07",
            "5c164bcf23a84c3daf7a555ab2e4fefb",
            "bae62142472d4c69bf4702c22a6f7c80",
            "b3eac413da9e4664bc4c48e6d2fdfaad",
            "32b21bfdf76e49e78b8217e9a32494cd",
            "e9717a0eab82419486923fb0f3040f0a",
            "c4bfe4edeea9492e99f181c39b3447bc",
            "d4c46a2f79d6499481fdd1a6b5706835",
            "6e2a564bbbff4ffaa25d9f3834d54ed3",
            "cf65ce674b9a4bb091547bb7bc9fdecc",
            "72d87d62831b427aaedb0767ce0f2525",
            "97e20b0262814d21b9fc89ccd086c0a3",
            "5a7f5256cda74c8499ee3266e8cf20e0",
            "23e3bec76a97446a97934c50d5b8dd9c",
            "c49b36d7ffad4356924fb9a4e2602b78",
            "018b89abee884bf7a3adedcaf11ca75b",
            "56fcf98931394fb6914260d386128a01",
            "f0e077303c174bde88e7022cebceec90",
            "2d3d4de77fd5404dbea84be5e37d2743",
            "a515870ab4f54fa8a464e68bc2d42fc3",
            "35bc2a07593240a2b6f4cd0fb7e2b65d",
            "716e16ce473f43ce81d3b91f31ff8bdd",
            "7a7a9128cdbc4c6682e2b2e29a753288"
          ]
        },
        "id": "r1m1YLY3GNM0",
        "outputId": "70891eeb-8f92-440a-f60b-736b5eccfe52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42b0bcb700f2460a94f5800d1218a22d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c81815ab8f164e4e903e5332c2d50ba1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a84a4705d004fd888d71b402fdcb7be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83188b92f3f541c5b7b59c8b80c977ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c164bcf23a84c3daf7a555ab2e4fefb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a7f5256cda74c8499ee3266e8cf20e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "# BERT ëª¨ë¸ê³¼ tokenizer ì´ˆê¸°í™”\n",
        "tokenizer = BertTokenizer.from_pretrained('klue/bert-base') # base : ëª¨ë¸í¬ê¸°, (uncased : ì†Œë¬¸ì í•™ìŠµ)\n",
        "model = BertForMaskedLM.from_pretrained('klue/bert-base') # base : ëª¨ë¸í¬ê¸°, (uncased : ì†Œë¬¸ì í•™ìŠµ)\n",
        "# tokenizer : raw textë¥¼ ê°œë³„ í† í°ìœ¼ë¡œ ë¶„ë¦¬(Wordpiece). í† í¬ë‚˜ì´ì € ì‚¬ì „ì— ë”°ë¼ ê³ ìœ í•œ ì •ìˆ˜ ì¸ë±ìŠ¤(ê³ ì •ê°’)ë¥¼ ë§¤í•‘í•¨.\n",
        "\n",
        "# tokenizerë¥¼ í†µí•´ ìƒì„±ëœ ì •ìˆ˜ ì¸ë±ìŠ¤ : í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì½ì„ ìˆ˜ ìˆë„ë¡ ìˆ«ìí™”\n",
        "# Embedding vector : ë‹¨ì–´ì˜ ì˜ë¯¸ì , ë¬¸ë§¥ì  íŠ¹ì„±ì„ ëª¨ë¸ë§"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ4zRzObNqAd"
      },
      "source": [
        "### 1-2. ì…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„ ë° í† í°í™”\n",
        "- [MASK] í† í°ì„ ì‚½ì…í•œ ë¬¸ì¥ì„ ì„¤ì •í•˜ê³ , `tokenizer.tokenize()`ë¡œ í…ìŠ¤íŠ¸ í† í°í™”\n",
        "- [MASK]ë¥¼ í™œìš©í•˜ì—¬ ë¬¸ë§¥ ì†ì—ì„œ íŠ¹ì • ë‹¨ì–´ë¥¼ ì¶”ë¡ í•˜ëŠ” ìƒí™©ì„ ë§Œë“¤ê³ , ì´ë¥¼ ëª¨ë¸ì´ í•™ìŠµí•œ íŒ¨í„´ê³¼ ë¹„êµí•˜ë„ë¡ í•¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKUXDaU1GRwy",
        "outputId": "7b18a44f-c40f-4d81-cbf3-dcf5181083d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'ì—´', '##ì—¬', '##ëŸ', ',', 'ìš°ë¦¬', '##ëŠ”', 'ì„œë¡œ', '##ì˜', 'ì´ë¦„', '##ì„', 'ì²˜ìŒ', 'ë¶ˆë €', '##ë‹¤', '.', 'ê·¸ë¦¬ê³ ', 'ìŠ¤ë¬¼', 'í•˜ë‚˜', ',', 'ìš°ë¦°', '[MASK]', 'ì„', 'í–ˆ', '##ë‹¤', '.', '[SEP]']\n",
            "[2, 1432, 2173, 3542, 16, 3616, 2259, 4084, 2079, 3934, 2069, 3790, 6895, 2062, 18, 3673, 10514, 3657, 16, 8983, 4, 1498, 1902, 2062, 18, 3]\n"
          ]
        }
      ],
      "source": [
        "# í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥\n",
        "text = \"[CLS] ì—´ì—¬ëŸ, ìš°ë¦¬ëŠ” ì„œë¡œì˜ ì´ë¦„ì„ ì²˜ìŒ ë¶ˆë €ë‹¤. ê·¸ë¦¬ê³  ìŠ¤ë¬¼ í•˜ë‚˜, ìš°ë¦° [MASK]ì„ í–ˆë‹¤.[SEP]\"\n",
        "\n",
        "# ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë³€í™˜\n",
        "tokenized_text = tokenizer.***(text) # text(ë¬¸ì¥)ì„ í† í°ìœ¼ë¡œ ë³€í™˜í•´ tokenized_textì— ì €ì¥í•©ì‹œë‹¤!\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(***) # í† í°ì„ indexë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œ!\n",
        "\n",
        "print(tokenized_text) # í† í°ì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
        "print(indexed_tokens) # ì¸ë±ìŠ¤ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nrWA_XEN4I3"
      },
      "source": [
        "### 1-3. MASKëœ ìœ„ì¹˜ í™•ì¸\n",
        "- [MASK] í† í°ì˜ ìœ„ì¹˜ë¥¼ íƒì§€í•´ í•´ë‹¹ ìœ„ì¹˜ì—ì„œ ëª¨ë¸ì´ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ì§€ì •í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAOGhKOrKrRC",
        "outputId": "f567151d-b346-45de-dac3-56177baec72d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "# ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ ì°¾ê¸°\n",
        "masked_index = ***.index(\"[MASK]\") # í† í°í™”ëœ ê²°ê³¼ë¬¼ì—ì„œ index ë©”ì„œë“œë¥¼ í†µí•´ [MASK]ì˜ ìœ„ì¹˜ë¥¼ í™•ì¸í•´ë´…ì‹œë‹¤!\n",
        "print(masked_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oqIAqDOEo_"
      },
      "source": [
        "### 1-4. ì •ìˆ˜í™”ëœ í† í° ì¸ë±ìŠ¤ë¥¼ tensorë¡œ ë³€í™˜ í›„ ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰\n",
        "- ëª¨ë¸ì€ **í…ì„œ ë°ì´í„° êµ¬ì¡°**ë¥¼ í•„ìš”ë¡œ í•˜ë¯€ë¡œ, **ì…ë ¥ê°’ì„ ë³€í™˜**í•˜ì—¬ ì í•©í•œ í˜•ì‹ìœ¼ë¡œ ë§Œë“¦.\n",
        "   - `torch.tensor(ì…ë ¥ê°’ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ì…ë ¥ê°’ ndarray)`ë¥¼ í†µí•´ tensor êµ¬ì¡°ë¡œ ë³€í™˜ ê°€ëŠ¥!\n",
        "- **ì—­ì „íŒŒë¥¼ ë¹„í™œì„±í™”**í•˜ê³ , ëª¨ë¸ì´ **ê° í† í°ì— ëŒ€í•´ ê°€ëŠ¥í•œ ëª¨ë“  ë‹¨ì–´ ì ìˆ˜ ì¶œë ¥**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2T3XriDKuJD",
        "outputId": "4acdcdb0-aee6-4011-fea1-01341876a516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ -6.0337,   4.5526,  -5.6312,  ...,  -7.3959,  -7.4220,  -5.8337],\n",
            "         [ -6.1762,   4.7585,  -7.3973,  ...,  -7.6209, -12.2124,  -6.0998],\n",
            "         [ -7.4194,   3.9148,  -6.1517,  ...,  -6.8162,  -9.5421,  -3.0473],\n",
            "         ...,\n",
            "         [ -7.3761,   8.6972,  -5.4904,  ...,  -8.7202,  -9.1403,  -5.8566],\n",
            "         [ -5.6347,  10.3884,  -4.3374,  ...,  -8.6900,  -7.4171,  -3.5043],\n",
            "         [ -5.6652,  10.3407,  -4.4099,  ...,  -8.7521,  -7.4286,  -3.6681]]])\n"
          ]
        }
      ],
      "source": [
        "# í† í°í™”ëœ í…ìŠ¤íŠ¸ -> ì •ìˆ˜ì¸ë±ìŠ¤(ID) -> Pytorch í…ì„œ\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(***) # í† í°ì„ indexë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œ!\n",
        "tokens_tensor = ***.***([indexed_tokens]) # ì…ë ¥ê°’(í† í° ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸)ì„ tensor êµ¬ì¡°ë¡œ ë³€í™˜í•´ë´…ì‹œë‹¤!\n",
        "\n",
        "# ëª¨ë¸ì— í† í° í…ì„œë¥¼ ì „ë‹¬í•˜ê³  ì˜ˆì¸¡ ì‹¤í–‰\n",
        "with torch.no_grad(): # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì—†ìœ¼ë¯€ë¡œ ìë™ë¯¸ë¶„ì—°ì‚°ì€ ë”\n",
        "    outputs = model(tokens_tensor) # ì˜ˆì¸¡. ëª¨ë¸ì€ ê° í† í°ìœ„ì¹˜ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜.\n",
        "    predictions = outputs[0] # logits (ê° í† í°ìœ„ì¹˜ì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  í† í°ë“¤ì˜ ì›ì‹œì ìˆ˜)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIkvkAL5Oalc"
      },
      "source": [
        "### 1-5. MASKëœ í† í° ì˜ˆì¸¡\n",
        "- [MASK] ìœ„ì¹˜ì—ì„œ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì€ í† í°ì˜ ì¸ë±ìŠ¤ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO5B91p4KwI0",
        "outputId": "7e8ca009-7489-4ef2-8a7e-584bacaa5945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: [CLS] ì—´ì—¬ëŸ, ìš°ë¦¬ëŠ” ì„œë¡œì˜ ì´ë¦„ì„ ì²˜ìŒ ë¶ˆë €ë‹¤. ê·¸ë¦¬ê³  ìŠ¤ë¬¼ í•˜ë‚˜, ìš°ë¦° [MASK]ì„ í–ˆë‹¤.[SEP]\n",
            "Masked: ['[CLS]', 'ì—´', '##ì—¬', '##ëŸ', ',', 'ìš°ë¦¬', '##ëŠ”', 'ì„œë¡œ', '##ì˜', 'ì´ë¦„', '##ì„', 'ì²˜ìŒ', 'ë¶ˆë €', '##ë‹¤', '.', 'ê·¸ë¦¬ê³ ', 'ìŠ¤ë¬¼', 'í•˜ë‚˜', ',', 'ìš°ë¦°', 'ì‚¬ë‘', 'ì„', 'í–ˆ', '##ë‹¤', '.', '[SEP]']\n",
            "Predicted token: ì‚¬ë‘\n",
            "\n",
            "Filled sentence: ì—´ì—¬ëŸ , ìš°ë¦¬ëŠ” ì„œë¡œì˜ ì´ë¦„ì„ ì²˜ìŒ ë¶ˆë €ë‹¤ . ê·¸ë¦¬ê³  ìŠ¤ë¬¼ í•˜ë‚˜ , ìš°ë¦° ì‚¬ë‘ ì„ í–ˆë‹¤ .\n"
          ]
        }
      ],
      "source": [
        "# ì˜ˆì¸¡ëœ í† í° í™•ì¸\n",
        "predicted_index = torch.argmax(predictions[0, masked_index]).item() # masked_indexì— ë“¤ì–´ê°ˆ ê²ƒìœ¼ë¡œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ì¸ë±ìŠ¤\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "\n",
        "# ì˜ˆì¸¡ëœ í† í°ìœ¼ë¡œ ë§ˆìŠ¤í¬ ì±„ìš°ê¸°\n",
        "tokenized_text[masked_index] = predicted_token\n",
        "# í† í°í™”ëœ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
        "filled_text = tokenizer.convert_tokens_to_string(tokenized_text[1:-1])\n",
        "\n",
        "print(\"Original:\", text)\n",
        "print(\"Masked:\", tokenized_text)\n",
        "print(\"Predicted token:\", predicted_token) ; print()\n",
        "\n",
        "print(\"Filled sentence:\", filled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4ncI4tBOWLh"
      },
      "source": [
        "\n",
        "### 1-6. MASKëœ ë¬¸ì¥ ë³µì›\n",
        "- ì˜ˆì¸¡ëœ í† í°ìœ¼ë¡œ [MASK]ë¥¼ ëŒ€ì²´í•˜ì—¬ ì™„ì„±ëœ ë¬¸ì¥ ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQJBSmfZK2l2",
        "outputId": "7bcfdc48-551e-4844-87a9-64bddd89eaf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "# BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
        "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
        "model = BertForMaskedLM.from_pretrained('klue/bert-base')\n",
        "\n",
        "def fill_mask(input_text):\n",
        "    # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜\n",
        "    tokenized_text = tokenizer.tokenize(input_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ ì°¾ê¸°\n",
        "    masked_index = ***.index(\"[MASK]\") # í† í°í™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ë¥¼ ì°¾ì•„ë´…ì‹œë‹¤!\n",
        "\n",
        "    # í† í°í™”ëœ í…ìŠ¤íŠ¸ -> ì •ìˆ˜ì¸ë±ìŠ¤(ID) -> Pytorch í…ì„œ\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "    # ëª¨ë¸ì— í† í° í…ì„œë¥¼ ì „ë‹¬í•˜ê³  ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor)\n",
        "        predictions = outputs[0]\n",
        "\n",
        "    # ì˜ˆì¸¡ëœ í† í° í™•ì¸\n",
        "    predicted_index = torch.***(predictions[0, masked_index]).item() # ìœ„ ì½”ë“œ ì°¸ê³ ! masked_indexì— ë“¤ì–´ê°ˆ ê²ƒìœ¼ë¡œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ì¸ë±ìŠ¤\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "\n",
        "    # ë§ˆìŠ¤í¬ë¥¼ ì±„ìš´ ë¬¸ì¥ ë°˜í™˜\n",
        "    tokenized_text[***] = predicted_token # í† í°í™”ëœ í…ìŠ¤íŠ¸ì˜ ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜(ì¸ë±ìŠ¤ ì‚¬ìš©)ë¥¼ ì˜ˆì¸¡ëœ í† í°ìœ¼ë¡œ ì±„ì›Œë´…ì‹œë‹¤!\n",
        "    return tokenizer.convert_tokens_to_string(tokenized_text[1:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeAYIzyYPVVt"
      },
      "source": [
        "ì˜ˆì‹œ ë¬¸ì¥ ë§Œë“¤ì–´ì„œ MLMì„ ì‚¬ìš©í•´ ë³´ì„¸ìš”!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q81b8zuYK3E9",
        "outputId": "4efa38e1-8d0a-4ef0-aa1d-329b8b8d7e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filled sentence: ìœ¤í•´ë¯¼ì€ INSGIHT í•™íšŒë¥¼ ì •ë§ ì‚¬ë‘í•œë‹¤ . ê·¸ëŠ” INSIGHT í•™íšŒë¥¼ ìœ„í•´ í—Œì‹  ì„ í–ˆë‹¤ .\n"
          ]
        }
      ],
      "source": [
        "# ì˜ˆì‹œ 1: \"[CLS] ì—´ì—¬ëŸ, ìš°ë¦¬ëŠ” ì„œë¡œì˜ ì´ë¦„ì„ ì²˜ìŒ ë¶ˆë €ë‹¤. ê·¸ë¦¬ê³  ìŠ¤ë¬¼ í•˜ë‚˜, ìš°ë¦° [MASK]ì„ í–ˆë‹¤.[SEP]\"\n",
        "# ì˜ˆì‹œ 2: \"[CLS] ìœ¤í•´ë¯¼ì€ INSGIHT í•™íšŒë¥¼ ì •ë§ ì‚¬ë‘í•œë‹¤. ê·¸ëŠ” INSIGHT í•™íšŒë¥¼ ìœ„í•´ [MASK]ë¥¼ í–ˆë‹¤.[SEP]\"\n",
        "# ì´ ì˜ˆì‹œë¥¼ ì°¸ê³ í•´ì„œ [CLS](ë§¨ ì•), [MASK](ì±„ìš¸ ë¶€ë¶„), [SEP](ë§¨ ë’¤)ë¥¼ ì¶”ê°€í•´ì„œ ì—¬ëŸ¬ë¶„ë§Œì˜ ì˜ˆì‹œ ë¬¸ì¥ ë§Œë“¤ì–´ì£¼ì„¸ìš”\n",
        "input_text = \"[CLS] ìœ¤í•´ë¯¼ì€ INSGIHT í•™íšŒë¥¼ ì •ë§ ì‚¬ë‘í•œë‹¤. ê·¸ëŠ” INSIGHT í•™íšŒë¥¼ ìœ„í•´ [MASK]ì„ í–ˆë‹¤.[SEP]\"\n",
        "filled_text = fill_mask(input_text)\n",
        "print(\"Filled sentence:\", filled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ” 2. GPT, RAG, Langchain"
      ],
      "metadata": {
        "id": "tmCHerNBVflo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-1 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "\n",
        "langchain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€, pdf ë¬¸ì„œë¥¼ ì½ì„ìˆ˜ ìˆëŠ” pypdf ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í• ê²Œìš”!"
      ],
      "metadata": {
        "id": "BJXM4PpMfBB_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHh7iUqLyHQk",
        "outputId": "d3a1023e-a8c1-49d4-eb92-2b485be5a6cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.8/93.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m326.6/326.6 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.2/471.2 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# â¬‡ï¸ Colabì—ì„œ í•œ ë²ˆë§Œ ì‹¤í–‰\n",
        "%pip install -qU \"langchain>=0.3\" langchain-community langchain-openai langgraph chromadb pypdf tiktoken\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-2. í‚¤ê°’ ì„¤ì •"
      ],
      "metadata": {
        "id": "eWTYIcdbe4VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################\n",
        "### ê³¼ì œ ì œì¶œ ì‹œ ì´ ì½”ë“œ ì…€ì€ ì‚­ì œí•˜ê³  ì €ì¥ í›„, add, commit, push í•´ì£¼ì„¸ìš” ###\n",
        "##################################################################\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-API-KEY\"\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"your-API-KEY\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"practice_langgraph\"\n",
        "\n"
      ],
      "metadata": {
        "id": "6jHacWxMy_AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-3 pdf ë¬¸ì„œ ì„ íƒ (ë¡œë“œ)\n",
        "\n",
        "\n",
        "ì—¬ëŸ¬ë¶„ë“¤ì´ í™œìš©í•˜ê³  ì‹¶ì€ ì•„ë¬´ pdf ë¬¸ì„œë¥¼ ì„ íƒí•´ë³´ì„¸ìš”!"
      ],
      "metadata": {
        "id": "hOjJzln6fLaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os, re, unicodedata\n",
        "\n",
        "os.makedirs(\"/content/pdfs\", exist_ok=True)\n",
        "print(\"ğŸ“¥ ì—…ë¡œë“œí•  PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥).\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "name_map = {}\n",
        "for i, (orig_name, data) in enumerate(uploaded.items(), 1):\n",
        "    nf = unicodedata.normalize(\"NFC\", orig_name)\n",
        "    base, ext = os.path.splitext(nf)\n",
        "    # í•œê¸€/ì˜ë¬¸/ìˆ«ì/ë°‘ì¤„/ëŒ€ì‹œ/ì ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” _\n",
        "    safe_base = re.sub(r\"[^\\w\\u3131-\\u318E\\uAC00-\\uD7A3.\\-]+\", \"_\", base).strip(\"._'â€˜â€™â€œâ€\")\n",
        "    # íŒŒì¼ëª… ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ëƒ„ (ìµœëŒ€ ~140ì ì •ë„ë¡œ)\n",
        "    safe_base = safe_base[:120] if len(safe_base) > 120 else safe_base\n",
        "    new_name = f\"doc_{i}_{safe_base}{ext.lower()}\"\n",
        "    with open(os.path.join(\"/content/pdfs\", new_name), \"wb\") as f:\n",
        "        f.write(data)\n",
        "    name_map[new_name] = orig_name\n",
        "\n",
        "print(\"âœ… ì €ì¥ ì™„ë£Œ (Colab íŒŒì¼ëª… â†’ ì›ë³¸ëª… ë§¤í•‘):\")\n",
        "for k, v in name_map.items():\n",
        "    print(f\"  {k}  <=  {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "FH8xlYI0zEiG",
        "outputId": "3148a4fb-8434-4694-e4b8-a4f27f5724ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ ì—…ë¡œë“œí•  PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dd8d90b6-a492-4445-a5c9-552140405adc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dd8d90b6-a492-4445-a5c9-552140405adc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving á„€á…¯á†«á„†á…µá†«á„Œá…µ(2024), á„‰á…µá†«á„‹á…¥ á„’á…§á†¼á„‰á…¥á†¼á„‹á…¦ á„‚á…¡á„á…¡á„‚á…¡á„‚á…³á†« á„‹á…³á†·á„‹á…®á†«á„Œá…¥á†¨ á„á…³á†¨á„Œá…µá†¼ á„‡á…®á†«á„‰á…¥á†¨ - á„‹á…³á†·á„‹á…®á†« á„á…¡á†¯á„…á…¡á†¨á„€á…ª á„‹á…³á†·á„‹á…®á†« á„‡á…§á†«á„’á…§á†¼.pdf to á„€á…¯á†«á„†á…µá†«á„Œá…µ(2024), á„‰á…µá†«á„‹á…¥ á„’á…§á†¼á„‰á…¥á†¼á„‹á…¦ á„‚á…¡á„á…¡á„‚á…¡á„‚á…³á†« á„‹á…³á†·á„‹á…®á†«á„Œá…¥á†¨ á„á…³á†¨á„Œá…µá†¼ á„‡á…®á†«á„‰á…¥á†¨ - á„‹á…³á†·á„‹á…®á†« á„á…¡á†¯á„…á…¡á†¨á„€á…ª á„‹á…³á†·á„‹á…®á†« á„‡á…§á†«á„’á…§á†¼.pdf\n",
            "âœ… ì €ì¥ ì™„ë£Œ (Colab íŒŒì¼ëª… â†’ ì›ë³¸ëª… ë§¤í•‘):\n",
            "  doc_1_ê¶Œë¯¼ì§€_2024_ì‹ ì–´_í˜•ì„±ì—_ë‚˜íƒ€ë‚˜ëŠ”_ìŒìš´ì _íŠ¹ì§•_ë¶„ì„_-_ìŒìš´_íƒˆë½ê³¼_ìŒìš´_ë³€í˜•.pdf  <=  á„€á…¯á†«á„†á…µá†«á„Œá…µ(2024), á„‰á…µá†«á„‹á…¥ á„’á…§á†¼á„‰á…¥á†¼á„‹á…¦ á„‚á…¡á„á…¡á„‚á…¡á„‚á…³á†« á„‹á…³á†·á„‹á…®á†«á„Œá…¥á†¨ á„á…³á†¨á„Œá…µá†¼ á„‡á…®á†«á„‰á…¥á†¨ - á„‹á…³á†·á„‹á…®á†« á„á…¡á†¯á„…á…¡á†¨á„€á…ª á„‹á…³á†·á„‹á…®á†« á„‡á…§á†«á„’á…§á†¼.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-4 ë¬¸ì„œ ë¶„í•  í›„ ë²¡í„° DBì— ì €ì¥\n",
        "ë¡œë”©í•œ ë¬¸ì„œë¥¼ ë¶„í•  í•  ë•Œ,\n",
        "- chunk_sizeëŠ” `900`\n",
        "- chunk_overlapì€ `150`\n",
        "- separators(êµ¬ë¶„ì)ëŠ” `[\"\\n\\n\", \"\\n\", \" \", \"\"]`\n",
        "ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì²­í‚¹í•´ì£¼ì„¸ìš”!"
      ],
      "metadata": {
        "id": "Rt0o0nGvbS8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "import glob\n",
        "\n",
        "# 1) PDF â†’ Documents\n",
        "pdf_paths = sorted(glob.glob(\"/content/pdfs/*.pdf\"))\n",
        "docs = []\n",
        "for p in pdf_paths:\n",
        "    try:\n",
        "        loader = PyPDFLoader(p)\n",
        "        docs.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ ë¡œë”© ì‹¤íŒ¨: {p} -> {e}\")\n",
        "\n",
        "print(f\"ì´ ë¬¸ì„œ ì¡°ê°(í˜ì´ì§€ ê¸°ì¤€): {len(docs)}\")\n",
        "\n",
        "# 2) ë¬¸ì„œ ë¶„í• \n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size= ***, # ì²­í¬ ì‚¬ì´ì¦ˆ: ë¬¸ì„œë¥¼ ëª‡ ê°œì˜ í† í° ë‹¨ìœ„ë¡œ ë‚˜ëˆŒ ê²ƒì¸ì§€ë¥¼ ì •í•©ë‹ˆë‹¤.\n",
        "    chunk_overlap= ***, # ì²­í¬ ì˜¤ë²„ë©: ë¶„í• ëœ ë ë¶€ë¶„ì—ì„œ ë§¥ë½ì´ ì´ì–´ì§ˆ ìˆ˜ ìˆë„ë¡ ì¼ë¶€ë¥¼ ê²¹ì³ì„œ(overlap) ë¶„í• í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n",
        "    separators= ***, # êµ¬ë¶„ì: ì—”í„°, ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ”\n",
        ")\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"ì´ ì²­í¬ ìˆ˜: {len(chunks)}\")\n",
        "\n",
        "# 3) ë²¡í„°ìŠ¤í† ì–´ (ì˜ì† ë””ë ‰í† ë¦¬ ì“°ë©´ ëŸ°íƒ€ì„ ì¬ì‹œì‘í•´ë„ ìœ ì§€)\n",
        "persist_dir = \"/content/chroma_pdf_db\"\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # ë¹„ìš© ì ˆì•½í˜•\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=persist_dir\n",
        ")\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(\"âœ… ì¸ë±ì‹± ì™„ë£Œ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfApbza30fU1",
        "outputId": "0a59bc2b-2d50-43d7-aefa-d545ff226b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ ë¬¸ì„œ ì¡°ê°(í˜ì´ì§€ ê¸°ì¤€): 21\n",
            "ì´ ì²­í¬ ìˆ˜: 31\n",
            "âœ… ì¸ë±ì‹± ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-5 RA<U>Generate</U> : ìƒì„± ë¶€ë¶„ ì„ ì–¸\n",
        "\n",
        "PROMPT ë¶€ë¶„ì€ ì—¬ëŸ¬ë¶„ë“¤ì´ ì›í•˜ì‹œëŠ”ëŒ€ë¡œ ì…ë ¥í•´ë„ ì¢‹ìŠµë‹ˆë‹¤!\n",
        "\n",
        "ğŸ¤¥ ì˜ì–´ë¡œ ì…ë ¥í•˜ë©´ ëª¨ë¸ì´ ë” ì˜ ì´í•´í•œëŒ€ìš”"
      ],
      "metadata": {
        "id": "RF2U8Ddmhyu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "PROMPT = ChatPromptTemplate.from_template(\n",
        "\"\"\"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸(ì—…ë¡œë“œí•œ PDFì—ì„œ ì¶”ì¶œë¨)ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µí•˜ë¼.\n",
        "ë¶ˆí™•ì‹¤í•˜ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ë¼. í•µì‹¬ ê·¼ê±°ë¥¼ ìš”ì•½í•˜ê³ , ê° ê·¼ê±° ì˜†ì— ì¶œì²˜(íŒŒì¼/í˜ì´ì§€)ë¥¼ í‘œì‹œí•˜ë¼.\n",
        "\n",
        "[ì§ˆë¬¸]\n",
        "{question}\n",
        "\n",
        "[ì»¨í…ìŠ¤íŠ¸]\n",
        "{context}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "rag_chain = (PROMPT | llm | StrOutputParser())\n"
      ],
      "metadata": {
        "id": "8qaycjB50fSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-6 RAG íŒŒì´í”„ë¼ì¸ langgraphë¡œ êµ¬ì„±\n",
        "\n",
        "ë²¡í„°dbì—ì„œ ê²€ìƒ‰í•˜ëŠ” ê²ƒì´ <U>Retrieve</U>AGì´ê³ ,\n",
        "\n",
        "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìƒì„± ë‹¨ê³„ì— ë˜ì ¸ì£¼ëŠ” ê²ƒì´ R<U>Augment</U>Gì…ë‹ˆë‹¤!"
      ],
      "metadata": {
        "id": "ZMP-y_6bjw5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# format_docsì˜ ì—­í• ì€ ë¬¸ì„œë¥¼ í¬ë§·íŒ…í•´ì„œ ëª¨ë¸ì—ê²Œ ì œê³µí•  'context'ë¥¼ ì •ë¦¬í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤!\n",
        "def format_docs(docs):\n",
        "    # íŒŒì¼ëª…/í˜ì´ì§€ë¥¼ ë©”íƒ€ë°ì´í„°ì— ë‹´ì•„ë‘ëŠ” PyPDFLoader ê¸°ë³¸ê°’ ì‚¬ìš©\n",
        "    lines = []\n",
        "    for i, d in enumerate(docs, 1):\n",
        "        src = d.metadata.get(\"source\", \"\")\n",
        "        page = d.metadata.get(\"page\", None)\n",
        "        tag = f\"{os.path.basename(src)}\"\n",
        "        if page is not None:\n",
        "            tag += f\" p.{page+1}\"\n",
        "        lines.append(f\"[{i}] {tag}\\n{d.page_content}\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "# ê·¸ë˜í”„ ì„ ì–¸\n",
        "class QAState(TypedDict):\n",
        "    question: str\n",
        "    retrieved: List[Document]\n",
        "    answer: str\n",
        "\n",
        "def node_retrieve(state: QAState):\n",
        "    q = state[\"question\"]\n",
        "    topk = retriever.invoke(q)\n",
        "    return {\"retrieved\": topk}\n",
        "\n",
        "def node_generate(state: QAState):\n",
        "    q = state[\"question\"]\n",
        "    ctx = format_docs(state[\"retrieved\"])\n",
        "    ans = rag_chain.invoke({\"question\": q, \"context\": ctx})\n",
        "    return {\"answer\": ans}\n",
        "\n",
        "# --------- ê·¸ë˜í”„ êµ¬ì„± ---------\n",
        "# ë…¸ë“œë¼ë¦¬ ì—°ê²°í•˜ê¸°\n",
        "workflow = StateGraph(QAState)\n",
        "workflow.add_node(\"retrieve\", ***) # ì–´ë–¤ ë…¸ë“œê°€ ì¶”ê°€ë˜ì–´ì•¼ í• ê¹Œìš”?\n",
        "workflow.add_node(\"generate\", ***) # ì–´ë–¤ ë…¸ë“œê°€ ì¶”ê°€ë˜ì–´ì•¼ í• ê¹Œìš”?\n",
        "\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"***\", \"***\") # ì–´ë–¤ ë…¸ë“œì™€ ì–´ë–¤ ë…¸ë“œê°€ ì—°ê²°ë˜ì–´ì•¼ í• ê¹Œìš”?\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "app = workflow.compile()\n",
        "print(\"âœ… LangGraph ì»´íŒŒì¼ ì™„ë£Œ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuCAAX4E0fOz",
        "outputId": "cdcc86f3-982b-451a-9a2f-bb12ea5776e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LangGraph ì»´íŒŒì¼ ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-7 ì‹¤ì œ êµ¬ë™í•´ë³´ê¸°\n",
        "\n",
        "questionì— ì—¬ëŸ¬ë¶„ë“¤ì´ ì…ë ¥í•˜ê³  ì‹¶ì€ ì§ˆë¬¸ì„ ë„£ì–´ë³´ì„¸ìš”!\n",
        "\n",
        "RAG ê³¼ì œëŠ” ì´ë ‡ê²Œ ê°„ë‹¨í•˜ê²Œ ëë‚´ê² ìŠµë‹ˆë‹¤ğŸ˜"
      ],
      "metadata": {
        "id": "KNAKcKePchhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"ì´ ë…¼ë¬¸ì˜ ì„œë¡  ë° ì„ í–‰ì—°êµ¬ ë¶€ë¶„ì„ ë²”ì£¼í™”í•˜ì—¬ ì •ë¦¬í•´ì¤˜.\"\n",
        "final = app.invoke({\"question\": question})\n",
        "print(\"â–¼ ë‹µë³€\")\n",
        "print(final[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC-HXC7j0wHs",
        "outputId": "a07be453-1688-46f8-a56b-df6de91c59eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â–¼ ë‹µë³€\n",
            "ì´ ë…¼ë¬¸ì˜ ì„œë¡  ë° ì„ í–‰ì—°êµ¬ ë¶€ë¶„ì„ ë²”ì£¼í™”í•˜ì—¬ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
            "\n",
            "1. **ì—°êµ¬ ëª©ì **\n",
            "   - ì‹ ì–´ì˜ ë…íŠ¹í•œ ë‹¨ì–´ í˜•ì„± ë°©ë²•ì¸ ì¶•ì•½ê³¼ í˜¼ì„±ì— ë‚˜íƒ€ë‚˜ëŠ” ìŒìš´ì  íŠ¹ì§•ì„ ë¶„ì„í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤. (ì¶œì²˜: [3] p.2)\n",
            "\n",
            "2. **ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„**\n",
            "   - ê¸°ì¡´ì˜ í•œêµ­ì–´ ì‹ ì–´ ê´€ë ¨ ì—°êµ¬ëŠ” ì£¼ë¡œ í˜•íƒœì  íŠ¹ì§•ì— ì§‘ì¤‘ë˜ì–´ ìˆìœ¼ë©°, ìŒìš´ì  íŠ¹ì„±ì— ëŒ€í•œ ë¶„ì„ì´ ë¶€ì¡±í•˜ë‹¤. ì´ëŠ” ì‹ ì–´ ì¡°ì–´ë²•ì˜ ë³µí•©ì ì¸ íŠ¹ì„±ì„ ì´í•´í•˜ëŠ” ë° í•œê³„ê°€ ìˆë‹¤. (ì¶œì²˜: [3] p.2)\n",
            "\n",
            "3. **ì‹ ì–´ì˜ ì •ì˜ ë° ë¶„ë¥˜**\n",
            "   - ì‹ ì–´ëŠ” ìƒˆë¡­ê²Œ í˜•ì„±ëœ ë‹¨ì–´ë¥¼ ì˜ë¯¸í•˜ë©°, ë¬¸ê¸ˆí˜„(1999)ì˜ ë¶„ë¥˜ì— ë”°ë¼ ì‹ ì–´, ì‹ ì¡°ì–´, ì‹ ìƒì–´ë¡œ ë‚˜ë‰œë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê°€ì¥ ë„“ì€ ì˜ë¯¸ì˜ 'ì‹ ì–´'ë¥¼ ì‚¬ìš©í•œë‹¤. (ì¶œì²˜: [3] p.2)\n",
            "\n",
            "4. **ì—°êµ¬ ìë£Œ**\n",
            "   - ì—°êµ¬ì— ì‚¬ìš©ëœ ì‹ ì–´ëŠ” êµ­ë¦½êµ­ì–´ì›ì—ì„œ ë°œê°„ëœ <ì‹ ì–´ ì¡°ì‚¬ ë³´ê³ ì„œ>ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ë©°, 2002ë…„ë¶€í„° 2019ë…„ê¹Œì§€ì˜ ìë£Œë¥¼ í™œìš©í•œë‹¤. (ì¶œì²˜: [3] p.2)\n",
            "\n",
            "5. **ìŒìš´ì  ìš”ì¸ì˜ ì¤‘ìš”ì„±**\n",
            "   - ì‹ ì–´ í˜•ì„± ê³¼ì •ì—ì„œ ìŒìš´ì  ìš”ì¸ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, í–¥í›„ ì—°êµ¬ê°€ ì˜ë¯¸ì  ë³€í™” ë° ì‚¬íšŒë¬¸í™”ì  ì˜í–¥ê³¼ ê´€ë ¨í•˜ì—¬ íƒêµ¬í•  ìˆ˜ ìˆëŠ” ê¸°ì´ˆë¥¼ ë§ˆë ¨í•˜ê³ ì í•œë‹¤. (ì¶œì²˜: [2] p.2)\n",
            "\n",
            "ì´ì™€ ê°™ì´ ì„œë¡  ë° ì„ í–‰ì—°êµ¬ ë¶€ë¶„ì€ ì—°êµ¬ì˜ ëª©ì , ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„, ì‹ ì–´ì˜ ì •ì˜ ë° ë¶„ë¥˜, ì—°êµ¬ ìë£Œ, ìŒìš´ì  ìš”ì¸ì˜ ì¤‘ìš”ì„±ìœ¼ë¡œ ë²”ì£¼í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"ì´ ë…¼ë¬¸ì— ëŒ€í•´ì„œ ìš”ì•½í•´ì¤˜\"\n",
        "final = app.invoke({\"question\": question})\n",
        "print(\"â–¼ ë‹µë³€\")\n",
        "print(final[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWRhK7jv0y81",
        "outputId": "b1ec6708-fbc8-44ff-cc0d-ef7e1117ac7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â–¼ ë‹µë³€\n",
            "ì´ ë…¼ë¬¸ì€ ì‹ ì–´(æ–°èª) ìƒì„±ì˜ ë…íŠ¹í•œ ë‹¨ì–´ í˜•ì„± ë°©ë²•ì¸ ì¶•ì•½ê³¼ í˜¼ì„±ì— ë‚˜íƒ€ë‚˜ëŠ” ìŒìš´ì  íŠ¹ì§•ì„ ë¶„ì„í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•˜ê³  ìˆë‹¤. ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ì´ ì£¼ë¡œ ì‹ ì–´ì˜ í˜•íƒœì  íŠ¹ì§•ì— ì§‘ì¤‘í•´ì™”ë˜ ë°˜ë©´, ì´ ì—°êµ¬ëŠ” ì‹ ì–´ì˜ ìŒìš´ì  íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ìƒˆë¡œìš´ ê´€ì ì„ ì œì‹œí•˜ê³ ì í•œë‹¤. ì—°êµ¬ëŠ” 2002ë…„ë¶€í„° 2019ë…„ê¹Œì§€ì˜ êµ­ë¦½êµ­ì–´ì›ì—ì„œ ë°œê°„ëœ <ì‹ ì–´ ì¡°ì‚¬ ë³´ê³ ì„œ>ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆë‹¤.\n",
            "\n",
            "ì£¼ìš” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:\n",
            "\n",
            "1. **ì‹ ì–´ì˜ ì •ì˜ì™€ ì—°êµ¬ í•„ìš”ì„±**: ì‹ ì–´ëŠ” ìƒˆë¡­ê²Œ í˜•ì„±ëœ ë‹¨ì–´ë¡œ, í˜•íƒœì  ë¶„ì„ë§Œìœ¼ë¡œëŠ” ê·¸ ë³µí•©ì ì¸ íŠ¹ì„±ì„ ì´í•´í•˜ëŠ” ë° í•œê³„ê°€ ìˆë‹¤. ë”°ë¼ì„œ ìŒìš´ì  íŠ¹ì„± ë¶„ì„ì´ í•„ìš”í•˜ë‹¤. (ì¶œì²˜: [3] p.2)\n",
            "\n",
            "2. **ìŒìš´ì  íŠ¹ì§• ë¶„ì„**: ì‹ ì–´ í˜•ì„±ì—ì„œ ìŒìš´íƒˆë½, ìŒìš´ë³€í˜• ë“±ì˜ ìŒìš´ì  ê¸°ì œê°€ ì‘ìš©í•˜ë©°, ì´ë¥¼ í†µí•´ ì‹ ì–´ í˜•ì„±ì˜ ë‹¤ì¸µì  íŠ¹ì„±ì„ ì„¤ëª…í•˜ê³ ì í•œë‹¤. (ì¶œì²˜: [5] p.17)\n",
            "\n",
            "3. **ì—°êµ¬ì˜ í•œê³„ì™€ ë¯¸ë˜ ë°©í–¥**: ì—°êµ¬ëŠ” 2000ë…„ëŒ€ë¶€í„° 2019ë…„ê¹Œì§€ì˜ ì‹ ì–´ì— í•œì •ë˜ì—ˆìœ¼ë‚˜, í˜„ì¬ì˜ ì–¸ì–´ ì‚¬ìš© ê²½í–¥ì„ ê³ ë ¤í•  ë•Œ 2020ë…„ ì´í›„ì—ëŠ” ë” ë§ì€ ì‹ ì¡°ì–´ê°€ ë“±ì¥í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤. (ì¶œì²˜: [5] p.17)\n",
            "\n",
            "ê²°ë¡ ì ìœ¼ë¡œ, ì´ ì—°êµ¬ëŠ” ì‹ ì–´ì˜ ìŒìš´ì  íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ ì‹ ì–´ ì—°êµ¬ì— ìƒˆë¡œìš´ ë°©í–¥ì„±ì„ ì œì‹œí•˜ê³  ìˆìœ¼ë©°, í–¥í›„ ì—°êµ¬ê°€ ì˜ë¯¸ì  ë³€í™” ë° ì‚¬íšŒë¬¸í™”ì  ì˜í–¥ê³¼ ê´€ë ¨í•˜ì—¬ ë”ìš± íƒêµ¬í•  ìˆ˜ ìˆëŠ” ê¸°ì´ˆë¥¼ ë§ˆë ¨í•˜ê³  ìˆë‹¤. (ì¶œì²˜: [2] p.2, [4] p.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"ì´ ë…¼ë¬¸ì— ìˆëŠ” ì˜ˆì‹œë¥¼ í†µí•´ ìŒìš´ì  íŠ¹ìˆ˜ì„±ì´ ë‚˜íƒ€ë‚œ ì‹ ì–´ë“¤ì— ëŒ€í•œ ë¶„ì„ì„ ì„¤ëª…í•´ì¤˜.\"\n",
        "final = app.invoke({\"question\": question})\n",
        "print(\"â–¼ ë‹µë³€\")\n",
        "print(final[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFIJftFPpXb9",
        "outputId": "80bbd8eb-e564-4eeb-b443-53e264147632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â–¼ ë‹µë³€\n",
            "ì´ ë…¼ë¬¸ì—ì„œëŠ” ì‹ ì–´ í˜•ì„±ì— ë‚˜íƒ€ë‚˜ëŠ” ìŒìš´ì  íŠ¹ìˆ˜ì„±ì„ ë¶„ì„í•˜ê³  ìˆìœ¼ë©°, ìŒìš´ì  íŠ¹ìˆ˜ì„±ì´ ë‚˜íƒ€ë‚œ ì‹ ì–´ë“¤ì— ëŒ€í•œ ëª‡ ê°€ì§€ ì˜ˆì‹œë¥¼ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
            "\n",
            "1. **ìŒìš´íƒˆë½**: ì‹ ì–´ì˜ ìŒìš´íƒˆë½ ì˜ˆë¡œ 'ì…©ì¥'(ìˆ˜ì˜ì¥)ê³¼ 'ìœ°ì°¨'(ìœ ëª¨ì°¨)ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ ìŒì ˆì´ íƒˆë½í•˜ì—¬ ìƒˆë¡œìš´ í˜•íƒœë¡œ ë³€í˜•ëœ ê²½ìš°ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ìŒìš´íƒˆë½ì€ ì‹ ì–´ì˜ í˜•ì„± ê³¼ì •ì—ì„œ ë…íŠ¹í•œ ì–‘ìƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. (ì¶œì²˜: [1] p.11)\n",
            "\n",
            "2. **ìŒìš´ë³€í˜•(ì²¨ê°€)**: 'ê°•ë ¤í¬í•˜ë‹¤'(ê°•ë ¥í•˜ë‹¤)ë¼ëŠ” ë‹¨ì–´ëŠ” ìŒìš´ì´ ë³€í˜•ëœ ì˜ˆë¡œ, ê¸°ì¡´ì˜ ë‹¨ì–´ì—ì„œ ìŒìš´ì´ ì¶”ê°€ëœ í˜•íƒœì…ë‹ˆë‹¤. ì´ëŠ” ì‹ ì–´ì˜ í˜•ì„±ì—ì„œ ìŒìš´ì  ë³€í˜•ì´ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. (ì¶œì²˜: [1] p.11)\n",
            "\n",
            "3. **ìŒìš´ë³€í˜•(ë³µí•©)**: 'ê°•ì¥'(ê°•ì•„ì§€)ëŠ” ë³µí•© ìŒìš´ë³€í˜•ì˜ ì˜ˆë¡œ, ê¸°ì¡´ ë‹¨ì–´ì˜ ìŒìš´ì´ ë³€í˜•ë˜ì–´ ìƒˆë¡œìš´ í˜•íƒœë¡œ ë‚˜íƒ€ë‚œ ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³µí•©ì ì¸ ìŒìš´ì  ê³¼ì •ì€ ì‹ ì–´ í˜•ì„±ì˜ ë‹¤ì¸µì  íŠ¹ì„±ì„ ì„¤ëª…í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤. (ì¶œì²˜: [1] p.11)\n",
            "\n",
            "ì´ ì—°êµ¬ëŠ” ì‹ ì–´ì˜ ìŒìš´ì  íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬, ì‹ ì–´ í˜•ì„±ì˜ ìŒìš´ì  ê³¼ì •ì´ ë‹¨ìˆœí•œ ì¶•ì•½ì´ë‚˜ ë³€í˜•ì´ ì•„ë‹ˆë¼, ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ë³€ê²½í•˜ëŠ” ë³µì¡í•œ ê³¼ì •ì„ í¬í•¨í•˜ê³  ìˆìŒì„ ê°•ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤. (ì¶œì²˜: [4] p.1, [5] p.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ” 3. sLM"
      ],
      "metadata": {
        "id": "bCgJ30gFVuQR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfsDR_omdNea"
      },
      "source": [
        "## 3-1. sLM(Gemma) ë¡œë“œ\n",
        "### sLM(small Language Model)ì˜ ëŒ€í‘œ ëª¨ë¸ì¸ Gemmaë¥¼ ë¡œë“œ í•´ë´…ì‹œë‹¤!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXFZFUJHgTcU"
      },
      "source": [
        "### (1) ì‚¬ì „ ì¤€ë¹„ ì‚¬í•­\n",
        "### 1. HuggingFace ë¡œê·¸ì¸\n",
        "\n",
        "https://huggingface.co/\n",
        "\n",
        "ì´ ë§í¬ë¡œ ë“¤ì–´ê°€ì„œ HuggingFaceì— ë¡œê·¸ì¸í•©ë‹ˆë‹¤.(ê³„ì •ì´ ì—†ë‹¤ë©´ íšŒì›ê°€ì… í•´ì£¼ì„¸ìš”!)\n",
        "\n",
        "### 2. í† í° ë°œê¸‰ë°›ê¸°\n",
        "\n",
        "https://huggingface.co/settings/tokens\n",
        "\n",
        "- ì´ ë§í¬ë¡œ ë“¤ì–´ê°€ì„œ 'Create new token'ì„ í´ë¦­\n",
        "- Token type: 'Read'ë¡œ í•´ì£¼ì„¸ìš”.\n",
        "- Token name: ì•„ë¬´ê±°ë‚˜ ìƒê´€ ì—†ìŠµë‹ˆë‹¤.\n",
        "- (â­ ë§¤ìš° ì¤‘ìš”!) í† í° ë§Œë“œì‹œê³  **ë°˜ë“œì‹œ í† í° ë¬¸ìì—´ (hf_... ë¡œ ì‹œì‘í•¨)ì„ ë³µì‚¬í•´ì£¼ì„¸ìš”!**  \n",
        "í•œ ë²ˆë§Œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
        "\n",
        "### 3. google colabì—ì„œ í† í° ë“±ë¡í•˜ê¸°\n",
        "- ì™¼ìª½ì— ì—´ì‡  ëª¨ì–‘ ë²„íŠ¼(ë³´ì•ˆ ë¹„ë°€)ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.\n",
        "- ì´ë¦„: colab-gemma\n",
        "- ê°’: ë³µì‚¬í•´ë‘” í† í° ë¬¸ìì—´(hf_...ë¡œ ì‹œì‘í•¨)\n",
        "- ë…¸íŠ¸ë¶ ì•¡ì„¸ìŠ¤ : ìŠ¤ìœ„ì¹˜ í‚¤ê¸°(íŒŒë€ìƒ‰)\n",
        "\n",
        "### 4. gemma-2b-it ëª¨ë¸ ì‚¬ìš© ì•½ê´€ ë™ì˜\n",
        "\n",
        "https://huggingface.co/google/gemma-2b-it\n",
        "\n",
        "- (â­ ë§¤ìš° ì¤‘ìš”!) ë°˜ë“œì‹œ 2ë²ˆ ë‹¨ê³„ì—ì„œ í† í°ì„ ë°œê¸‰ë°›ì•˜ë˜ ë°”ë¡œ ê·¸ Hugging Face ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸ëœ ìƒíƒœì—ì„œ ìœ„ ë§í¬ë¡œ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤.\n",
        "- ë§í¬ë¡œ ë“¤ì–´ê°€ì„œ 'Access Gemma on Hugging Face' ë¶€ë¶„ì˜ ì•½ê´€ì„ ì½ê³  ë™ì˜ ë²„íŠ¼ì„ í´ë¦­í•´ ì£¼ì„¸ìš”.\n",
        "- \"You have been granted access to this model\"ì´ë¼ëŠ” ë©”ì‹œì§€ê°€ ëœ¨ë©´ ì„±ê³µì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwjo5_Uucxkw"
      },
      "source": [
        "### (2) í•„ìš”í•œ íˆ´ ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_nXPEsF7UWQ",
        "outputId": "68032301-5b20-4e09-984d-1e1094a71d1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install --upgrade -q transformers huggingface_hub peft \\\n",
        "  accelerate bitsandbytes datasets trl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00df732f"
      },
      "source": [
        "# HuggingFaceì— ë¡œê·¸ì¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# ë”°ìŒí‘œ ì•ˆì— ë³¸ì¸ì˜ ë³´ì•ˆ ë¹„ë°€ í‚¤ì˜ ì´ë¦„ì„ ì ìŠµë‹ˆë‹¤.(ì—¬ê¸°ì„  colab-gemma)\n",
        "try:\n",
        "    login(token=userdata.get(\"colab-gemma\"))\n",
        "except Exception as e:\n",
        "    print(f\"Error logging in to Hugging Face: {e}\")\n",
        "    print(\"Please make sure you have added your Hugging Face token to Colab secrets with the name 'colab-gemma'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_z4600bwvSq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558,
          "referenced_widgets": [
            "c5edfb688c5e4f1789ef405ce4cc5db2",
            "ebdc7f0cf1cb470f96ee84ac5b856849",
            "b17055b387374af5aeec69fa859c5356",
            "ee16fd84281c468b8d88ad1b855c2a4b",
            "1cbd8be1861c4d2aae79730d4acd801f",
            "129cadd9bf5b4d3b8826c1e2c6a3ef13",
            "8b11042e19f04674b276d5a145e8b483",
            "c2bac02accff4461a1be51e89a765775",
            "7b1b28182ede4cd08e0e1c2b9a14bc69",
            "41e417b067a5490ca76e0a6e21f6dfd9",
            "87e159f3247c488a9f245fea94fd2879",
            "72815cf026024605a0f549b4ce4a6fb5",
            "de1be98b91224fcba82193799b771b88",
            "6535c89a48de4a65bf443f840d7d8116",
            "1e3b7d74b4004c15a1aee4785951299c",
            "fd0b150be30a4647bc3c43830a92bfd3",
            "ad20cbc57c5e4852b7183bbfa7a29f7c",
            "34a867f9a21c4425b4a1270121ff5842",
            "6f3f46149fde49438ecf609b64305eb5",
            "68b13240939c42a5be85c419e80ed430",
            "21738358066d448195c4aa4e59fd0452",
            "1111f8349e6447a7adc46b69b3a5688b",
            "7cf3f0eb3f084b88b5fbc3ff85a01442",
            "133134380bf04de791e3980679ccabcf",
            "969de039aa0d40ed85c0630715c06303",
            "6d4a68a729924e26b952050cd6618d58",
            "1510e20a30114a3884753fe847e641d3",
            "f84f2af580da43f5b877858e74403a01",
            "3acc92a0ccd34467baf8b89584d44e81",
            "1ba36c52e3124253aa9eefcb5c791161",
            "8041bf864b7541a58574e0f2a3b5958a",
            "bfaa1c11ada348df9c50fe3c3c16b1c1",
            "2b6b45c0652d4ab5b82ef849244eb110",
            "8a7c264ca8064694bd8465f219d40391",
            "c53df6f20c3e4da38bbbe90f810853de",
            "4a5bc1ef21254b8db42e9df67d332736",
            "f6d6bb29f221439f98753d2b478f9fd7",
            "945166c651d6430b9634ad22971c1723",
            "5fb1dacdfb1b4ccc9641a2e1e3290bcf",
            "be73546e846047d496ac45e86651ffea",
            "969405c5538945f6bbb17618a6aef99b",
            "0820889b03cd432a806ddf35a97b0046",
            "e1c01619fc06499c942f23210bbbc53e",
            "fb6244b642fb46c5bcfcff74057624c1",
            "7e8d7a9a9151400c8b94edc009888987",
            "cafdbecdfb134b67993002ea4ef81261",
            "d77a36a9dced4466ac57c5d2b6ac5df8",
            "556ecdc02eb44c02a7f48b950480300f",
            "06a46251db474cd785b719efa9054f79",
            "5976fac421df44529b2bfdcb5eec467c",
            "871f43f1b7b84c0b98814489cd8096fd",
            "5b5de13cb47746e8bbb1fc921c235d37",
            "3150a33c46164ccfb20b3a8bef23ef8c",
            "f17263cc6bed45afad43454d7ef19b46",
            "c823ec78047648418e7916498d8c5a1f",
            "f99b22bfe5514886ba1e3c556077b288",
            "eb20dcf162c94f9b849f5f7ddae8df7f",
            "7825c5a4655d4ed49f34dde6a5a45d6d",
            "37a079e7f5504066800ee8efc51f0589",
            "497b5db17115480ba9fa4c9c6fe1ab94",
            "1b26cff3abea460fb9f870d885fdc254",
            "b272b653218a42779b713bed095a3ffc",
            "6ee3e08231384f95b740db0634218b6d",
            "d539cf3b8eae4ccc8031dbbc23956cf1",
            "6a64330ccbf64e56bc666153d2088721",
            "c60ce90787cb4c10869aa6818f213948",
            "46d9e4c41292406b99394e9a54bab497",
            "b729a044d23c4a8b9f93a8659f6ddc6a",
            "42d4bae05fd144ed9917d1ea236335b7",
            "4885f40222be471cb1f24da895ff941d",
            "12e7d4c8bfef48118d3502cd2e7bcefd",
            "e872bc45514b49349cb3288748a587b9",
            "8a146f6417bc413ca3d5908a53e5f368",
            "eb99e13da68242b5bb846b7d10b15d4b",
            "64d4da3a4d7147cca8f04bb6af810421",
            "6982f638f102406191952b6f0078f938",
            "a1ee215d67f8468e9b051f780e594f80",
            "3e91cb7bf3864561974f3b8ca155b599",
            "c3deced74abb489d92420a2698b9d21b",
            "fa22202704394ad5a0fbabebb2560a6f",
            "a0d8d13ee6a34963bb5c330c2d854c31",
            "bebe6b275e3f4eff843645c0464e92bd",
            "053bf20349c149a5a44c275603a8b357",
            "4407d8d343d14e2ea957e3dd311ab039",
            "ef71cf76c3b04337988d86a0e0da466b",
            "1aa34a949a814cbd8b37f1bd446b1ca0",
            "340ceeec9fad43699100a169295dfd9f",
            "35461dd408f54f24b3bf7c1059e0cded",
            "75cf4691b31c4089aa4741aceb5fa39a",
            "88127c87c1874ba1bc741016cae30971",
            "8091ed408eb140f5b0a0e0af896268ac",
            "4db136b7b8fb48fc855a641e767ab240",
            "b8d579511cee49de9bea4f2c62097e15",
            "127529c752e54075a70d3c343bf081a6",
            "0927b15b22e5493ab1c1ba30d37287bb",
            "d185754d8f014768ae35db5d9360d48f",
            "4e6b72f2f60d43c3b4e5358fffb1f7d3",
            "36cbee1bf5d04364be26b4d2d399430c",
            "2613c30042a64fe38b85cd43895d8fc0",
            "dbe763b1167e4cb4a416acfe765f8236",
            "d14c9ab29b4b4889962234926b76917e",
            "f7f94eb7aaac42349a0817d8ef9b21cf",
            "79d2bf751485417a9b41e6c065242b3b",
            "8499286fee3c46119c57457fd71ed5da",
            "32f9c4cdbb05431286d005b65b99eb0e",
            "82a7476521b948eba15947119596c526",
            "fa201fa5250a461c846126dbd93c1ead",
            "ef5ce854666b48fb8aba1598b60bfb15",
            "1d6a530af9484bd390cfb8dcd8a450dd",
            "bab270bc18fc4fcc8319719c262c8d19",
            "52b5c905f73e49b4a74c96b49ee52352",
            "737e08920540495faa213b2764200030",
            "4507d587012f4f1ea24effe8f6df790c",
            "88465326577b47b798fdc0192e6a7a3a",
            "400bf84773274a0a8a4347a8f7904eb0",
            "6872649fa3ed4b4398b8eecfb4247f87",
            "497898efd40c465ab79675529573067e",
            "123212aacb0d459789b102ad62b209d9",
            "32140fb8dc1a439a8ca29a3f41e8a50a",
            "f2bb7d1491884f78a94358ff28cf93a2",
            "2b99766f7edd4deab4661b4b14b1861a"
          ]
        },
        "outputId": "798c9a3d-e7e5-4af3-8900-8df36c4e0dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5edfb688c5e4f1789ef405ce4cc5db2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72815cf026024605a0f549b4ce4a6fb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cf3f0eb3f084b88b5fbc3ff85a01442"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a7c264ca8064694bd8465f219d40391"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e8d7a9a9151400c8b94edc009888987"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f99b22bfe5514886ba1e3c556077b288"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46d9e4c41292406b99394e9a54bab497"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e91cb7bf3864561974f3b8ca155b599"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75cf4691b31c4089aa4741aceb5fa39a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbe763b1167e4cb4a416acfe765f8236"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52b5c905f73e49b4a74c96b49ee52352"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# ê³„ì‚° ë¶€í•˜ë¥¼ ì¤„ì´ê³  ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ 'ì–‘ìí™”'í•©ë‹ˆë‹¤.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# sLM(gemma)ë¥¼ Q&A, ìš”ì•½ ë“±ì— í™œìš©í•˜ê¸° ìœ„í•´ 'ì§€ì‹œ íŠœë‹(Instruction-Tuned) ë²„ì „ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "# \"google/gemma-2b-it\" ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "model_id = \"******\"\n",
        "\n",
        "# í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    ************_config=bnb_config, # ì–‘ìí™”(quantization)\n",
        "    device_map={\"\":\"cuda:0\"} # Changed from \"0\" to \"cuda:0\"\n",
        ")\n",
        "\n",
        "# GPUë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlmGvFMg4a2q"
      },
      "source": [
        "## 3-2. sLM í™œìš©\n",
        "- sLMì€ LLMì„ ê²½ëŸ‰í™”í•œ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ì–¸ì–´ ëª¨ë¸ì˜ ê¸°ëŠ¥ì¸ ì§ˆì˜ì‘ë‹µ(ì¶”ë¡ ), ìš”ì•½, ë¶„ë¥˜(ê°ì •ë¶„ì„) ë“±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
        "- **ì†ë„ëŠ” ë¹„êµì  ë¹ ë¥´ì§€ë§Œ, ì„±ëŠ¥ì´ ê·¸ë‹¥ ì¢‹ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**\n",
        "\n",
        "### Gemma í™œìš© ì˜ˆì‹œ (Common Use Cases)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) ì§ˆì˜ì‘ë‹µ(Reasoning)\n",
        "\n",
        "#### ì›í•˜ëŠ” ì§ˆë¬¸ì„ í•œ ë²ˆ ë„£ì–´ë³´ì„¸ìš”!"
      ],
      "metadata": {
        "id": "7VylD-KiV2ow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGU1uuui4a2q",
        "outputId": "503eebe0-a162-4853-cc30-80a644bf9f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ì˜ˆì‹œ 1: ì§ˆì˜ì‘ë‹µ (Reasoning) ---\n",
            "**AIê°€ íšŒê³„ ë¶„ì•¼ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë  ìˆ˜ ìˆëŠ”ì§€**\n",
            "\n",
            "**1. ë°ì´í„° ë¶„ì„ ë° ì˜ˆì¸¡**\n",
            "* AIëŠ” íšŒê³„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì˜ˆì¸¡í•˜ì—¬ ë¯¸ë˜ ê²°ê³¼ë¥¼ ì¶”ì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "* ì˜ˆë¥¼ ë“¤ì–´, íšŒê³„ ìˆ˜ìµê³¼ ë¹„ìš©ì„ ì˜ˆì¸¡í•˜ê³ , íšŒê³„ ì„±ê³¼ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ ì „ëµì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "**2. íšŒê³„ ìë™í™”**\n",
            "* AIëŠ” íšŒê³„ ì—…ë¬´ë¥¼ ìë™í™”í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¥¼ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "* ì˜ˆë¥¼ ë“¤ì–´, ë°ì´í„° ì…ë ¥, ê³„ì • ê´€ë¦¬, ë³´ê³ ì„œ ìƒì„± ë“±ì„ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "**3. íšŒê³„ ì˜ì‚¬ ê²°ì • ì§€ì›**\n",
            "* AIëŠ” íšŒê³„ ì˜ì‚¬ ê²°ì •ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆëŠ” ì •ë³´ì™€ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "* ì˜ˆë¥¼ ë“¤ì–´, íšŒê³„ ì„±ê³¼ì™€ ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•˜ê³ , íˆ¬ì ë° ì¬ë¬´ ê³„íšì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "**4. íšŒê³„ ì†”ë£¨ì…˜ ì œê³µ**\n",
            "* AIëŠ” íšŒê³„ ì†”ë£¨ì…˜ì„ ì œê³µí•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "* ì˜ˆë¥¼ ë“¤ì–´, íšŒê³„ ì‹œìŠ¤í…œ êµ¬ì¶•, ë°ì´í„°\n"
          ]
        }
      ],
      "source": [
        "print(\"--- ì˜ˆì‹œ 1: ì§ˆì˜ì‘ë‹µ (Reasoning) ---\")\n",
        "\n",
        "# content : '~~' ë”°ìŒí‘œ ì•ˆì— ì§ˆë¬¸ì„ ì¨ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "# í•˜ê³  ì‹¶ì€ ì§ˆë¬¸ ì•„ë¬´ê±°ë‚˜ ë„£ì–´ë³´ì„¸ìš”!\n",
        "\n",
        "# \"role\": \"user\" ì…ë‹ˆë‹¤.\n",
        "\n",
        "chat = [\n",
        "    # Gemma ëª¨ë¸ì˜ ì±„íŒ… í˜•ì‹ì— ë§ì¶° 'role'ê³¼ 'content'ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "    { \"role\": \"****\", \"content\": \"AIê°€ íšŒê³„ ë¶„ì•¼ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë  ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì¤˜\" }\n",
        "]\n",
        "\n",
        "# 1. ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸('chat')ë¥¼ Gemma ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ê³µì‹ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# 2. ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ëª¨ë¸ì´ ì…ë ¥ë°›ì„ ìˆ˜ ìˆëŠ” í† í° ID í…ì„œë¡œ ë³€í™˜í•˜ê³ , GPU('device')ë¡œ ë³´ëƒ…ë‹ˆë‹¤.\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 3. í† í°í™”ëœ ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ í…ìŠ¤íŠ¸ \"\"ìƒì„±\"\"ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ìµœëŒ€ 256ê°œì˜ ìƒˆ í† í° ìƒì„±)\n",
        "# ìƒì„±ì„ ìœ„í•´ 'generate' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "outputs = model.********(**inputs, max_new_tokens=256) # **inputsì—ì„œ **ëŠ” ë¹ˆì¹¸ì´ ì•„ë‹™ë‹ˆë‹¤.\n",
        "\n",
        "# 4. 'outputs'ì—ëŠ” 'ì…ë ¥'ê³¼ 'ë‹µë³€'ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "# ë”°ë¼ì„œ ì›ë³¸ ì…ë ¥ í† í°ì˜ ê¸¸ì´(ì…ë ¥, 'inputs.input_ids.shape[1]')ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ê·¸ ì´í›„ì— 'ìƒˆë¡œ ìƒì„±ëœ' í† í°(ë‹µë³€)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "new_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
        "\n",
        "# 5. ì¶”ì¶œëœ í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ë¡œ \"\"ë””ì½”ë”©\"\"í•©ë‹ˆë‹¤.\n",
        "# ë””ì½”ë”©ì„ ìœ„í•´ 'decode' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "text = tokenizer.******(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "# 6. ìµœì¢… ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) ìš”ì•½ (Summarization)\n",
        "\n",
        "#### ì›í•˜ëŠ” ê¸€ì„ ë„£ì–´ì„œ ìš”ì•½í•´ ë³´ì„¸ìš”!"
      ],
      "metadata": {
        "id": "5jqUC9Arb6pf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPxJeg244a2q",
        "outputId": "392a0a37-58d6-4ff5-fa65-990b6a145000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ì˜ˆì‹œ 2: ìš”ì•½ (Summarization) ---\n",
            "1. ìµœê·¼ ì„ ë‹¬ ë™ì•ˆ êµ­ë‚´ ì¦ì‹œ ì½”ìŠ¤í”¼ê°€ 1,000p ì•ˆíŒìœ¼ë¡œ ì˜¤ë¥¼ ë§Œí¼ ê°•ì„¸ì¸ë° í™˜ìœ¨ì´ ê¸‰ë“±í•˜ëŠ” í˜„ìƒì´ ë‚˜íƒ€ë‚˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "\n",
            "2. ì›, ë‹¬ëŸ¬ í™˜ìœ¨ì˜ ì•½ì„¸ ì´ìœ ëŠ” í™˜ìœ¨ ìƒìŠ¹ì˜ ì„±ê²©ê³¼ ì£¼ê°€ ìƒìŠ¹ì„ ì´ë„ëŠ” ì£¼ì²´ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ê³ ìš”.\n",
            "\n",
            "\n",
            "3. í•œêµ­ì€ ë¯¸êµ­ ì¤‘ì‹¬ì˜ ê¸€ë¡œë²Œ ìš”ì¸ë³´ë‹¤ëŠ” êµ­ë‚´ ìš”ì¸ë³´ë‹¤ëŠ” ë¯¸êµ­ êµ­ì±„ ê¸ˆë¦¬ê°€ ê¸‰ë“±í•˜ê³ ì•ˆì „ìì‚° ì„ í˜¸ ì‹¬ë¦¬ê°€ ë”í•´ì§€ë©´ì„œ ë‹¬ëŸ¬ì¸ë±ìŠ¤ë„ ê°•ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- ì˜ˆì‹œ 2: ìš”ì•½ (Summarization) ---\")\n",
        "\n",
        "# text_to_summarize = \"\"\"~~~\"\"\" ë”°ìŒí‘œ ì•ˆì— ìš”ì•½í•˜ê³  ì‹¶ì€ ê¸€ì„ ë„£ì–´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\"\"\n",
        "# ìš”ì•½í•˜ê³  ì‹¶ì€ ê¸€ ì•„ë¬´ê±°ë‚˜ ë„£ì–´ë³´ì„¸ìš”!\n",
        "\n",
        "text_to_summarize = \"\"\"\n",
        "ì½”ìŠ¤í”¼ ì„ ë‹¬ ë™ì•ˆ 1,000p ì˜¬ëëŠ”ë° í™˜ìœ¨ ê¸‰ë“±í•œ ì´ìœ ëŠ”?\n",
        "\n",
        "[ì•µì»¤]\n",
        "ìµœê·¼ ì„ ë‹¬ ë™ì•ˆ êµ­ë‚´ ì¦ì‹œ ì½”ìŠ¤í”¼ê°€ 1,000p ì•ˆíŒìœ¼ë¡œ ì˜¤ë¥¼ ë§Œí¼ ê°•ì„¸ì¸ë°í™˜ìœ¨ì´ ê¸‰ë“±í•˜ëŠ” í˜„ìƒì´ ë‚˜íƒ€ë‚˜ê³  ìˆìŠµë‹ˆë‹¤.ì›Â·ë‹¬ëŸ¬ í™˜ìœ¨ì˜ ì•½ì„¸ ì´ìœ ì™€ ì£¼ì‹ ì‹œì¥ ë™í–¥ì„ ìµœì¬ë¯¼ í•´ì„¤ìœ„ì›ê³¼ í•¨ê»˜ ì§šì–´ë³´ê² ìŠµë‹ˆë‹¤. ì–´ì„œ ì˜¤ì‹­ì‹œì˜¤. í†µìƒì ìœ¼ë¡œ ì£¼ì‹ ì‹œì¥ì´ ê´œì°®ìœ¼ë©´ í™˜ìœ¨ì€ ë‚´ë ¤ê°€ëŠ” ê²Œ ì •ì„¤ì¸ë°ì§€ê¸ˆì€ í™˜ìœ¨ì´ ê¸‰ë“±í•˜ê³  ìˆê±°ë“ ìš”. ì–´ë–¤ ì´ìœ  ë•Œë¬¸ì¼ê¹Œìš”?\n",
        "\n",
        "[ê¸°ì]\n",
        "ê·¸ë ‡ìŠµë‹ˆë‹¤. ì£¼ì‹ ì‹œì¥ê³¼ í™˜ìœ¨ì€ ë¹„ìŠ·í•˜ê²Œ ì›€ì§ì´ëŠ” ê²Œ í†µìƒì ì¸ë°ìš”. ê·¸ëŸ°ë° ìµœê·¼ì—ëŠ” ì—­ì„¤ì ì´ê³  íŠ¹ì´í•œ í˜„ìƒì´ ë‚˜íƒ€ë‚œë‹¤ê³  ë³¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ì£¼ìš” ì›ì¸ì€ í™˜ìœ¨ ìƒìŠ¹ì˜ ì„±ê²©ê³¼ ì£¼ê°€ ìƒìŠ¹ì„ ì´ë„ëŠ” ì£¼ì²´ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ê³ ìš”. í™˜ìœ¨ì´ ì˜¤ë¥´ëŠ” ê±´ êµ­ë‚´ ìš”ì¸ë³´ë‹¤ëŠ” ë¯¸êµ­ ì¤‘ì‹¬ì˜ ê¸€ë¡œë²Œ ìš”ì¸ì´ ë” í¬ê²Œ ì‘ìš©í•˜ê³  ìˆë‹¤ê³  ë´ì•¼ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë¯¸ ì—°ì¤€ì´ ë¬¼ê°€ ì•ˆì •ì„ ìœ„í•´ì„œ í˜„ì¬ì˜ ê¸°ì¤€ê¸ˆë¦¬ ì¸í•˜ë¥¼ ë” ëŠ¦ì¶œ ê°€ëŠ¥ì„±ì´ í¬ë‹¤ëŠ” ì „ë§ì´ ìµœê·¼ ìš°ì„¸í•œ ìƒí™©ì´ê³  ì´ ë•Œë¬¸ì— ë¯¸êµ­ êµ­ì±„ ê¸ˆë¦¬ê°€ ê¸‰ë“±í•˜ê³ ì•ˆì „ìì‚° ì„ í˜¸ ì‹¬ë¦¬ê°€ ë”í•´ì§€ë©´ì„œ ë‹¬ëŸ¬ì¸ë±ìŠ¤ë„ ê°•ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "[ì•µì»¤]\n",
        "ë‹¬ëŸ¬ì¸ë±ìŠ¤ ì–˜ê¸°í•´ ì£¼ì…¨ëŠ”ë° ê¸ˆìœµì‹œì¥ì—ì„œ ë¯¸êµ­ ë‹¬ëŸ¬í™”ì˜ ê°€ì¹˜ë¥¼ ì¸¡ì •í•˜ëŠ” ê²Œ ë‹¬ëŸ¬ì¸ë±ìŠ¤ì–ì•„ìš”. ìµœê·¼ì— ì–¼ë§ˆë‚˜ ê°•ì„¸ì¸ ê±´ê°€ìš”?\n",
        "\n",
        "[ê¸°ì]\n",
        "ìœ ë¡œì™€ ì—”, íŒŒìš´ë“œ ë“± ì£¼ìš” 6ê°œêµ­ í†µí™” ê·¸ëŸ¬ë‹ˆê¹Œ ì˜êµ­ íŒŒìš´ë“œ, ìºë‚˜ë‹¤ ë‹¬ëŸ¬ í¬í•¨í•´ì„œ 6ê°œ ë‚˜ë¼ ëŒ€ë¹„ ë¯¸êµ­ ë‹¬ëŸ¬í™”ì˜ í‰ê· ì ì¸ ê°€ì¹˜ë¥¼ ì¸¡ì •í•œ ì§€ìˆ˜ê°€ ë‹¬ëŸ¬ì¸ë±ìŠ¤ì¸ë° 1973ë…„ 3ì›” ê¸°ì¤€ì  100ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ìˆ˜ì¹˜ë³´ë‹¤ ë†’ìœ¼ë©´ ë‹¬ëŸ¬ ê°€ì¹˜ê°€ ê°•ì„¸, ë‚®ìœ¼ë©´ ì•½ì„¸ë¥¼ ì˜ë¯¸í•˜ëŠ” ê±°ê±°ë“ ìš”. ìµœê·¼ ì„ ë‹¬ ì „ì—ëŠ” 98ëŒ€ ì •ë„ì—ì„œ ì›€ì§ì´ë‹¤ê°€ 9ì›” ë§ì—ëŠ” 96ëŒ€ê¹Œì§€ ë‚´ë ¤ê°”ìŠµë‹ˆë‹¤. ë‚´ë ¤ê°”ë‹¤ê°€ ì§€ë‚œë‹¬ë¶€í„° ì˜¤ë¥´ê¸° ì‹œì‘í•´ 99âˆ¼100ëŒ€ë¥¼ì™”ë‹¤ê°”ë‹¤í•˜ê³  ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤. ê·¸ëŸ°ë° ì˜¬í•´ ì´ˆì—ëŠ” 110ê¹Œì§€ ì˜¬ëì—ˆëŠ”ë°ê·¸ë•Œë„ ê·¸ë˜í”„ë¥¼ ë³´ë©´ ì•Œê² ìŠµë‹ˆë‹¤ë§Œ ê·¸ë•Œë„ ì›ë‹¬ëŸ¬ í™˜ìœ¨ì€ 1,460ì›âˆ¼1,480ì›ì´ì—ˆëŠ”ë° ì§€ë‚œì£¼ ê¸ˆìš”ì¼ ì›Â·ë‹¬ëŸ¬ í™˜ìœ¨ì€ 7ê°œì›” ë§Œì— 1,450ì›ì„ ë„˜ì–´ì„œì„œ ì¢…ê°€ ê¸°ì¤€ìœ¼ë¡œ 1458ì›ì— ê·¼ì ‘í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‹ˆê¹Œ ê·¸ë•Œ ë‹¬ëŸ¬ì¸ë±ìŠ¤ê°€ ê°•ì„¸ì¼ ë•Œí•˜ê³  ë¹„ìŠ·í•œ ì •ë„ë¡œ ê°”ë‹¤ëŠ” ê±°ê³ ìš”. ë‹¬ëŸ¬ì¸ë±ìŠ¤ê°€ 10pë‚˜ ë‚®ì€ë°ë„ì›ë‹¬ëŸ¬ í™˜ìœ¨ì€ ê·¸ë•Œì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì´ë‹ˆê¹ŒìƒëŒ€ì ìœ¼ë¡œ ì›í™”ê°€ ê·¸ë•Œë³´ë‹¤ ë” í˜ì„ ëª» ì“°ê³  ìˆë‹¤ê³  ë³´ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì›í™” ê°€ì¹˜ëŠ” ì¼ì£¼ì¼ ë™ì•ˆ 2%ë‚˜ ê¸‰ë½í–ˆê³ ìš”. ì¤‘ìš” í†µí™”êµ­ ê°€ìš´ë° ì ˆí•˜ìœ¨ì´ ê°€ì¥ í½ë‹ˆë‹¤. ë˜ í•œê°€ì§€ ì£¼ëª©í•  ê²ƒì€ ì—”í™” ì•½ì„¸ê°€ ê³„ì†ë˜ê³  ìˆë‹¤ëŠ” ì ì¸ë° ì‹œì¥ì—ì„œ ìš°ë ¤ê°€ ë‚˜ì˜¤ëŠ” ìˆ˜ì¤€ì…ë‹ˆë‹¤. 1ë‹¬ëŸ¬ì— 150ì—”ëŒ€ ì¤‘ë°˜ì—ì„œ ì›€ì§ì´ê³  ìˆê³ ìš”. 3ë…„ ì „ì— 1ë‹¬ëŸ¬ì— 150ì—”ì„ ë„˜ì–´ì„°ëŠ”ë° ì¢€ì²˜ëŸ¼ ì•½ì„¸ë¥¼ íƒˆì¶œí•  ê¸°ë¯¸ë¥¼ ë³´ì´ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì™€ ê²½ìŸ ê´€ê³„ì— ìˆëŠ” ì¼ë³¸ ì—”í™”ì˜ ê°€ì¹˜ê°€ ë–¨ì–´ì§€ë©´ ìš°ë¦¬ ìˆ˜ì¶œ ê¸°ì—…ì˜ ê°€ê²© ê²½ìŸë ¥ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì•„ì ¸ ì›í™” ì•½ì„¸ ì••ë ¥ì´ ë” ì»¤ì§„ë‹¤ëŠ” ê±¸ ì˜ë¯¸í•œë‹¤ê³  ë³´ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "[ì•µì»¤]\n",
        "ê·¸ëŸ°ë° ì—°ë§ì—ëŠ” ì›Â·ë‹¬ëŸ¬ í™˜ìœ¨ì´ ë” ì˜¤ë¥¼ ìˆ˜ë„ ìˆë‹¤, ì´ë ‡ê²Œ ì „ë§í•˜ëŠ” ì „ë¬¸ê°€ë“¤ë„ ìˆëŠ” ê²ƒ ê°™ë”ë¼ê³ ìš”.\n",
        "\n",
        "[ê¸°ì]\n",
        "ì—¬ì „íˆ ë†’ì€ ë³€ë™ì„±ì„ ì˜ˆìƒí•˜ëŠ” ê²Œ ì¤‘ë¡ ì´ê³ ìš”. ìƒë‹¨ì„ 1,500ì›ë„ ì—´ì–´ë‘¬ì•¼ í•œë‹¤ëŠ” ì „ë§ë„ ë‚˜ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. ê°€ì¥ í° ê²Œ ì—­ì‹œ ë¯¸êµ­ì˜ ê¸ˆë¦¬ ì¸í•˜ ì—¬ë¶€ì¸ë°ìš”. ë¯¸êµ­ì€ ê¸°ì¤€ê¸ˆë¦¬ê°€ í•˜ë‹¨ì´ 3.75%, ìƒë‹¨ì´ 4%ê³ ìš”. ìš°ë¦¬ ê¸°ì¤€ê¸ˆë¦¬ëŠ” 2.5% ìˆ˜ì¤€ì´ê±°ë“ ìš”. ë¯¸êµ­ì€ ì§€ë‚œë‹¬ 0.25%p ê¸ˆë¦¬ ì¸í•˜ë¥¼ ì‹œí–‰í–ˆìŠµë‹ˆë‹¤. ì˜¬í•´ëŠ” ë¯¸ ì—°ì¤€ì˜ FOMC íšŒì˜ê°€ ì´ì œ í•œ ì°¨ë¡€ë§Œ ë‚¨ì•˜ëŠ”ë°ìš°ë¦¬ ì‹œê°ìœ¼ë¡œ ë‹¤ìŒ ë‹¬ 11ì¼ì— ì—´ë¦½ë‹ˆë‹¤. ì˜¬í•´ì˜ í†µí™”ì •ì±… ë°©í–¥ì„ ë§ˆë¬´ë¦¬í•˜ê³ ë‚´ë…„ ê¸ˆë¦¬ ê²½ë¡œì— ëŒ€í•œ ì¤‘ìš”í•œ ë‹¨ì„œë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ì‹œì¥ì˜ ì´ëª©ì´ ì§‘ì¤‘ë¼ ìˆëŠ”ë° ìµœê·¼ ìœ ëŸ½ê³¼ ì¤‘êµ­ì˜ ê²½ê¸° ë‘”í™” ê°€ëŠ¥ì„±ì´ ì»¤ì§€ë©´ì„œ íˆ¬ììë“¤ì´ ë¦¬ìŠ¤í¬ê°€ í° ì›í™”ì™€ ê°™ì€ ì‹ í¥êµ­ ìì‚°ì„ íšŒí”¼í•˜ê³  ì•ˆì „í•œ ìì‚°ìœ¼ë¡œ ì˜®ê²¨ê°€ê³  ìˆëŠ” ì¶”ì„¸ê³ ìš”. ì´ ë•Œë¬¸ì— ì™¸í™˜ì‹œì¥ì—ì„œ ë‹¬ëŸ¬ì˜ í¬ì†Œì„±ì„ ë†’ì—¬ ì›ë‹¬ëŸ¬ í™˜ìœ¨ì„ ëŒì–´ì˜¬ë¦¬ëŠ” íš¨ê³¼ë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "[ì•µì»¤]\n",
        "ì§€ê¸ˆê¹Œì§€ëŠ” ê¸€ë¡œë²Œ ìš”ì¸ë“¤ì„ ì„¤ëª…í•´ ì£¼ì…¨ëŠ”ë° ê·¸ë ‡ë‹¤ë©´ ì›í™”ê°€ ì•½ì„¸ì¸ êµ­ë‚´ì  ìš”ì¸ë„ ìˆì„ê¹Œìš”?\n",
        "\n",
        "[ê¸°ì]\n",
        "ì•„ë¬´ë˜ë„ ìœ ê°€ì¦ê¶Œì‹œì¥ì—ì„œ ì°¾ì„ ìˆ˜ ìˆì„ ê²ƒ ê°™ê³ ìš”. ì½”ìŠ¤í”¼ê°€ 4,000ì„ ì„ ë‚´ì¤¬ì–´ìš”. 3953ì„ ì—ì„œ ë§ˆë¬´ë¦¬ëê³  ê¸ˆìš”ì¼ì—ë§Œ 72í¬ì¸íŠ¸ í•˜ë½í•´ì„œ 4000ì„ ì´ ë¬´ë„ˆì¡ŒëŠ”ë° ê°€ì¥ í° ì›ì¸ì´ ì™¸êµ­ì¸ë“¤ì´ ì£¼ì‹ì„ íŒ”ê³  ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì£¼ê°„ ê¸°ì¤€ ì™¸êµ­ì¸ì˜ ìœ ê°€ì¦ê¶Œì‹œì¥ ìˆœë§¤ë„ì•¡ì´ ì—­ëŒ€ ìµœëŒ€ ìˆ˜ì¤€ì´ê³ ìš”. ì§€ë‚œì£¼ ì™¸êµ­ì¸ì˜ ì½”ìŠ¤í”¼ ì‹œì¥ ìˆœë§¤ë„ì•¡ì€ 7ì¡° 2,640ì–µìœ¼ë¡œ ì§‘ê³„ëëŠ”ë°,ì§ì „ ì—­ëŒ€ 1ìœ„ ê¸°ë¡ì€ ì§€ë‚œ 2021ë…„ 8ì›” ë‘˜ì§¸ ì£¼ì— ê¸°ë¡í•œ 7ì¡°450ì–µ ì›ì´ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì§€ë‚œ 4ì¼ì—ëŠ” ì™¸êµ­ì¸ë“¤ì´ 2ì¡° 2,300ì–µ ì› ê°€ê¹Œì´ ë‚´ë‹¤ íŒ”ë©´ì„œ 4ë…„ 3ê°œì›” ë§Œì— ìµœëŒ€ì¹˜ë¥¼ ê¸°ë¡í–ˆê³ ìš”. ì¦ê¶Œê°€ì—ì„œëŠ” ë‹¹ë¶„ê°„ ì™¸êµ­ì¸ì˜ ë§¤ë„ì„¸ê°€ ì§€ì†í•  ê²ƒìœ¼ë¡œ ì „ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì›ë‹¬ëŸ¬ ìƒìŠ¹í•˜ëŠ” ì´ìœ ë„ ì™¸êµ­ì¸ì˜ íˆ¬ìì‹¬ë¦¬ë¥¼ ìœ„ì¶•í•˜ëŠ” ìš”ì†Œë¡œ ì‘ìš©í•˜ê³  ìˆê³  ê·¸ëŸ°ë° í•œêµ­ì€í–‰ì€ ê²½ê¸° ì¹¨ì²´ì™€ ë†’ì€ ê°€ê³„ ë¶€ì±„ ë¶€ë‹´ ë•Œë¬¸ì— ê¸ˆë¦¬ë¥¼ ì˜¬ë¦¬ê¸° ì–´ë ¤ìš´ ë”œë ˆë§ˆì— ì²˜í•´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” í•œêµ­ì€í–‰ì´ í™˜ìœ¨ ë°©ì–´ë³´ë‹¤ëŠ” êµ­ë‚´ ê²½ì œë¥¼ ìš°ì„ í•  ìˆ˜ ìˆë‹¤ëŠ” ì‹ í˜¸ë¥¼ ì¤˜ì„œì›í™” ì•½ì„¸ë¥¼ ë¶€ì¶”ê¸¸ ìˆ˜ ìˆë‹¤ëŠ” ì§€ì ì…ë‹ˆë‹¤.\n",
        "\n",
        "[ì•µì»¤]\n",
        "ê·¸ë¦¬ê³  ë˜ ë¯¸ ì—°ë°©ì •ë¶€ ì…§ë‹¤ìš´ì´ 40ì¼ ê°€ê¹Œì´ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆê¹Œ? ì´ ë¶€ë¶„ë„ ìš°ë¦¬ ê²½ì œë‚˜ ê¸ˆìœµì‹œì¥ì— ì˜í–¥ì„ ë¯¸ì¹ ê¹Œìš”?\n",
        "\n",
        "[ê¸°ì]\n",
        "ë‹¨ê¸° ì…§ë‹¤ìš´ì€ í¬ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë° í•˜ì§€ë§Œ ì¥ê¸°ì ìœ¼ë¡œ ë¯¸êµ­ ê²½ì œ ì„±ì¥ ë‘”í™”ì™€ ì†Œë¹„ ìœ„ì¶•ì„ í†µí•´ì„œ ìš°ë¦¬ ê¸ˆìœµ ì‹œì¥ê³¼ ì‹¤ë¬¼ ê²½ì œ ì „ë°˜ì— ë¶€ì •ì ì¸ íŒŒì¥ì„ ë¯¸ì¹˜ëŠ” ì•…ì¬ë¡œ ì‘ìš©í•  ê°€ëŠ¥ì„±ì´ êµ‰ì¥íˆ í¬ê³ ìš”. ì´ë¯¸ ê·¸ ì•…ì˜í–¥ì€ ìš°ë¦¬ ê²½ì œì—ë„ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆë‹¤ê³  ë³´ëŠ” ê²Œ ë§ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì…§ë‹¤ìš´ ì¥ê¸°í™” ì‹œ ì •ë¶€ ê¸°ê´€ì˜ ì—…ë¬´ê°€ ì¤‘ë‹¨ë˜ë©´ì„œ, ì¤‘ë‹¨ë˜ë©´ ì˜ˆë¥¼ ë“¤ë©´ ì¤‘ìš”í•œ ê³ ìš©ë³´ê³ ì„œì„œë‚˜ êµ­ë‚´ì´ìƒì‚° ê°™ì€ í•µì‹¬ ê²½ì œì§€í‘œì˜ ë°œí‘œê°€ ì§€ì—°ë˜ê±°ë“ ìš”. ê°€ì¥ ì¤‘ìš”í•œ ê²Œ ë¯¸êµ­ì˜ í†µí™”ì •ì±…ì¸ë° ë¯¸ ì—°ì¤€ì€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸ˆë¦¬ ì •ì±…ì„ ê²°ì •í•©ë‹ˆë‹¤. ê·¸ë ‡ê²Œ ë˜ë©´ ì£¼ìš” ì§€í‘œë¥¼ í™•ì¸í•  ìˆ˜ ì—†ê²Œ ë˜ë©´ í†µí™”ì •ì±… ê²°ì •ì— ì°¨ì§ˆì´ ìƒê¸°ê²Œ ë˜ê³ ìš”. ì´ëŠ” ê³§ ì‹œì¥ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ë”ìš± í™•ëŒ€í•˜ëŠ” ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê²Œ ë©ë‹ˆë‹¤. ë¯¸êµ­ ì†Œë¹„ì ì‹¬ë¦¬ê°€ ìœ„ì¶•ë˜ë©´ ìš°ë¦¬ì˜ ì£¼ë ¥ ìˆ˜ì¶œí’ˆì¸ ìë™ì°¨ì™€ ì „ìì œí’ˆ ê°™ì€ ë¯¸êµ­ ìˆ˜ì¶œ ìˆ˜ìš”ë¥¼ ë‘”í™”ì‹œí‚¬ ìœ„í—˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "[ì•µì»¤]\n",
        "ì…§ë‹¤ìš´ì´ í•œ ë‹¬ ë„˜ì€ ìƒí™©ì¸ë° ì–¸ì œì¯¤ í•´ê²°ë  ê²ƒìœ¼ë¡œ ì „ë§ì„ í•˜ì‹œë‚˜ìš”?\n",
        "\n",
        "[ê¸°ì]\n",
        "ì¥ë‹´í•  ìˆ˜ ìˆëŠ” ìƒí™©ì´ ì•„ë‹ˆê³ ìš”. 2018âˆ¼2019ë…„ì˜ 35ì¼ì¸ ì—­ëŒ€ ìµœì¥ ê¸°ë¡ì„ ì´ë¯¸ ë„˜ì–´ì„°ìŠµë‹ˆë‹¤. ì°¸ê³ ë¡œ ë¯¸êµ­ì˜ íšŒê³„ì—°ë„ëŠ” 10ì›”ì— ì‹œì‘í•´ì„œ ì´ë“¬í•´ 9ì›”ì— ë§ˆë¬´ë¦¬ë˜ëŠ”ë° íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì´ ì…§ë‹¤ìš´ì„ ì •ì¹˜ì  ìˆ˜ë‹¨ìœ¼ë¡œí™œìš©í•˜ë ¤ëŠ” ì˜ì§€ê°€ ê°•í•´ì„œ ê³¼ê±°ì˜ ì…§ë‹¤ìš´ë³´ë‹¤ í•´ê²°ì´ ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆë‹¤ëŠ” ê²½ê³ ê°€ ì§€ë°°ì ì…ë‹ˆë‹¤. ì…§ë‹¤ìš´ì€ í•œêµ­ ì¦ì‹œì˜ í•µì‹¬ì ì¸ ë¶ˆì•ˆ ìš”ì¸ ê°€ìš´ë° í•˜ë‚˜ì´ê¸°ë„ í•˜ê³ ìš”. íˆ¬ìì, ê·¸ëŸ¬ë‹ˆê¹Œ íŠ¹íˆ ì™¸êµ­ì¸ íˆ¬ììì˜ ì…€ ì½”ë¦¬ì•„ë¥¼ ë”ìš± ë¶€ì¶”ê¸¸ ê°€ëŠ¥ì„±ë„ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "[ì•µì»¤]\n",
        "ê·¸ë¦¬ê³  ì–¼ë§ˆ ì „ì— í•œë¯¸ ê´€ì„¸í˜‘ìƒ íƒ€ê²° ì†Œì‹ì´ ìˆì—ˆì–ì•„ìš”. ì—°ê°„ 200ì–µ ë‹¬ëŸ¬ì”© íˆ¬ìí•˜ê¸°ë¡œ í•œ ê²ƒë„ ì›ë‹¬ëŸ¬ í™˜ìœ¨ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤ê³  ë´ì•¼ê² ì£ ?\n",
        "\n",
        "[ê¸°ì]\n",
        "ì•„ë¬´ë˜ë„ í™˜ìœ¨ ìƒìŠ¹ ì••ë ¥ìœ¼ë¡œ ì‘ìš©í•  ê°€ëŠ¥ì„±ì´ í¬ë‹¤ê³  ë´ì•¼ í•  ê²ƒ ê°™ê³ ìš”. ì™¸í™˜ì‹œì¥ì˜ ìˆ˜ê¸‰ ë¶ˆê· í˜•ì„ ì‹¬í™”í•˜ëŠ” êµ¬ì¡°ì  ìš”ì¸ìœ¼ë¡œ ì‘ìš©í•  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. ë‹¨ê¸°ì ìœ¼ë¡œ ê´€ì„¸ë¼ëŠ” ë¶ˆí™•ì‹¤ì„±ì„ í•´ì†Œí•  ìˆ˜ ìˆì§€ë§Œ, ì¥ê¸°ì ìœ¼ë¡œëŠ” ì§€ì†ì ì´ê³  êµ¬ì¡°ì ì¸ ìƒìŠ¹ ì••ë ¥ìœ¼ë¡œ ì‘ìš©í•  ê²ƒ ê°™ê³ ìš”. ì´ ë•Œë¬¸ì— ë‹¹ë¶„ê°„ì€ 1,200ì›ì´ë‚˜ 1,300ì›ëŒ€ ì´ˆë°˜ í™˜ìœ¨ì€ ê¸°ëŒ€í•˜ê¸° ì–´ë µë‹¤ëŠ” ë¶„ì„ì´ ì§€ë°°ì ì…ë‹ˆë‹¤. ì „ë¬¸ê°€ë“¤ì€ ëŒ€ë¯¸ íˆ¬ìì— ë”°ë¥¸ ìˆ˜ê¸‰ ë¶€ë‹´ì´ ê²¹ì³ì„œ 1,400ì›ëŒ€ í™˜ìœ¨ì€ 'ë‰´ ë…¸ë©€'ë¡œ ìë¦¬ ì¡ì„ ê°€ëŠ¥ì„±ì´ í¬ë‹¤ê³  ìš°ë ¤í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "[ì•µì»¤]\n",
        "ëìœ¼ë¡œ ì´ë²ˆ ì£¼ êµ­ë‚´ ì£¼ì‹ ì‹œì¥,ì „ë¬¸ê°€ë“¤ì€ ì–´ë–»ê²Œ ì „ë§í•˜ê³  ìˆëŠ”ì§€ ì„¤ëª…í•´ ì£¼ì‹œì£ .\n",
        "\n",
        "[ê¸°ì]\n",
        "ì „ë°˜ì ìœ¼ë¡œ ì´ë²ˆ ì£¼ëŠ” ê¸€ë¡œë²Œ ë¶ˆí™•ì‹¤ì„±ì´ ê·¹ë„ë¡œ ë†’ì€ ìƒí™©ì—ì„œ ë³€ë™ì„±ì´ ë§¤ìš° í´ ê²ƒìœ¼ë¡œ ì „ë§í•˜ê³  ìˆê³ ìš”. ë‚™í­ ê³¼ëŒ€ ê¸°ìˆ ì£¼ì™€ ì •ì±… ìˆ˜í˜œê°€ ì˜ˆìƒë˜ëŠ” ì¢…ëª©ì„ ì„ ë³„ì ìœ¼ë¡œ ì ‘ê·¼í•˜ëŠ” ì „ëµì„ ê¶Œê³ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‹ˆê¹Œ ë§Œì•½ ì´ë²ˆ ì£¼ì— ì…§ë‹¤ìš´ ê´€ë ¨ íƒ€ê²° ì†Œì‹ì´ ë‚˜ì˜¤ë©´ë‹¨ê¸°ì ìœ¼ë¡œ ê°€ì¥ ê°•ë ¥í•œ ì¦ì‹œ ë°˜ë“± ë™ë ¥ì´ ë  ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "[ì•µì»¤]\n",
        "ì•Œê² ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ë§ì”€ ì—¬ê¸°ê¹Œì§€ ë“£ê² ìŠµë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ìµœì¬ë¯¼ í•´ì„¤ìœ„ì›ê³¼ í•¨ê»˜í–ˆìŠµë‹ˆë‹¤. ê³ ë§™ìŠµë‹ˆë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "# content : '~~~'ì— ìš”ì²­ì‚¬í•­ì„ ì ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ìœ„ ê¸€ì„ ì„¸ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ë‹¬ë¼ê³  ìš”ì²­í–ˆìŠµë‹ˆë‹¤.\n",
        "# \"role\": \"user\" ì…ë‹ˆë‹¤.\n",
        "\n",
        "chat = [\n",
        "    { \"role\": \"****\", \"content\": f\"Could you summarize the following text in three sentence?\\n\\nText:\\n{text_to_summarize}\" }\n",
        "]\n",
        "\n",
        "# ì—¬ê¸°ë¶€í„°ëŠ” ê° ì‚¬ë¡€ ëª¨ë‘ ì½”ë“œê°€ ë™ì¼í•©ë‹ˆë‹¤.\n",
        "\n",
        "# 1. ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸('chat')ë¥¼ Gemma ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ê³µì‹ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# 2. ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ëª¨ë¸ì´ ì…ë ¥ë°›ì„ ìˆ˜ ìˆëŠ” í† í° ID í…ì„œë¡œ ë³€í™˜í•˜ê³ , GPU('device')ë¡œ ë³´ëƒ…ë‹ˆë‹¤.\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 3. í† í°í™”ëœ ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ í…ìŠ¤íŠ¸ \"\"ìƒì„±\"\"ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ìµœëŒ€ 256ê°œì˜ ìƒˆ í† í° ìƒì„±).\n",
        "# ìƒì„±ì„ ìœ„í•´ 'generate' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "outputs = model.********(**inputs, max_new_tokens=256) # **inputsì—ì„œ **ëŠ” ë¹ˆì¹¸ì´ ì•„ë‹™ë‹ˆë‹¤.\n",
        "\n",
        "# 4. 'outputs'ì—ëŠ” 'ì…ë ¥'ê³¼ 'ë‹µë³€'ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "# ë”°ë¼ì„œ ì›ë³¸ ì…ë ¥ í† í°ì˜ ê¸¸ì´(ì…ë ¥, 'inputs.input_ids.shape[1]')ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ê·¸ ì´í›„ì— 'ìƒˆë¡œ ìƒì„±ëœ' í† í°(ë‹µë³€)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "new_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
        "\n",
        "# 5. ì¶”ì¶œëœ í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ë¡œ \"\"ë””ì½”ë”©\"\"í•©ë‹ˆë‹¤.\n",
        "# ë””ì½”ë”©ì„ ìœ„í•´ 'decode' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "text = tokenizer.******(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "# 6. ìµœì¢… ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (3) ë¶„ë¥˜ (Classification)\n",
        "\n",
        "ì›í•˜ëŠ” ë¬¸ì¥ì„ ë„£ê³  ê°ì • ë¶„ì„ì„ í•´ë³´ì„¸ìš”!"
      ],
      "metadata": {
        "id": "l_UiOd02cuK2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN5y1c0v4a2q",
        "outputId": "84b0f6ce-20df-456d-ff9e-f28a1392702c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ì˜ˆì‹œ 3: ë¶„ë¥˜ (Classification) ---\n",
            "Neutral.\n",
            "\n",
            "The text is describing a movie review without expressing a positive or negative sentiment.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- ì˜ˆì‹œ 3: ë¶„ë¥˜ (Classification) ---\")\n",
        "\n",
        "# content: '~~~' ë”°ìŒí‘œ ì•ˆì— ìš”ì²­ì‚¬í•­ì„ ì ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "# ì—¬ê¸°ì„œëŠ” ê¸ì •, ë¶€ì •, ì¤‘ë¦½ìœ¼ë¡œ ê°ì •ë¶„ì„ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "# \\n\\nText: ì—¬ê¸°ì— ê°ì •ë¶„ì„í•  ë¬¸ì¥ì„ ì ì–´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "# ì›í•˜ëŠ” ë¬¸ì¥ì„ ë„£ê³  ê°ì • ë¶„ì„ì„ í•´ë³´ì„¸ìš”!\n",
        "\n",
        "# \"role\": \"user\" ì…ë‹ˆë‹¤.\n",
        "\n",
        "chat = [\n",
        "    { \"role\": \"****\", \"content\": \"Classify the text into neutral, negative, or positive. Generate only the class, nothing else.\\n\\nText: ë„ˆë¬´ ì¬ë¯¸ê°€ ì—†ë‹¤(ì˜í™” ë¦¬ë·°)\" }\n",
        "]\n",
        "\n",
        "# 1. ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸('chat')ë¥¼ Gemma ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ê³µì‹ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# 2. ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ëª¨ë¸ì´ ì…ë ¥ë°›ì„ ìˆ˜ ìˆëŠ” í† í° ID í…ì„œë¡œ ë³€í™˜í•˜ê³ , GPU('device')ë¡œ ë³´ëƒ…ë‹ˆë‹¤.\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 3. í† í°í™”ëœ ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ í…ìŠ¤íŠ¸ \"\"ìƒì„±\"\"ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ìµœëŒ€ 256ê°œì˜ ìƒˆ í† í° ìƒì„±).\n",
        "# ìƒì„±ì„ ìœ„í•´ 'generate' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "outputs = model.********(**inputs, max_new_tokens=256) # **inputsì—ì„œ **ëŠ” ë¹ˆì¹¸ì´ ì•„ë‹™ë‹ˆë‹¤.\n",
        "\n",
        "# 4. 'outputs'ì—ëŠ” 'ì…ë ¥'ê³¼ 'ë‹µë³€'ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "# ë”°ë¼ì„œ ì›ë³¸ ì…ë ¥ í† í°ì˜ ê¸¸ì´(ì…ë ¥, 'inputs.input_ids.shape[1]')ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ê·¸ ì´í›„ì— 'ìƒˆë¡œ ìƒì„±ëœ' í† í°(ë‹µë³€)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "new_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
        "\n",
        "# 5. ì¶”ì¶œëœ í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ë¡œ \"\"ë””ì½”ë”©\"\"í•©ë‹ˆë‹¤.\n",
        "# ë””ì½”ë”©ì„ ìœ„í•´ 'decode' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "text = tokenizer.******(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "# 6. ìµœì¢… ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### sLM í™œìš©ì‚¬ë¡€ ê²°ê³¼ ìš”ì•½\n",
        "\n",
        "- sLMì€ ì§ˆì˜ì‘ë‹µì´ë‚˜ ìš”ì•½ì€ ì–´ëŠ ì •ë„ ìˆ˜í–‰í•˜ì§€ë§Œ, í€„ë¦¬í‹°ê°€ ì•„ì‰½ìŠµë‹ˆë‹¤.\n",
        "- ê°ì • ë¶„ì„ì—ì„œ 'ë„ˆë¬´ ì¬ë¯¸ê°€ ì—†ë‹¤'ë¥¼ ì¤‘ë¦½ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„ ë¶„ë¥˜ ì‘ì—…ì—ì„œë„ í•œê³„ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "- ì´ë ‡ê²Œ ë²”ìš©ì ì¸ í™œìš©ì—ì„œëŠ” í•œê³„ê°€ ë‚˜íƒ€ë‚˜ì§€ë§Œ, ë¯¸ì„¸ì¡°ì •(Fine-Tuning)ì„ í†µí•´ íŠ¹ì • ì‘ì—…ì— íŠ¹í™”í•˜ë©´ í•´ë‹¹ ì‘ì—…ì—ëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ë©´ì„œë„ ì—¬ì „íˆ ê²½ëŸ‰í™”ëœ ëª¨ë¸ì˜ ì¥ì ì„ ëˆ„ë¦´ ìˆ˜ ìˆëŠ” ê²ƒì´ sLMì˜ íŠ¹ì§•ì…ë‹ˆë‹¤!"
      ],
      "metadata": {
        "id": "mCExY4TXqUcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤©ğŸ¤© ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤ ğŸ¤©ğŸ¤©\n",
        "â­ï¸â­ï¸ 2-2ì˜ API KEY ì…€ì„ ì§€ìš°ê³  ê³¼ì œ ì œì¶œí•´ì£¼ì„¸ìš”!! â­ï¸â­ï¸"
      ],
      "metadata": {
        "id": "NwtpRrJivCh7"
      }
    }
  ]
}