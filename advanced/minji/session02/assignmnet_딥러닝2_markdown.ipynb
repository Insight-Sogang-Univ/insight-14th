{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc260bf",
   "metadata": {},
   "source": [
    "#  1. 과거를 기억하는 신경망: RNN (Recurrent Neural Network)\n",
    "\n",
    "\n",
    "## 1-1. 순차(Sequence) 데이터란?\n",
    "\n",
    "> ###  순차 데이터(Sequence Data)의 정의\n",
    "- 순서에 의미가 있으며, 순서가 달라질 경우 의미가 손상되는 데이터\n",
    "\n",
    "\n",
    "###  순차 데이터의 세부 분류\n",
    "\n",
    "| 구분 | 설명 | 예시 |\n",
    "|------|------|------|\n",
    "| **순차 데이터 (Sequence Data)** | 가장 넓은 개념의 순서 데이터 | 문장, 오디오, DNA 등 |\n",
    "| **시간적 순차 데이터 (Temporal Sequence Data)** | 순서가 시간의 흐름을 의미 | 음성, 주가 등 |\n",
    "| **시계열 데이터 (Time Series Data)** | 일정한 시간 간격으로 기록된 시간적 순차 데이터 | 주가, 온도, 센서 데이터 |\n",
    "\n",
    "\n",
    "\n",
    "###  순차 데이터의 예시\n",
    "\n",
    "- **음성 및 오디오 데이터:** 소리의 파형은 시간에 따라 연속적으로 변화  \n",
    "- **자연어:** 문장 내 단어 순서가 의미를 결정 \n",
    "- **생물학적 서열 데이터:** DNA, 단백질 서열처럼 순서가 중요한 데이터  \n",
    "- **비디오 데이터:** 여러 이미지 프레임이 순서대로 구성된 데이터  \n",
    "\n",
    "\n",
    "\n",
    "###  기존 정형 데이터와의 차이점\n",
    "\n",
    "| 구분 | 일반적인 정형 데이터 | 순차 데이터 |\n",
    "|------|------------------|--------------|\n",
    "| **샘플 독립성** | 각 행(샘플)은 서로 독립 | 이전 시점이 다음 시점에 영향을 줌 |\n",
    "| **특성 순서 독립성** | 열(특성)의 순서는 중요하지 않음 | **요소의 순서가 핵심 정보** |\n",
    "| **자기상관성** | 없음 | 존재함 (이전 정보가 이후에 영향) |\n",
    "\n",
    ">  **결론:**  \n",
    "> 순차 데이터의 특성(순서 의존성, 자기상관성)을 반영하려면  \n",
    "> 기존의 정적 신경망 대신 ‘과거를 기억하는 구조’가 필요하다.\n",
    "\n",
    "\n",
    "\n",
    "## 1-2. RNN의 구조와 원리\n",
    "\n",
    "> ###  RNN 정의  \n",
    "> **RNN (Recurrent Neural Network)**  \n",
    "> : 순환 구조를 가진, 순차 데이터 처리에 특화된 인공 신경망\n",
    "\n",
    "\n",
    "\n",
    "### 이것만 알면 RNN 마스터\n",
    "\n",
    "- 시퀀스 데이터를 입력받아 순서 정보를 유지하며 처리\n",
    "- 은닉층(Hidden Layer) 내부의 순환 구조를 통해 이전 시점 정보를 전달\n",
    "\n",
    "\n",
    "\n",
    "###  은닉층 vs 은닉 상태\n",
    "\n",
    "| 구분 | 은닉층 (Hidden Layer) | 은닉 상태 (Hidden State) |\n",
    "|------|------------------|--------------------|\n",
    "| **정의** | 신경망의 구성 요소 (가중치 + 활성화함수 포함) | 특정 시점의 ‘정보 요약값(벡터)’ |\n",
    "| **비유** | 컨베이어 벨트 | 벨트 위의 물건 |\n",
    "| **시간에 따른 변화** | 변하지 않음 (가중치 공유) | 시점마다 변화 ($h_{t-1} \\rightarrow h_t$) |\n",
    "\n",
    "\n",
    "\n",
    "###  RNN의 기본 구조\n",
    "\n",
    "- $x_t$: 시간 스텝 t의 **입력값 (Input)**  \n",
    "- $h_t$: 시간 스텝 t의 **은닉 상태 (메모리)**  \n",
    "- $o_t$: 시간 스텝 t의 **출력 (Output)**\n",
    "\n",
    "| 가중치 | 의미 |\n",
    "|---------|------|\n",
    "| **U (입력 가중치)** | 현재 입력 $x_t$ 가 은닉 상태에 주는 영향 |\n",
    "| **V (순환 가중치)** | 이전 은닉 상태 $h_{t-1}$ 가 현재에 주는 영향 |\n",
    "| **W (출력 가중치)** | 현재 은닉 상태가 최종 출력에 주는 영향 |\n",
    "\n",
    "\n",
    "\n",
    "###  정보 흐름\n",
    "\n",
    "1. $t-1$ 시점: 입력 $x_{t-1}$ → 은닉 상태 $h_{t-1}$ 업데이트  \n",
    "2. $t$ 시점: $x_t$ + $h_{t-1}$ → 새로운 은닉 상태 $h_t$ 계산  \n",
    "3. $h_t$ → 출력 $o_t$\n",
    "\n",
    "즉, **이전 정보($h_{t-1}$)** 가 다음 시점으로 전달되어  \n",
    "**시간적 문맥(Context)** 을 반영한다.\n",
    "\n",
    "\n",
    "\n",
    "###  예시\n",
    "\n",
    "> “I work at Google”\n",
    "\n",
    "| Step | 입력 | RNN 내부 기억 |\n",
    "|------|------|---------------|\n",
    "| 1 | “I” | “I” |\n",
    "| 2 | “work” | “I work” |\n",
    "| 3 | “at” | “I work at” |\n",
    "| 4 | “google” | “I work at google” |\n",
    "\n",
    ">  실제로는 단어 자체가 아닌, 단어의 벡터 표현(숫자) 을 전달한다.\n",
    "\n",
    "\n",
    "\n",
    "###  RNN 아키텍처 유형\n",
    "\n",
    "| 유형 | 예시 |\n",
    "|------|------|\n",
    "| **One-to-One** | Vanilla RNN (단순 문제) |\n",
    "| **One-to-Many** | 이미지 캡셔닝 |\n",
    "| **Many-to-One** | 감정 분석, 스팸 분류, 시계열 예측 |\n",
    "| **Many-to-Many** | 기계 번역, 비디오 인식 |\n",
    "\n",
    "\n",
    "\n",
    "###  RNN의 핵심 원리: **가중치 공유 (Weight Sharing)**\n",
    "\n",
    "- 동일한 가중치(U, W, b)를 모든 시점에서 반복 사용  \n",
    "- 시퀀스 길이에 상관없이 모델 파라미터 수가 일정하게 유지\n",
    "\n",
    "> 비유: 문장을 읽을 때 첫 단어와 다섯 번째 단어를 해석하는 ‘뇌’가 다르지 않은 것처럼,  \n",
    "> 동일한 문법 규칙(=가중치)을 반복적으로 적용한다.\n",
    "\n",
    "\n",
    "\n",
    "###  가중치 공유의 장점\n",
    "\n",
    "1. 파라미터 수 감소 → 효율적 학습 가능\n",
    "2. 일반화 능력 향상 → 새로운 시퀀스에도 유연하게 대응\n",
    "\n",
    "\n",
    "\n",
    "## 1-3.  RNN의 한계 및 장기 의존성 문제 (The Long-Term Dependency Problem)\n",
    "\n",
    "\n",
    "\n",
    "###  RNN의 한계\n",
    "\n",
    "RNN은 전체 시퀀스를 모두 읽은 뒤 **역전파(Backpropagation Through Time)** 를 수행  \n",
    "이 과정에서 **chain rule** 로 인해 기울기(gradient)가 반복 곱해진다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} = \\text{diag}(g'(a_t)) \\times W_{hh}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "###  기울기 소실 (Vanishing Gradient)\n",
    "\n",
    "- 반복된 곱셈으로 기울기 값이 0에 가까워지는 현상\n",
    "- → 먼 과거의 정보 학습 불가\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 기울기 폭주 (Exploding Gradient)\n",
    "\n",
    "- 반복된 곱셈으로 기울기가 급격히 커지는 현상\n",
    "- → 학습 불안정, 계산 불능(NaN, Infinity) 발생\n",
    "\n",
    " 해결책:\n",
    "> **Gradient Clipping** — 기울기의 크기에 상한선을 두어 안정화\n",
    "\n",
    "\n",
    "\n",
    "###  느린 훈련 속도\n",
    "\n",
    "- $h_t$ 계산 시 반드시 $h_{t-1}$이 필요 → 순차적 계산만 가능\n",
    "- 병렬 연산이 어려워 훈련 속도가 매우 느림\n",
    "\n",
    "\n",
    "### 장기 의존성 문제 (Long-Term Dependency Problem)\n",
    "\n",
    "> 시퀀스의 **앞부분 정보가 소실되어 문맥 이해가 어려워지는 문제**\n",
    "\n",
    "\n",
    "### - 새로운 모델의 필요성\n",
    "\n",
    "RNN의 한계를 극복하기 위해 제안된 새로운 구조:\n",
    "\n",
    "- 중요한 정보는 오래 기억하고  \n",
    "- 불필요한 정보는 선택적으로 잊는  \n",
    "**LSTM (Long Short-Term Memory)**, **GRU (Gated Recurrent Unit)**\n",
    "\n",
    "\n",
    " **정리 요약**\n",
    "\n",
    "| 문제 | 원인 | 결과 | 해결방안 |\n",
    "|------|------|------|-----------|\n",
    "| 기울기 소실 | 작은 기울기의 반복 곱 | 과거 정보 소실 | LSTM / GRU |\n",
    "| 기울기 폭주 | 큰 기울기의 반복 곱 | 학습 불안정 | Gradient Clipping |\n",
    "| 느린 훈련 | 순차적 계산 | 비효율적 학습 | 병렬 가능한 Transformer 등장 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3d40f",
   "metadata": {},
   "source": [
    "# 2. 똑똑하게 기억하고 잊는 법: LSTM & GRU\n",
    "\n",
    "\n",
    "## 2-1. 게이트(Gate)로 정보의 흐름을 제어하다: LSTM\n",
    "\n",
    "\n",
    "###  LSTM (Long Short-Term Memory)\n",
    "\n",
    "앞의 정보가 뒤로 충분히 전달되지 못하는 RNN의 **기울기 소실 문제(Vanishing Gradient)**를 해결하기 위한 구조\n",
    "\n",
    "\n",
    "\n",
    "###  이것만 알면 LSTM 마스터\n",
    "\n",
    "1. **핵심 아이디어:** 기억할 내용과 잊을 내용을 선택해 중요한 정보를 오래 유지  \n",
    "   → “GATE”를 통해 곱셈을 덧셈으로 바꾼다  \n",
    "2. **RNN과의 차이:** $c_t$ (장기기억), $h_t$ (단기기억) 두 개의 순환층  \n",
    "3. **세 가지 게이트:**  \n",
    "   - Forget gate: 얼마나 잊을지  \n",
    "   - Input gate: 얼마나 저장할지  \n",
    "   - Output gate: 어떤 정보를 다음 단계로 보낼지  \n",
    "4. **Final memory cell:** Input, Forget 게이트 결합 → 현재 기억 업데이트\n",
    "\n",
    "\n",
    "\n",
    "### LSTM 단계별 수식\n",
    "\n",
    "#### Forget Gate\n",
    "과거 정보를 얼마나 잊을지 결정  \n",
    "- $h_{t-1}$, $x_t$를 sigmoid에 통과 → $f_t$ (0~1)  \n",
    "- 1에 가까우면 기억 유지, 0이면 잊음\n",
    "\n",
    "#### Input Gate\n",
    "입력 정보 중 어떤 것을 기억할지 결정  \n",
    "- sigmoid → 0~1 (기억 비율)  \n",
    "- tanh → 후보값 $\\tilde{C}_t$ 생성\n",
    "\n",
    "#### Final Memory Cell\n",
    "이전 셀 상태와 새로운 후보값을 조합해 업데이트  \n",
    "- $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n",
    "\n",
    "#### Output Gate\n",
    "현재 기억을 바탕으로 출력($h_t$) 결정  \n",
    "- $h_t = o_t * \\tanh(C_t)$\n",
    "\n",
    "\n",
    "###  LSTM의 장단점\n",
    "\n",
    "**장점:**  \n",
    "- 기울기 소실 완화  \n",
    "- 중요한 정보만 선택적으로 저장\n",
    "\n",
    "**단점:**  \n",
    "- 구조가 복잡하고 학습 파라미터가 많음\n",
    "\n",
    "\n",
    "## 2-2. 더 단순하고 효율적인 구조: GRU (Gated Recurrent Unit)\n",
    "\n",
    "\n",
    "###  GRU (Gated Recurrent Units)\n",
    "\n",
    "LSTM에서 발전된 형태로, **메모리 셀 제거 + 게이트 수 축소**\n",
    "\n",
    "\n",
    "###  이것만 알면 GRU 마스터\n",
    "\n",
    "- Forget + Input gate → Update gate 하나로 통합\n",
    "- Reset gate 도입\n",
    "- 게이트 개수: 3 → 2  \n",
    "  → 학습 속도 향상, 성능은 LSTM과 유사\n",
    "\n",
    "\n",
    "\n",
    "###  GRU 단계별 수식\n",
    "\n",
    "**Update gate:**  \n",
    "이전 상태와 현재 상태의 반영 비율  \n",
    "- $z_t = \\sigma(W_z [h_{t-1}, x_t])$\n",
    "\n",
    "**Reset gate:**  \n",
    "과거 정보를 얼마나 무시할지  \n",
    "- $r_t = \\sigma(W_r [h_{t-1}, x_t])$\n",
    "\n",
    "**New hidden state:**  \n",
    "- $\\tilde{h}_t = \\tanh(W [r_t * h_{t-1}, x_t])$\n",
    "- $h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t$\n",
    "\n",
    "\n",
    "###  GRU의 장단점\n",
    "\n",
    "**장점:** 단순한 구조로 빠르고 효율적  \n",
    "**단점:** 긴 시퀀스 처리에서는 한계 존재\n",
    "\n",
    "\n",
    "## 2-3. LSTM vs GRU: 언제 무엇을 선택할까?\n",
    "\n",
    "\n",
    "\n",
    "| 구분 | LSTM | GRU |\n",
    "|------|------|------|\n",
    "| **게이트 수** | 3개 (입력, 삭제, 출력) | 2개 (업데이트, 리셋) |\n",
    "| **복잡도** | 높음 | 낮음 |\n",
    "| **학습 속도** | 느림 | 빠름 |\n",
    "| **메모리 사용량** | 큼 | 작음 |\n",
    "| **장기 의존성 처리** | 강력 | 다소 약함 |\n",
    "| **연구 사례** | 많음 | 적음 |\n",
    "| **성능** | 데이터 충분 & 복잡한 문제에 강함 | 데이터 적거나 효율성 중시할 때 유리 |\n",
    "\n",
    "\n",
    "\n",
    "### 선택 가이드\n",
    "\n",
    "**LSTM이 유리한 경우**\n",
    "- 긴 시퀀스에서 문맥 이해가 중요한 작업\n",
    "- 데이터가 많고 복잡한 패턴 학습 필요\n",
    "- 계산 효율보다 성능이 중요한 경우  \n",
    "➡ 기계 번역, 언어 모델링, 장기 예측(금융·기후)\n",
    "\n",
    "**GRU가 유리한 경우**\n",
    "- 자원이 제한적이고 속도가 중요한 경우\n",
    "- 데이터가 적거나 실시간 예측이 필요한 경우  \n",
    "➡ 음성 인식, 스트리밍, 짧은 시퀀스 텍스트 분류\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6887511",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f41488e9",
   "metadata": {},
   "source": [
    "# 3. 문장을 입력받아 문장을 출력하다: Seq2Seq\n",
    "\n",
    "\n",
    "###  Seq2Seq의 핵심 특징\n",
    "\n",
    "1. **입력과 출력의 길이가 달라도 된다!**  \n",
    "   → 짱구의 짧은 명령도 흰둥이는 긴 행동으로 표현 가능. (번역·요약 등에 적합)\n",
    "\n",
    "2. **인코더와 디코더가 분리되어 있다!**  \n",
    "   → 의미를 요약하는 인코더 + 문맥을 생성하는 디코더 구조\n",
    "\n",
    "3. **문맥 벡터(Context Vector)로 연결된다!**  \n",
    "   → 짱구의 말은 컨텍스트 벡터로 전달되어 흰둥이의 행동으로 변환됨\n",
    "\n",
    "\n",
    "\n",
    "## 3-1. Seq2Seq의 기본 구조: 인코더와 디코더\n",
    "\n",
    " **Seq2Seq란?**  \n",
    "한 시퀀스를 다른 시퀀스로 변환하는 딥러닝 모델로, **기계 번역**에서 대표적으로 사용된다.\n",
    "\n",
    "\n",
    "\n",
    "###  특징 1\n",
    "입력 개수와 출력 개수가 같지 않아도 된다.  \n",
    "→ (예: 입력 3개 단어 → 출력 4개 단어)\n",
    "\n",
    "\n",
    "###  특징 2\n",
    "인코더와 디코더 모듈로 구성된 **Encoder-Decoder 모델**\n",
    "\n",
    "\n",
    "\n",
    "###  인코더와 디코더란?\n",
    "\n",
    "####  인코더 (Encoder)\n",
    "\n",
    "- 입력 시퀀스를 읽고 압축하여 **디코더에게 넘겨줄 문맥 정보(context vector)** 생성  \n",
    "- 각 시점의 입력 단어와 이전 은닉 상태(hidden state)를 이용해 다음 은닉 상태 계산  \n",
    "- 최종 은닉 상태가 컨텍스트 벡터로 사용됨\n",
    "\n",
    "####  디코더 (Decoder)\n",
    "\n",
    "- 인코더의 컨텍스트 벡터를 바탕으로 출력 시퀀스를 생성  \n",
    "- t시점의 출력이 t+1시점의 입력으로 사용됨  \n",
    "\n",
    "\n",
    "\n",
    "####  컨텍스트 벡터 (Context Vector)\n",
    "\n",
    "- 인코더의 **마지막 은닉 상태(hidden state)**  \n",
    "- 입력 문장의 **요약 정보** 역할  \n",
    "- 일반적으로 256~1024차원 정도로 설정됨\n",
    "\n",
    "\n",
    "\n",
    "**전체 과정 설명**\n",
    "\n",
    "- 입력 단어들이 순차적으로 RNN/LSTM을 거치며 정보를 누적  \n",
    "- 마지막 단어 입력 후의 은닉 상태가 문장의 요약본인 **Context Vector**  \n",
    "- 디코더는 이 정보를 바탕으로 새로운 문장 생성\n",
    "\n",
    "\n",
    "\n",
    "## 3-2. Seq2Seq의 한계: 병목 현상 (Bottleneck)\n",
    "\n",
    "**문장 전체를 고정된 크기의 벡터로 압축 → 정보 손실 발생**\n",
    "\n",
    "\n",
    "### 예시\n",
    "\n",
    "“나는 사과를 먹는다” → 짧은 문장은 문제 없음  \n",
    "하지만 아래처럼 긴 문장은?\n",
    "\n",
    "> “2025년 봄이었다. 그날은 5월 치고는 너무 더운 날씨였다. 수업을 마치고 집으로 향하던 길. 무슨 바람이 불었던 걸까, 갑자기 먹지도 않던 사과가 먹고 싶어졌다…”\n",
    "\n",
    " 모든 정보를 고정된 벡터 하나에 담기 어려움 → **병목 현상 발생**\n",
    "\n",
    "\n",
    "\n",
    "###  Seq2Seq의 한계 요약\n",
    "\n",
    "1. **고정된 Context Vector로 인한 정보 손실**  \n",
    "   → 긴 문장일수록 요약 정보가 불충분\n",
    "\n",
    "2. **기울기 소실 문제 (Vanishing Gradient)**  \n",
    "   → 앞쪽 단어의 영향이 뒤로 갈수록 사라짐\n",
    "\n",
    "3. **병렬화 불가능**  \n",
    "   → 순차적 처리 때문에 학습 시간 증가\n",
    "\n",
    "이 한계를 극복하기 위해 **Attention Mechanism**이 등장!\n",
    "\n",
    "\n",
    "\n",
    "### 세줄 요약\n",
    "\n",
    "1. RNN은 순차 데이터를 처리하지만, **장기 의존성 문제**로 앞의 정보를 잊어버린다.  \n",
    "2. LSTM/GRU는 **Gate 구조**로 필요한 정보만 기억하도록 개선했다.  \n",
    "3. Seq2Seq는 **문장을 입력받아 문장을 출력하는 인코더-디코더 구조**지만,  \n",
    "   하나의 컨텍스트 벡터로 모든 정보를 담을 때 **병목 현상**이 발생한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93606d7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
