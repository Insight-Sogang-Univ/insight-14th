{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a28cbe88",
   "metadata": {},
   "source": [
    "# 1 인공신경망의 시작 : 퍼셉트론\n",
    "\n",
    "## 1.1 인공신경망이란?\n",
    "* 인공신경망 : 뇌 뉴런을 모방해서 만든 신경망\n",
    "\n",
    "-> 다수입력을 가지돌기 처럼, 이걸 모아서 일정 기준 넘으면 output 출력\n",
    "\n",
    "* 퍼셉트론 : 인공신경망의 초기 형태\n",
    "\n",
    "-> 가중치(값의 중요정도)를 이용해서 선형적으로 더하고 값을 임계값 기준으로 출력 정해짐(활성화 함수), 학습을 통해 가중치 조정\n",
    "\n",
    "임계값 vs 편향? -> 표현의 차이인데, 입계값을 좌편으로 넘겨서 0 기준으로 나타내기\n",
    "\n",
    "## 1.2 단층 퍼셉트론과 그 한계\n",
    "\n",
    "* 단층 퍼셉트론 : 두개의 층으로만 이루어진 퍼셉트론 (입쳑 -> 출력으로)\n",
    "\n",
    "* 논리 게이트를 단층 퍼셉트론으로 구현 가능! (2 입력 을 1 출력으로)\n",
    "    1. AND : 둘 다 1 이어야 1 출력\n",
    "    2. NAND : 둘 중 하나만 1인 경우 0, 둘 다 1이거나 0이면 1\n",
    "    3. OR : 둘 중 하나라도 1이면 1, 이외 0\n",
    "\n",
    "* 단층 퍼셉트론의 한계 : 선형적 분류만 가능! XOR 같은 건 나타내지 못한다\n",
    "\n",
    "## 1.3 다층 퍼셉트론\n",
    "* 단층으로 표현한 논리 게이트들을 짬뽕해서 새로운 게이트를 만들 수 있다 -> 퍼셉트론 층 쌓기!\n",
    "* 입력층 -> 은닉층(1개이상) -> 출력층 으로 나타남\n",
    "* 2개 이상의 은닉층을 가지는 MLP가 딥러닝의 시초 모델로 발전\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5133641",
   "metadata": {},
   "source": [
    "# 2 딥러닝 모델을 어떻게 학습할까?\n",
    "## 2.1 순전파와 역전파\n",
    "* 학습 알고리즘\n",
    "    1. 순전파로 입력값을 통한 예측 (정방향으로 일단 학습)\n",
    "    2. 오차 계산해가면서 예측 정확도 측정 (틀린 정도 확인)\n",
    "    3. 역전파해서 오차 전달(어디서 틀렸지?)\n",
    "    4. 가중치 업뎃( 틀린 곳 업뎃)\n",
    "    5. 반복\n",
    "\n",
    "* chain rule : 변수간 상관관계 확인하기\n",
    "    * 변수간 관계를 통해 실패를 가장 크게 기여한 부분 찾기 \n",
    "    * 하나가 다른 하나에 영향을 주고, 그 녀석이 다음 녀석에게 영향을주고....이런식으로! -> 어디를 고쳐야 할 지, 가중치를 어떻게 둘 지 고려할때 사용할 요소!\n",
    "    * 기울기 소실 문제 : 은닉층이 많아질수록, 초기 layer의 영향력이 사라짐\n",
    "    \n",
    "## 2.2 손실 함수\n",
    "* 실제값과 예측값의 차이를 수치로 나타내는 함수!\n",
    "* MSE : 편차의 제곱을 더하는 함수 : 연속적이며, 실제 차이를 측정할 수 있어 회귀에 적합\n",
    "* BCE : 이진 분류 문제에 적합\n",
    "    * 뽀록으로 맞춰도 확신을 못하고, 확신해서 틀리면 그 loss를 크게! (예상과 다를 시 손실이 큼)\n",
    "* CEE : 예측과 실제가 얼마나 일치하는지, surprise의 정도에 따라 결정될것\n",
    "\n",
    "## 2.3 홠성화 함수\n",
    "* 입력값과 가중치, 편향 이용해서 뭔가 값을 일단 내고 이 값을 이후 출력에서 쓰기 위해 가공을 하기 위해 사용하는 함수\n",
    "    1. 계단 함수 : 하나의 수 기준으로 1 0 칼로 자르듯 나눈다\n",
    "    2. 시그모이드 함수 : 계단함수와 비슷한테 비선형적 형태로 나눔\n",
    "    3. ReLU : 0아래는 0, 0이후부터는 y = x 인 가장 많이 사용하는 함수\n",
    "        * 왜 가장 많이 쓰나? 1. 단순하고 2. 비선형적이면서 3. 미분값이 감소하지 않고 1 그대로 전달 가능\n",
    "    4. 하이퍼볼릭탄젠트함수 : 시그모이드와 비슷한 형태인데 출력 범위가 다름 -> 가운데를 0으로 맞추로 싶을 때!\n",
    "    5. softmax : 다중 클래스 분류 문제의 활성화 함수로 사용되고, **개별 클래스의 확률**을 알 수 있다\n",
    "        * 소프트맥스와 교차엔트로피 손실 함수의 조합이 가장 표준적임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb57b28",
   "metadata": {},
   "source": [
    "## 2.4 경사 하강법과 옵티마이저\n",
    "* 경사 하강법 : 손실을 최소화하여 모델 학습, 미분을 1번씩만 해도 되서 좋긴한데...  데이터가 너무 방대해서 시간이 오래걸림 -> batch!\n",
    "* 배치 : 데이터셋을 묶어서 묶은 애들끼리 매개변수 조정\n",
    "* 이포크 : 모든 데이터셋을 학습하는 횟수 -> 전체 기준!\n",
    "* 이터레이션 혹은 스탭 : 한번의 에포크를 위해 필요한 배치 수, 즉 배치 크기랑 이터레이션을 곱하면 1 이포크에서 다루는 데이터수\n",
    "* 경사 하강법 종류 \n",
    "    1. 배치 경사 하강법 : 전체 데이터를 하나로 하는 기본적 경사 하강법\n",
    "        * 업데이트 수가 적고, 메모리 많이 필요, 수렴 안정적\n",
    "    2. 배치 크기가 1인 확률적 경사 하강법 : 전체를 쓰면 너무 크니까, 랜덤으로 선택한 하나의 데이터에 대해서만 계산 진행\n",
    "        * 빠르지만 정확도가 낮을 수 있음\n",
    "    3. 미니 배치 경사 하강법 : 1 2 사이 어딘가\n",
    "        * 배치 크기를 적절하게 지정해 매개 변수 값 조정. 배치 크기는 2의 지수승으로 주로 결정\n",
    "* 옵티마이저 : 경사하강법을 수행하기 위한 도우미들!\n",
    "    1. 모멘텀 : 이전 접선의 기울기를 이용해서, 관성을 부여/ 로컬 미니마 문제 해결에 도움!\n",
    "    2. 알엠에스프롭 : 기울기에 따라 너무 많이 혹은 적게 움직이는 문제 해결 -> 학습률 조정!\n",
    "        * 급할때는 속도를 줄이고, 완만할 때는 속도를 유지 혹은 증가\n",
    "    3. 아담 : 1 + 2를 잘 적당히! -> 각 파라미들의 1차 및 2차 모멘트 조정을 통해 학습률을 조정\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d3025",
   "metadata": {},
   "source": [
    "## 2.5 학습을 향상시키는 기법 : 데이터 증강과 전이 학습\n",
    "* 기존 데이터에 다양한 변형을 만들어 학습 데이터를 늘린다\n",
    "    1. 모델 성능 향상\n",
    "    2. 데이터 의존성 감소 : 다양한 패턴 학습\n",
    "    3. 과적합 완화 \n",
    "    4. 데이터 프라이버시 보호\n",
    "\n",
    "* 전이 학습 : 하나에서 학습한 내용을 다른 문제에 활용! -> 재활용을 통해 새로운 문제를 빠르고, 잘 해결!\n",
    "    1. 계산 비용 절감\n",
    "    2. 데이터셋 크기 문제 완화\n",
    "    3. 일반화 가능성 향상\n",
    "    4. 성능 항상\n",
    "\n",
    "* 전체적인 워크 플로우\n",
    "    1. pre-trained model :일반적 특징\n",
    "        -> 일단 큰 데이터셋으로 학습해서 계산 비용 절감 및 작은 데이터셋 성능 발휘 : 전이 학습!\n",
    "    2. fine - tuning\n",
    "        -> 1번에서 만든 모델로 새로운 데이터 및 문제에 맞춘다. 상황에 맞게 층 조정!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e853f9",
   "metadata": {},
   "source": [
    "# 3 합성곱 신경망 CNN\n",
    "* 원본 이미지를 픽셀 단위로 나누고 냅다 학습시키면, 순서가 뒤섰여 이미지의 특징을 잃어버림\n",
    "* CNN을 통해, 위치별 특징을 추출해서 이를 분류한다! 이미지 공간정보를 유지시키자!\n",
    "    * 추출은 convolution과 pooling을 사용한다.\n",
    "## 3.1 왜 CNN을 쓰는가?\n",
    "0. 이미지가 다른 데이터와 다른 구조를 가짐 (픽셀 내부의 RGB 숫자)\n",
    "1. 위치, 크기, 회전과 같은 변화에 취약 -> 어떤 영역에 대해 특징이 있는가를 확인하는 방법으로 해결!\n",
    "2. 파라미터 폭발 -> 연결 너무 많아져서 파라미터 급증문제 : 부분적인 연결 및 합성곱을 통해 특징에만 집중할 수 있음\n",
    "3. 공간 정보 손실 : 2차원 이상이미지가 1차원 벡터화 되는 과정에서 이미지 공간 정보 손실 : 다차원 이미지 그대로 받고 공간 정보를 보존하는 방법으로 해결\n",
    "\n",
    "## 3.2 합성곱과 풀링\n",
    "* 특징추출, 클래스 분류로 CNN이 이루어져 있어 공간적 관계 및 지역에 대한 특징을 중요시한다\n",
    "1. 합성곱 레이어 : 전체 이미지를 nxn 매트릭스가 훑으며 합성곱 진행 후 피쳐 맵에 저장\n",
    "2. 패딩 : 정보의 손실을 막기 위해 이미지 밖을 0으로 둘러쌈 - > 합성곱의 안정성 확보\n",
    "3. stride (보폭) : 필터가 몇칸씩 움직일 지 결정!\n",
    "4. 풀링 : 피처맵에서 특징 추출을 위해 이미지를 격자 형태로 분할, 각 격자별 대표값 설정 (평균 혹은 맥스)\n",
    "\n",
    "## 3.3 대표적인 CNN 아키텍쳐\n",
    "1. AlexNet : 최초로 GPU를 이용!\n",
    "    * 5개의 합성곱층, 3개의 fully connected layers\n",
    "    * Relu 함수 이용해서 기울기 소실 해서\n",
    "    * dropout을 통해 과적합 방지\n",
    "2. VGGNet\n",
    "    * 합성곱과 풀링을 많이해서 층의 깊이를 늘려 정확도를 높임\n",
    "    * 전이 학습에 많이 사용\n",
    "3. ResNet\n",
    "    * 잔차연결 도입 -> 층을 건너뛰고 다음 레이어로 직접 가서 기울기 소실을 해결\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3ea40",
   "metadata": {},
   "source": [
    "# 4 이미지 딥러닝 응용\n",
    "## 4.1 이미지 분류\n",
    "    * 이미지를 수치 데이터로 인식 -> 수치에 따라 이미 정해진 클래스로 분류\n",
    "## 4.2 객체 탐지 (Object Detection)\n",
    "    * 이미지에서 객체 검출은 객체와 객체를 둘러싼 작은 직사각형으로 정의되는 bounding box 찾는 작업.\n",
    "## 4.3 이미지 캡셔닝\n",
    "    * 이미지 내의 객체에 대해 관계를 파악하고 자연어의 형태로 알맞게 표현\n",
    "    * CNN과 RNN이 대표적 -> 이미지의 CNN, 문장 완성의 RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb86980",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
