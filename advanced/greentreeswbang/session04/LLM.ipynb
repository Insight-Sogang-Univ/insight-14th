{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee2c56b",
   "metadata": {},
   "source": [
    "# 1. 언어 모델\n",
    "\n",
    "## 1.1 언어 모델이란?\n",
    "\n",
    "* 언어 모델 : 단어에 확률을 할당해 가장 자연스로운 단어 시퀀스를 찾아냄 -> 이전 단어들을 이용해서 다음 단어 예측\n",
    "* 통계 기반 vs 인공 신경망 기반\n",
    "\n",
    "## 1.2 SLM(통계적 언어 모델)\n",
    "* 분포 가설 : 비슷한 문맥에서 함께 나타나는 단어들은 비슷한 의미를 가진다\n",
    "    1. 조건부 확률 높으면 더 자연스럽다\n",
    "    2. 카운트 기반 : 등장 단어 빈도\n",
    "    * 희소 문제 : 가지고 있는 데이터가 부족해 언어를 정확하게 모델링하지 못함\n",
    "* N-gram : 일부 단어만 참고한다 -> 희소 문제 완화\n",
    "    * n개의 단어 묶음으로 토큰화를 진행\n",
    "    * n-1 개의 단어만 참고\n",
    "    * 한계 : 여전히 희소 문제\n",
    "* PPL: 언어 모델이 특정 문장을 얼마나 혼란스러워 하는지 -> 낮을수록 좋음!\n",
    "    * 문장이 희소하고 단어의 수가 적을수록 PPL 증가\n",
    "\n",
    "## 1.3 딥러닝 기반 언어모델\n",
    "* LLM : 대규모 언어 모델, 방대한 양의 데이터를 학습해 인간의 언어를 통해 작업이 가능한 인공지능 모델\n",
    "     * 학습 : 언어의 패턴 학습\n",
    "     * 예측 및 생성 : 언어를 받으면 학습 기반으로 다음에 올 가장 확률 높은 단어 예측\n",
    "     * 텍스트 생성, 기계 번역, 질의응답, 요약, 감정 분석 등에 사용 간으\n",
    "* BERT : 인코더만을 이용한 양방향 언어 모델\n",
    "    * 양방향: 단어 좌우 문맥\n",
    "    * 트랜스포머의 인코더를 쌓아올려서 제작\n",
    "    * 인풋, 임베딩, 인코더, 아웃풋 구조로 작동\n",
    "    * 사전학습\n",
    "        1. 마스크 언어 모델 : 단어 가리고 문맥으로\n",
    "        2. 다음 문장 예측 : 두 문장이 실제 이어지는지 판단\n",
    "* ROBERTA : 마스킹을 동적으로 변경\n",
    "* ALBERTA : BERT 경량화\n",
    "* GPT : 단방향, 생성 중심\n",
    "* LLM 한계 : 환각 및 업데이트 비용\n",
    "* RAG : 모르는 정보에 대한 환각 문제 해결 : 모르면 물어보자!\n",
    "    1. 질문 인코더\n",
    "    2. 지식 검색기\n",
    "    3. 지식 증강 생성기\n",
    "    * 실제 데이터 기반 답변으로 환각 현상을 줄임\n",
    "* CAG : 사실을 위한 문서 확인 + 문서에 대한 신뢰성까지 확인\n",
    "* LangChain : LLM + RAG를 위한 파이프라인 쉽게 구성 도와주는 프레임워크\n",
    "    1. 추상화 : 각종 작업의 간결화\n",
    "    2. 표준화 : 비슷한 기능들을 동일 형식으로 표준화\n",
    "    3. 체이닝 : 자주 활용하는 컴포넌트 쉽게 연결\n",
    "* LangGraph : 비선형적 작업 형태가 가능\n",
    "    * 노드 : 특정 작업 수행 단위\n",
    "    * 엣지 : 노드와 노드를 연결하는 선\n",
    "* LangSmith : 사용자의 질문이 LLM에게 전달되고 어떻게 가공되어 최종 답변되는지 파악\n",
    "    * 디버깅 및 추적\n",
    "    * 테스트 및 평가\n",
    "    * 모니터링\n",
    "    * 프롬프트 허브\n",
    "* 외부 API 활용하기 : LLM 외부 API와 연동해서 답변\n",
    "\n",
    "## 1.4 sLM\n",
    "* 소규모 언어 모델 : LLM에 비해 규모와 범위가 작게 사용, 효율을 위해!\n",
    "* 모델 압축\n",
    "    1. 가지치기\n",
    "    2. 양자화\n",
    "    3. 자식 증류\n",
    "* 장점: 프라이버시, 비용 절감, 효율 및 맞춤화\n",
    "* 한계 : 편향, 제한된 일반화, 환각, 성능 저하\n",
    "* LLM과 sLM의 결합 : sLM이 먼저 풀어보고, 어려운 부분에 대해서 LLM이 도움을 준다 -> LLM 토큰 절약 가능!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36b41c",
   "metadata": {},
   "source": [
    "# 2. 생성 모델  \n",
    "\n",
    "## 2.1 생성 모델 vs 분류 모델\n",
    "* 생성 모델 : 주어진 학습 데이터를 통해 유사한 데이터를 생성하는 모델\n",
    "* 샘플 레이블 지정보다 입력 데이터의 확률 분포에 관심\n",
    "\n",
    "## 2.2  AE, VAE\n",
    "* 오토인코더 : 입력과 동일한 출력을 만드는 것을 목적으로 하는 신경망\n",
    "    * 인코더 : 고차원 입력 데이터를 잠재 표현으로 변환\n",
    "    * 디코더 : 잠재 표현을 재복원\n",
    "    * 잠재 표현 : 원본 데이터를 저차원으로 압축\n",
    "* VAE : 잠재 코드의 값을 가우시안 확률 분포에 기반한 확률값으로 나타냄 \n",
    "    * 인코더 : 입력을 분포로 변환\n",
    "    * 잠재 벡터 : 원본 데이터 저차원 압축\n",
    "    * 디코더 : 재복원\n",
    "\n",
    "## 2.3 GAN \n",
    "* 생성자 신경망과 판별자 신경망이 경쟁을 통해 훈련\n",
    "    * 생성자 : 진짜 분포에 가까운 가짜 생성\n",
    "    * 판별자 : 표본이 가짜에 속하는지 진짜에 속하는지 결정\n",
    "\n",
    "* 장점 : 진짜 같은 가짜 생성 가능\n",
    "* 단점 : 불안정한 학습 -> 진동의 가능성 존재\n",
    "\n",
    "## 2.4 확산 모델 \n",
    "* 노이즈를 여러 단계에 걸쳐 추가 및 제거, 입력 이미지와 유사한 확률 분포를 가진 이미지 생성\n",
    "    * 순확산 : 데이터에 노이즈를 점진적으로 추가\n",
    "    * 역확산 : 노이즈 데이터에서 원본 데이터를 재구성 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e44968",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
