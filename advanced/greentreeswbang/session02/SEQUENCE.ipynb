{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7269fd",
   "metadata": {},
   "source": [
    "# 1  과거를 기억하는 신경망 : RNN\n",
    "## 1.1 순차 데이터란?\n",
    "* 순차 데이터 : 순서가 의미가 있음!\n",
    "    * 예를 들어 음성 및 오디오, 자연어, 생물학적 서열, 비디오 등\n",
    "* 기존과 어떤 차이?\n",
    "    * 데이터 구성하는 요소들의 순서 자체가 핵심\n",
    "    * 이전 데이터가 이후 데이터에 영향을 준다\n",
    "    * 이에 따라 과거를 기억하고 순서에 따라 의미 학습이 가능한 신경망이 필요\n",
    "\n",
    "## 1.2 RNN의 구조와 원리\n",
    "* 순환하는 구조로 순차 데이터를 처리\n",
    "* 입력, 은닉, 출력층으로 이루어짐\n",
    "* 원리\n",
    "    1. 이전 인풋이 이전 히든스테이트 업데이트 후 출력\n",
    "    2. 이전 은닉층을 전달\n",
    "    3. 새 인풋에 따라 은닉층 업데이트 및 출력 진행\n",
    " * 가중치 공유\n",
    "    * 이때, 히든에서 히든 및 입력에서 히든을 갈 때 모든 순환과정에서 가중치가 동일\n",
    "    * 즉 길이 및 시점 위치에 상관없이 효과적으로 작동\n",
    "    * 학습 파라미터수가 감소해 빠른 학습 가능\n",
    "    * 일반적 규칙 학습에 따른 일반화 능력 향상\n",
    "\n",
    "## 1.3 RNN의 한계 및 장기 의존성 문제\n",
    "* 전체 시퀀스를 읽은 후 역전파가 이루어지는데, 이 과정에서 CHAIN RULE에 따라 미분값이 반복적 곱셈\n",
    "* 따라서 뒤에서 앞에 오는 과정에서, 기울기 소실 및 폭주가 발생\n",
    "    * 기울기가 1보다 작으면 소실 : 앞 내용 사라짐\n",
    "    * 기울기가 1보다 크면 폭주 : 앞 내용 과도해석\n",
    "* 느린 훈력 시간 : 앞 학습이 이루어져야 뒤가 이루어지는 직렬적 구조이므로 시간이 오래 걸림\n",
    "* 결론: 장기 의존성 문제, 시퀀스가 길어질수록 장기 기억력에 문제가 발생 -> 새로운 모델이 필요하다!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb941ce",
   "metadata": {},
   "source": [
    "# 2 똑똑하게 기억하고 잊는 법 : LSTM & GRU\n",
    "## 2.0 LSTM과 GRU 개관\n",
    "1. LSTM : 기억을 셀 상태와 은닉 상태로 나눔. 셀 상태는 장기 기억, 은닉 상태는 단기 기억\n",
    "    * 작동 방식 : 망각, 인풋, 아웃풋\n",
    "2. GRU : 장기 기억의 셀 상태 없이, 하나의 은닉 상태로 덮어쓰기를 진행\n",
    "    * 과거 기억을 얼마나 가져올지, 새 기억을 얼마로 넣을지 셜정\n",
    "\n",
    "## 2.1 게이트로 정보의 흐름을 제어하다 : LSTM\n",
    "* 앞의 정보가 뒤로 충분히 전달되지 못하는 기울기 소실 문제 해결\n",
    "* 구조\n",
    "    1. 망각 게이트 : 셀 층에서 필요 없는건 얼마나 버릴까\n",
    "    2. 인풋 : 얼마나 장기 기억으로 넣을까\n",
    "    3. 아웃풋 : 이중에 얼마나 은닉층으로 빼낼까\n",
    "* 장점 : 장기 기억층을 따로 두면서 은닉층 갱신 가능\n",
    "* 단점 : 복잡해서 파라미터가 많아짐\n",
    "\n",
    "## 2.2 더 단순하고 효율적인 구조 :GRU\n",
    "* LSTM의 단점을 보완하고자 셀 상태를 없애고 망각 게이트와 인풋 게이트를 하나로 합침\n",
    "* LSTM인데, 쉽게 생각하면 셀 상태와 은닉 상태로 두번 나누는 게 아니라 은닉 상태만 두고 덮어쓰기를 지속적으로 진행하는 모델\n",
    "* 장점 : 성능 및 효율 균형이 좋음\n",
    "* 단점 : 긴 시퀀스 처리에서는 한계가 존재\n",
    "\n",
    "## 2.3 LSTM VS GRU\n",
    "1. LSTM 우세\n",
    "    * 시퀀스가 길어 문맥 이해 필요\n",
    "    * 데이터 많고 복잡한 패턴\n",
    "    * 성능이 효율보다 중요\n",
    "    * 언어 번역 및 장기 시계열 등\n",
    "2. GRU 우세\n",
    "    * 자원이 적거나 속도가 중요\n",
    "    * 과적합 위험이 있을 때\n",
    "    * 실시간(빠른) 예측 필요\n",
    "    * 음성인식, 스트리밍 데이터, 실시간 비디오 분석 등등\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c5573",
   "metadata": {},
   "source": [
    "# 3. 문장을 입력받아 문장을 출력하다 : SEQ2SEQ\n",
    "## 3.0 SEQ2SEQ 개관\n",
    "* 인코더 : 말하는 애!\n",
    "    * 입력 시퀀스를 받아 하나의 핵심으로 압축\n",
    "    * 인풋을 바탕으로 문맥 CONTEXT VECTOR 생성\n",
    "* 디코더 : 행동하는 애\n",
    "    * 인코더가 만든 컨텍스트 벡터를 통해 새로운 문장이나 행동을 생성\n",
    "    * 즉 컨텍스트 벡터를 통해 아웃풋을 도출\n",
    "* 특징 \n",
    "    1. 입력과 출력의 길이 달라도 된다\n",
    "    2. 입력하는 인코더와 출력하는 디코다라 분리되어 있음\n",
    "    3. 그 사이에는 문맥 벡터가 존재\n",
    "\n",
    "## 3.1 SEQ2SEQ의 기본 구조 : 인코더와 디코더\n",
    "* 식투식은 시퀀스 사이의 변환이 일어난다!\n",
    "* 입력과 출력의 개수가 같지 않아도 되고, 각각 인코더와 디코더를 가짐\n",
    "1. 인코더 : 입력된 시퀀스를 읽고 압축\n",
    "    * 인코더에서는 T 시점의 임베딩 단어의 입력과 T-1 상태의 은닉 상태가 위 모델들의 입력으로 주어지고, T 시점의 은닉 상태가 나오게 된다\n",
    "2. 디코더 : 압축된 내용을 읽고 시퀀스 생성\n",
    "    * 인코더가 보내준 문백 벡터가 디코더의 첫번째 은닉 상태로 사용됨\n",
    "    * T 시점 나온 출력값이 다음 시점 입력값으로 넘어간다.\n",
    "* 구조 : 인코더와 디코더는 각각 여러 시퀀스 모델들의 조합으로 만들어진 형태이다\n",
    "* 문맥 벡터 : 인코더 마지막 시점의 은닉 상태로, 인코더의 아웃풋\n",
    "## 3.2 SEQ2SEQ의 문제 : 병목 현상\n",
    "* 병목 현상 : 문맥 벡터의 사이즈가 정해져 있어서 생기는 문제\n",
    "* 인코더에 입력되는 문장의 길이가 일정하지 않아 긴 문장에 대해서는 정보 손실이 일어날 수 있따\n",
    "\n",
    "-> 고정된 길이에 입력 시퀀스 정보를 다 담지 못하고 손실될 수 있는 병목 현상 발생\n",
    "* 또한 기울기 소실 문제 및 병렬화 불가 문제 존재 -> ATTENTION 메커니즘의 등장!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c6f88",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
