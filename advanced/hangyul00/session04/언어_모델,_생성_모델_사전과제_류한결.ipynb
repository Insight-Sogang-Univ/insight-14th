{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 언어 모델\n",
        "\n",
        "## 1-1. 언어 모델이란?\n",
        "\n",
        "언어 모델: 언어 모델은 단어 문장에 '확률'을 할당하여 가장 자연스러운 단어 시퀀스를 찾는 모델이다. 즉, '이전 단어들'이 주어졌을 때 '다음 단어'를 예측하는 것!\n",
        "\n",
        "통계 기반 모델과 인공 신경망 기반 모델로 구분된다."
      ],
      "metadata": {
        "id": "svyfMrE1P1Lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2. SLM(통계적 언어 모델)\n",
        "\n",
        "SLM: 비슷한 문맥에서 함께 나타나는 단어들은 비슷한 의미를 가진다는 가설로서 어떤 단어의 주변에 자주 등장하는 단어들을 통해 그 단어의 의미를 짐작할 수 있다는 의미를 지니는 모델이다.\n"
      ],
      "metadata": {
        "id": "tul-TKW4QO7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 조건부 확률과 카운트 기반\n",
        "\n",
        "조건부 확률: 두 확률이 있을 때, A가 일어났다는 전제 하에 B가 일어날 확률을 의미\n",
        "\n",
        "즉, 앞 단어로 시작할 확률을 계산하고, 앞 단어가 주어졌을 때 뒷 단어가 이어질 확률을 계산하며, 앞 두단어가 제시되었을 때 그 다음 단어가 나타날 확률을 계산하는 과정을 개별적으로 곱해서 확률을 계산한다.\n",
        "\n",
        "카운트 기반: 이전 단어 시퀀스의 등장 빈도를 통해 다음 단어의 확률을 계산\n",
        "\n",
        "학습데이터에서 앞 단어가 등장한 모든 경우 중에서, 바로 뒤에 그 뒷 단어가 따라 나온 경우의 비율을 계산한다.\n",
        "\n",
        "-> 그러나, 카운트 기반의 통계적 언어 모델은 직관적이지만 '희소 문제'를 가진다.\n",
        "\n",
        "(희소 문제: 가지고 있는 데이터가 부족해서 언어를 정확하게 모델링하지 못하는 문제)"
      ],
      "metadata": {
        "id": "_vIkycV8QcET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 희소 문제를 완화하기 위한 N-gram\n",
        "\n",
        "N-gram: 연속된 n개의 단어 묶음을 의미한다. 즉, 문장을 정해진 n개의 단위로 잘라서 토큰화하는 것을 의미한다.\n",
        "\n",
        "예를 들어, n=1이면 문장 내에 존재하는 각 묶음(chunk)이며, n=2이면 두 개씩 묶는 것...\n",
        "\n",
        "N-gram 모델에서는 다음 단어를 예측할 때, n-1개의 단어만 참고하는데, 예를 들어, 전체 문장을 참고하는 것이 아니라, n-1개의 단어를 참고하여 다음 단어를 예측한다.\n",
        "\n",
        "그러나, 통계적 언어 모델의 한계는 일부 개선하나 여전히 희소 문제와 문맥 파악의 한계가 있다."
      ],
      "metadata": {
        "id": "wbLQuUezRaL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perplexity(PPL)\n",
        "\n",
        "PPL: 테스트 데이터를 이용해 언어 모델의 성능을 빠르고 정량적으로 평가하기 위한 지표이며, 낮을 수록 좋다.\n",
        "\n",
        "1) 문장의 희소성: 문장이 희소하게 등장하면 예측할 때 혼란을 주기 때문에 문장이 희소하게 등장할수록 PPL이 커진다.\n",
        "\n",
        "2) 문장의 길이 (단어의 개수 N): 단어 개수가 커지면 함께 보는 단어가 많아지므로, 예측할 수 있는 단서가 늘어나 PPL을 낮추게 된다."
      ],
      "metadata": {
        "id": "vk_QcsKMSyIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-3. 딥러닝 기반 언어모델\n",
        "\n",
        "### LLM\n",
        "\n",
        "LLM: 대규모 언어 모델로서 방대한 양의 데이터를 학습하여 인간의 언어를 이해하고 생성하여 요약하는 등 다양한 작업을 수행하도록 설계된 인공지능 모델\n",
        "\n",
        "작동 방식\n",
        "- 대규모 학습: 방대한 데이터에서 언어의 패턴을 스스로 학습한다.\n",
        "- 예측 및 생성: 사용자의 질문이나 지시를 받으면 학습한 패턴을 기반으로 다음에 올 가장 확률이 높은 단어를 순서대로 예측하여 응답을 생성한다.\n",
        "\n",
        "예시: 텍스트 생성, 기계 번역, 질의응답, 감정 분석"
      ],
      "metadata": {
        "id": "Yhi6ptxXU53p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT\n",
        "\n",
        "BERT: 트랜스포머의 인코더 부분을 활용해 만든 양방향 언어 모델로서 감정 분석과 질문 응답에 사용된다.\n",
        "\n",
        "사례: 검색 엔진, 스마트폰 음성 비서, 이메일 자동 분류\n",
        "\n",
        "특징\n",
        "- 양방향: 문장을 읽을 때 앞뒤를 동시에 고려\n",
        "- 트랜스포머 기반: 트랜스포머 구조의 인코더 부분만 사용\n",
        "- 사전학습과 파인튜닝: 큰 데이터로 먼저 일반적인 언어 능력을 배우고, 나중에 특정 과제에 맞게 살짝 조정\n",
        "\n",
        "BERT의 구조 - 트랜스포머의 인코더를 쌓아 올린 구조\n",
        "\n",
        "처리과정\n",
        "1) input\n",
        "2) embedding\n",
        "3) 트랜스포머 인코더 12개(or 24개)\n",
        "4) output"
      ],
      "metadata": {
        "id": "aV0q5dDCVpQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT의 사전학습 1: masked language model\n",
        "- 문장 속 단어를 일부 가리고 그 가려진 단어를 맞추는 방식의 학습이며, 이를 통해 자연스럽게 문맥을 이해하고 추론하는 능력을 갖게 된다.\n",
        "\n",
        "BERT의 사전학습 2: next sentence prediction\n",
        "- 두 문장이 실제로 이어지는 문장인지 아닌지 맞히는 학습으로서 문장 간 관계 이해 능력을 기르기 위한 것이다."
      ],
      "metadata": {
        "id": "nkpaMK-3Wlu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT 이후의 모델\n",
        "\n",
        "1) RoBERTa: BERT의 사전 훈련 프로세스를 개선하여 성능을 향상시킨 모델\n",
        "\n",
        "2) ALBERT: BERT의 경량화 버전"
      ],
      "metadata": {
        "id": "LwcSB7vHW4Tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT\n",
        "\n",
        "GPT: Generative Pre-trained Transformer\n",
        "\n",
        "GPT의 한계: 환각\n",
        "- 사실과 다른 내용을 '그럴듯하게' 만들어내는 현상이다.\n",
        "- 많이 감소시켰으나 아직 해결이 된 것은 아니다."
      ],
      "metadata": {
        "id": "XvhCtfd0XCC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG\n",
        "\n",
        "RAG: Retrieval Augmented Generation의 줄임말로 관련된 정보를 검색하여 먼저 찾고, 이를 LLM에 같이 넣어 최종 답변을 생성하는 하나의 파이프라인 (모르면 물어보는 개념의 도입)\n",
        "\n",
        "RAG의 구조\n",
        "1) 질의 인코더: 사용자의 질문을 이해하는 언어 모델로 주어진 질문을 벡터 형태로 인코딩한다.\n",
        "2) 지식 검색기: 인코딩된 질문을 바탕으로 외부 지식 베이스에서 관련 정보를 검색한다.\n",
        "3) 지식 증강 생성기: 검색된 지식을 활용하여 질문에 대한 답변을 생성하는 언어 모델이다.\n",
        "\n",
        "RAG의 장점\n",
        "1) 풍부한 정보를 제공한다. (검색을 통해 얻은 외부 데이터를 활용하여 구체적이고 풍부한 정보를 제공)\n",
        "2) 실시간 정보 반영한다. (최신 데이터를 검색하여 반영하며 실시간으로 변화하는 정보에 대응)\n",
        "3) 환각을 방지한다. (검색을 통한 실제 데이터에 기반한 답변을 생성)\n",
        "\n",
        "CAG: RAG의 한계를 보완하기 위해 제안된 프레임워크\n",
        "- 기존 RAG는 지식 부족 문제와 환각을 어느 정도 완화하지만 검색된 문서 중 일부가 신뢰성이 낮거나 잘못된 정보일 경우 생성된 답변의 품질과 정확도가 떨어진다.\n",
        "- 이러한 부정적 영향을 최소화하고, 모델이 문서를 활용하는 수준을 넘어 신뢰도를 스스로 판단하고 활용하도록 훈련시키는 기법이다."
      ],
      "metadata": {
        "id": "o90lgy1TXSxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain\n",
        "\n",
        "LangChain: 모델 파이프라인을 쉽게 구성할 수 있도록 도와주는 역할을 하는 것으로서 LLM을 기반으로 앱을 만들 때 필요한 구성 요소를 쉽게 연결하도록 하며 다양한 라이브러리를 제공하여 다양하게 활용이 가능하다.\n",
        "\n",
        "LangChain의 특징\n",
        "1) 추상화: 각종 작업을 간결하게 표현하고 간소화한다.\n",
        "2) 표준화: 비슷한 기능을 갖춘 요소들을 똑같은 형식을 갖춘 컴포넌트로 표준화한다.\n",
        "3) 체이닝: 컴포넌트를 쉽게 연결해서 LLM 서비스의 로직을 쉽게 파악한다.\n",
        "\n",
        "LangChain의 한계 - 중간에 조건에 따라 다른 작업을 수행하거나, 결과가 마음에 들지 않을 때 이전 단계로 돌아가서 다시 실행하는 것과 같은 '복잡한' 로직 수행이 어렵다.\n",
        "\n",
        "Langgraph: 비선형적인 작업 형태로 '사이클', '루프'형태로 만드는 것으로서 순환/반복 로직이 필요하거나 여러 번 대화를 주고받아야 할 경우 유리하다.\n",
        "\n",
        "LangSmith: LLM 애플리케이션은 사용자의 질문이 어떤 프롬프트를 거쳐 LLM에게 전달되고 그 결과가 어떻게 가공되어 최종 답변이 되는지 파악하기가 어려워 이를 해결하고자 한 플랫폼\n",
        "\n",
        "LangSmith의 기능\n",
        "1) 디버깅 및 추적: 모든 실행 과정을 단계별로 시각화\n",
        "2) 테스트 및 평가: 미리 준비된 질문과 정답 데이터셋을 만들어 개발 중인 LLM 애플리케이션의 성능을 자동으로 평가\n",
        "3) 모니터링: 실제 사용을 모니터링\n",
        "4) 프롬프트 허브: 다양한 프롬프트를 저장하고 버전을 관리하며 쉽게 테스트할 수 있는 공간을 제공한다."
      ],
      "metadata": {
        "id": "JSfN5UW1YEpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-4. sLM (small Language Model)\n",
        "\n",
        "sLM: 자연어 콘텐츠를 처리, 이해 및 생성할 수 있는 AI 모델롤 규모와 범위가 작다."
      ],
      "metadata": {
        "id": "06TIxm-RZT7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 압축\n",
        "\n",
        "1. 가지치기: 신경망에서 중요도가 낮거나 중복되거나 불필요한 매개변수를 제거한다.\n",
        "2. 양자화: 고정밀 데이터를 저정밀 데이터로 변환하여 계산 부하를 줄이고 추론 속도를 높인다.\n",
        "3. 지식 증류: 사전 학습된 교사 모델의 학습 내용을 학생 모델로 이전하는 방법이다."
      ],
      "metadata": {
        "id": "rL1lx2F-ZdJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sLM의 예시\n",
        "\n",
        "ex) DistilBERT, Gemma, GPT-4o mini 등..."
      ],
      "metadata": {
        "id": "56bNPEtXZpOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sLM의 장점\n",
        "\n",
        "1. 프라이버시: 외부 서버가 아닌 로컬 환경에서 실행할 수 있다.\n",
        "2. 비용 절감: 학습과 추론에 필요한 자원이 적어 비용이 낮고 에너지 소비가 적다.\n",
        "3. 효율성 & 맞춤화: 특정 작업에 최적화되기 쉽고, 실시간 처리같은 분야에서 높은 성능을 보인다.\n",
        "\n",
        "### sLM의 한계\n",
        "\n",
        "1. 편항: LLM에 존재하는 편향으로부터 학습할 수 있고 성능저하로 이어질 수 있다.\n",
        "2. 제한된 일반화: LLM에 비해 광범위한 지식 기반이 부족하여 포괄적이거나 복잡한 지식이 필요할 때 성능이 떨어질 수 있다.\n",
        "3. 환각: LLM과 마찬가지로 환각 발생 가능하다.\n",
        "4. 성능과 용량의 한계: 학습과 추론에서 성능과 용량이 부족하다."
      ],
      "metadata": {
        "id": "Xk4MeemAZvGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM과 sLM의 결합 - 결합 추론\n",
        "\n",
        "sLM이 어려운 문제를 풀 때, 막히는 부분만 LLM에게 선택적으로 도움을 받아 해결하는 방식\n",
        "\n",
        "작동 원리\n",
        "1) 작은 모델이 먼저 해결\n",
        "2) 각 단계를 평가하고 점수를 매김\n",
        "3) 어려운 부분의 경우 LLM이 해결"
      ],
      "metadata": {
        "id": "8QX1d8TpaQUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 생성 모델\n",
        "\n",
        "### 2-1. 분류 모델 vs 생성 모델\n",
        "\n",
        "생성 모델: 주어진 학습 데이터를 학습하여 학습 데이터의 분포를 따르는 유사한 데이터를 생성하는 모델\n",
        "\n",
        "생성 모델의 분류\n",
        "1) 명시적 확률밀도 모델: 학습 데이터의 분포를 기반으로 생성하는 방법\n",
        "2) 암시적 확률밀도 모델: 학습 데이터의 분포와 상관없이 생성하는 방법"
      ],
      "metadata": {
        "id": "SSnk5YtfafMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2. AE, VAE\n",
        "\n",
        "AE(Auto Encoder): 입력과 동일한 출력을 만드는 것을 목적으로 하는 신경망으로 차원 축소, 특징 추출, 노이즈 제거, 이상 탐지 등 데이터 복원이나 특성 학습에 많이 사용된다.\n",
        "\n",
        "AE의 구조\n",
        "1) 인코더: 원래의 고차원 입력 데이터를 잠재 표현로 변환하는 네트워크\n",
        "2) 디코더: 잠재 표현을 풀어서 입력을 재복원하여 출력하는 네트워크\n",
        "3) 잠재 표현: 원본 데이터를 저차원으로 압축하여 함축된 정보를 저장하고 있는 벡터\n",
        "\n",
        "VAE(Variational Auto Encoder): 확률적 오토인코더는 데이터를 잠재 공간으로 인코딩하고 그 잠재 공간에서 다시 데이터를 디코딩하여 원본 데이터와 유사한 결과를 생성하는 방식으로 이미지 생성, 텍스트 생성, 신호 처리, 이미지 보간 등 새로운 데이터를 생성하는 작업에 활용된다.\n",
        "\n",
        "VAE의 구조\n",
        "1) 인코더: 원래의 고차원 데이터를 단일 벡터가 아닌 분포로 변환하는 네트워크\n",
        "2) 잠재 벡터: 원본 데이터를 저차원으로 압축하여 함축된 정보를 저장하고 있는 벡터\n",
        "3) 디코더: 샘플링된 잠재 표현을 풀어서 입력을 재복원하여 출력하는 네트워크"
      ],
      "metadata": {
        "id": "Xgtu8XCXasy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-3. GAN\n",
        "\n",
        "GAN(Generative Adversarial Network):생성적 적대 신경망은 생성자 신경망과 판별자 신경망이 서로 적대적으로 경쟁하면서 훈련을 통하여 자신의 작업을 점점 더 정교하게 수행하는 신경망 모델이다.\n",
        "\n",
        "GAN의 구조 - 서로 대립하는 2개의 신경망\n",
        "1) 위조품을 만들려는 '생성자'로 품질이 높은 위조품을 만드는 목적이 있다.\n",
        "2) 생성자가 생성한 위조품과 진품을 구별하는 '판별자'로 생성자가 만든 위조품을 정확하게 판별하는 목적이 있다.\n",
        "\n",
        "GAN의 적용 사례\n",
        "1) 가짜 이미지 생성\n",
        "2) 가짜 연설 영상\n",
        "3) Eye In-Painting\n",
        "\n",
        "GAN의 장점 - 진짜 같은 가짜를 생성할 수 잇다.\n",
        "\n",
        "GAN의 단점 - 학습이 불안정하다."
      ],
      "metadata": {
        "id": "-LzqL20rbbfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-4. 확산 모델\n",
        "\n",
        "확산 모델: 입력 이미지에 noise를 여러 단계에 걸쳐 추가하고, 여러 단계에 걸쳐 noise를 제거함으로써 입력 이미지와 유사한 확률 분포를 가진 결과 이미지를 생성하는 모델이다.\n",
        "\n",
        "1) 순확산\n",
        "- 순확산 과정은 데이터에 점진적으로 노이즈를 추가하는 과정이며 원래 데이터를 점차적으로 무작위 노이즈로 변형시키는 것이다.\n",
        "\n",
        "2) 역확산\n",
        "- 역확산 과정은 노이즈 데이터에서 원본 데이터를 재구성하는 과정이다. 순확산 과정에서 추가된 노이즈를 제거하여 원래의 데이터를 복원하는 것이다."
      ],
      "metadata": {
        "id": "9xAcirl9b866"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8ZD7YLfPQqG"
      },
      "outputs": [],
      "source": []
    }
  ]
}