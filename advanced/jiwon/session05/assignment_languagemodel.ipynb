{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BzWQexmpCCI"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/Insight-Sogang-Univ/insight-14th/blob/main/advanced/template/session05/assignment_languagemodel.ipynb\" target=\"_parent\">\n",
        "      <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaPYWHM5Y0Qh"
      },
      "source": [
        "# ğŸ” ì´ë¡  ë¬¸ì œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dafmjcvzoMh3"
      },
      "source": [
        "### 1. SLM(í†µê³„ì  ì–¸ì–´ëª¨ë¸)ì˜ í•œê³„ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdXgXdCqoQgd"
      },
      "source": [
        "**[1ë²ˆ ë¬¸ì œ ì •ë‹µ]**\n",
        "\n",
        "***ì—¬ê¸°ì— ì •ë‹µì„ ì¨ì£¼ì„¸ìš”!*** \\\n",
        "í¬ì†Œì„± ë¬¸ì œ: ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡, ê·¸ ë¬¸ì¥ ì•ˆì˜ íŠ¹ì • í‘œí˜„ì„ ë‹´ì€ í…ìŠ¤íŠ¸ë¥¼ ì›ë³¸ í•™ìŠµ ë°ì´í„°ì—ì„œ ì°¾ê¸°ê°€ ì–´ë ¤ì›Œì§„ë‹¤.  \n",
        "-> ê·¸ í‘œí˜„ì´ í•™ìŠµ ë°ì´í„°ì— ì—†ë‹¤ë©´, í™•ë¥ ì„ ê³„ì‚°í•  ìˆ˜ ì—†ì–´ì§€ëŠ” ë¬¸ì œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8cvXUjfoY0h"
      },
      "source": [
        "### 2. BERTì— ëŒ€í•œ ì„¤ëª…ì´ ì•„ë‹Œ ê²ƒì€?\n",
        "\n",
        "a) ì‚¬ì „ í•™ìŠµ ëª¨ë¸\n",
        "\n",
        "b) ì–‘ë°©í–¥ ë¬¸ë§¥ ì´í•´\n",
        "\n",
        "c) íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë””ì½”ë”ë¥¼ ìŒ“ì•„ ì˜¬ë¦° êµ¬ì¡°\n",
        "\n",
        "d) ì´í›„ RoBERTaì™€ ALBERTë¡œ ë°œì „"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuSdngcrob_v"
      },
      "source": [
        "**[2ë²ˆ ë¬¸ì œ ì •ë‹µ]**\n",
        "\n",
        "***ì—¬ê¸°ì— ì •ë‹µì„ ì¨ì£¼ì„¸ìš”!*** \\\n",
        "c: BERTëŠ” ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AavVEOYMoiXO"
      },
      "source": [
        "### 3. BERTì™€ GPTì˜ í•™ìŠµ ë°©ì‹ ì°¨ì´ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkhYXD6jolM8"
      },
      "source": [
        "**[3ë²ˆ ë¬¸ì œ ì •ë‹µ]**\n",
        "\n",
        "***ì—¬ê¸°ì— ì •ë‹µì„ ì¨ì£¼ì„¸ìš”!*** \\\n",
        "BERT: íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ë¥¼ ì‚¬ìš©, MLMê³¼ NSPë¥¼ í†µí•´ ì–‘ë°©í–¥ ë¬¸ë§¥ì„ ìœ„ì£¼ë¡œ í•™ìŠµ  \n",
        "GPT: íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”ë¥¼ ì‚¬ìš©, ë‹¨ë°©í–¥ì„±ìœ¼ë¡œ, ë‹¤ìŒì— ì˜¬ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ë©´ì„œ ë¬¸ì¥ì„ ìƒì„±"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F36oD9dOouGL"
      },
      "source": [
        "### 4. LLMì´ ì§€ë‹Œ í•œê³„ì™€ RAGì˜ í•„ìš”ì„±ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„œìˆ í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra1jp2PfoxSc"
      },
      "source": [
        "**[4ë²ˆ ë¬¸ì œ ì •ë‹µ]**\n",
        "\n",
        "***ì—¬ê¸°ì— ì •ë‹µì„ ì¨ì£¼ì„¸ìš”!*** \\\n",
        "í™˜ê°(í• ë£¨ì‹œ) ë¬¸ì œ: LLMì´ íŠ¹ì • ë¶„ì•¼ë‚˜ ì§€ì—½ì ì¸ ì£¼ì œì— ëŒ€í•´ ëª¨ë¥¼ ë•ŒëŠ” ë§ì„ ê·¸ëŸ´ ë“¯í•˜ê²Œ ì§€ì–´ë‚´ëŠ” í˜„ìƒ\n",
        "\n",
        "RAGì˜ í•„ìš”ì„±: ìˆ˜ë§ì€ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œë„ í™˜ê°ì€ ë°œìƒí•¨. ê·¸ë¦¬ê³  ìƒˆë¡œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì€ ì²œë¬¸í•™ì ì¸ ë¹„ìš©ì„ ìš”êµ¬í•¨. ë”°ë¼ì„œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ê³ +LLMê³¼ í•¨ê»˜ ë‹µë³€ ìƒì„± í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ë§Œë“  ê²ƒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci0-WuuQo2pu"
      },
      "source": [
        "### 5. LLMì„ ê²½ëŸ‰í™”í•œ ëª¨ë¸ì¸ sLMì„ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ëŒ€í‘œì ì¸ ê¸°ìˆ  3ê°€ì§€ë¥¼ ì ìœ¼ì„¸ìš”.(ë‹¨ë‹µí˜•)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCb-JWJVo5mf"
      },
      "source": [
        "**[5ë²ˆ ë¬¸ì œ ì •ë‹µ]**\n",
        "\n",
        "***ì—¬ê¸°ì— ì •ë‹µì„ ì¨ì£¼ì„¸ìš”!*** \\\n",
        "ì–‘ìí™”, ê°€ì§€ì¹˜ê¸°, ì§€ì‹ ì¦ë¥˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6InYiHirZzVy"
      },
      "source": [
        "# âœ ì–¸ì–´ ëª¨ë¸ ê³¼ì œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFWRcK_cZ9sK"
      },
      "source": [
        "### ğŸ§® GPU ì‚¬ìš©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJaw3Bh7aBhA",
        "outputId": "69fc9cc6-2c70-49cd-cc85-9146f6b59400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… [PyTorch] Using device: cuda\n",
            "GPU name: Tesla T4\n",
            "\n",
            "âœ… [TensorFlow] GPU Devices:\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "# PyTorch GPU ìƒíƒœ\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"âœ… [PyTorch] Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "\n",
        "# TensorFlow GPU ìƒíƒœ\n",
        "print(\"\\nâœ… [TensorFlow] GPU Devices:\")\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsVU927lLs2Z"
      },
      "source": [
        "# ğŸ” 1. BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMZ40xeWMFIu"
      },
      "source": [
        "**BERT (Bidirectional Encoder Representations from Transformers)**\n",
        "- ë¬¸ì¥ ë¶„ë¥˜(ìŠ¤íŒ¸ ë©”ì¼ íƒì§€), ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ(ì±—ë´‡, ê²€ìƒ‰ ì—”ì§„), ë²ˆì—­(ë‹¤êµ­ì–´ ì§€ì›), í…ìŠ¤íŠ¸ ìš”ì•½(ë‰´ìŠ¤ ìš”ì•½) ë“± ë‹¤ì–‘í•œ NLP ì‘ì—…ì— ì‚¬ìš©\n",
        "- ë¬¸ë§¥ì„ ì–‘ë°©í–¥ìœ¼ë¡œ ì´í•´í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ì •ë°€í•˜ê²Œ íŒŒì•…í•˜ë©°, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì‘ìš© ê°€ëŠ¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inLp1BtKM_e7"
      },
      "source": [
        "### 1-1. ëª¨ë¸ê³¼ tokenizer ì´ˆê¸°í™”\n",
        "- tokenizerë¥¼ í†µí•´ ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ì´ë¥¼ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397,
          "referenced_widgets": [
            "ca857567ace8425bada9d721cf1a1a98",
            "a98212b3d8ea4fb7acfb36de64145506",
            "eef35f2a95d74ca1ae8d45ffa15f080d",
            "e1b73c962a0c44389bed0fa775bee906",
            "4c4e9e4fe73f4dff93e16bdb9b725138",
            "935244da35c748b2909d5cbcf4194a9d",
            "2353884f37b24bba9dbfcaff6d09fd5a",
            "7283c0d29f78428c8cad5ab8fa16cb85",
            "0102a7c33be341689c26a87929f5eb02",
            "71aa85898b2b401287b77b6d9b8bc26f",
            "8a039e2d11ca4046ac49f9788390877f",
            "d48896e2f4a84eb28a0000c91960a59f",
            "14cacc2e80b148f39fcdac8bb1f07d59",
            "008a0691008a45fda9f34d1745b77b8f",
            "54b168f12e234d8c9af0d77bb56e9635",
            "bcb2750827ee49c39b075021be70e14a",
            "3f7bad76bc954e52be9ad59342428223",
            "77cc2b0fc14840a49f81c9166f9debf1",
            "e71e30808cfd4d918d9026b8dc02090c",
            "73c302867acc43ea83ff63b7faf62121",
            "e267fa5daac44f63b62a2f7c2a4d92ba",
            "5640cfbb7aa54eaeb8eae66e37bb25d3",
            "71c343d3301f4f25959914a295b2edf4",
            "18a5757642fe43baa7189a9e20fadfc4",
            "c89bdbc9b1944b92973fb6ee8260abf3",
            "959d3f8d858e4ef5910aececda9c8b53",
            "e43b8844286d4180aac3115cf4911458",
            "4cbe8c38766e44ca89621f65dc59a361",
            "f3e25d13952e4f2c994ff71ac37e27bf",
            "9df2184838b24ccba8b30d8bd730f93e",
            "ad991c8a54e240b387ef9a5ba950afa4",
            "56a24d0e6a554bda9252010c4a540d84",
            "46fb814a895146a1b57758a710109da3",
            "9a13884dd72040edbb5253f039e668c9",
            "b679bad4e988494a927bf5fde9e0799e",
            "e784714a2fbf4502b05175c4573ea37c",
            "c0c3700446114cfbaf2b48aaa7cab794",
            "e6568e9b97f34acea340f9e9a34c6a4b",
            "a29f0d77f21047cc8464aae496e47816",
            "c618a8621b5f4e9696c3e142b63db495",
            "97d130f79d5a4b6e84a56b34bbe5a777",
            "0ea613cd63aa4e7d9d4e49046a759b22",
            "0bb19b8cd1aa4c8a8bd7b9ec8bfaedea",
            "8e0e6b301d824d4387f7baea5ee8c306",
            "4bca2dd699ec4c26a286868ef8cee337",
            "fd4f070e955e4181bd990a258160d410",
            "66004ea5a3684770a32ca286ae6e7484",
            "a45bc2a4f07e40b49850529340deaf79",
            "13cd0b29a6284d609c151ffbf4bcff85",
            "54ba897fdb2e4b0bafe5037e5c811f36",
            "eb544eb00ade4f70b74773790ea36b44",
            "e0da96a5f3984292af77f38d092ea25f",
            "c035346ec22d470184be421ca966ecb7",
            "92c33c880dde4a089ef9a0ac68460de4",
            "161d0fc789ae49f3996f588a57e777ad",
            "8d3f5c5679c14aa08b691244f79bd000",
            "1f9c744d00d14b5b84c6e321087087d2",
            "dc3e816721fd4c539fb7be730260d473",
            "79cdbe4f0c3742ce9e526abdf932a07e",
            "65aca7deb96a4abebe5b3699d41274d0",
            "efe4b8e63ac84fb8844fa782a66bee2c",
            "2dbb3700ebb64339b435f1385081cff5",
            "b50c5579dd484f41ae9dade43520eae9",
            "c02e27cb87f14eaa91321982c6a6631e",
            "9cbbf6fb9fe8425f94687b67b7cc9797",
            "d728b83228bd45b68c974a083b1e6f58"
          ]
        },
        "id": "r1m1YLY3GNM0",
        "outputId": "b0b38885-b4b5-4195-bdd6-1ff00264f9e9"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "# BERT ëª¨ë¸ê³¼ tokenizer ì´ˆê¸°í™”\n",
        "tokenizer = BertTokenizer.from_pretrained('klue/bert-base') # base : ëª¨ë¸í¬ê¸°, (uncased : ì†Œë¬¸ì í•™ìŠµ)\n",
        "model = BertForMaskedLM.from_pretrained('klue/bert-base') # base : ëª¨ë¸í¬ê¸°, (uncased : ì†Œë¬¸ì í•™ìŠµ)\n",
        "# tokenizer : raw textë¥¼ ê°œë³„ í† í°ìœ¼ë¡œ ë¶„ë¦¬(Wordpiece). í† í¬ë‚˜ì´ì € ì‚¬ì „ì— ë”°ë¼ ê³ ìœ í•œ ì •ìˆ˜ ì¸ë±ìŠ¤(ê³ ì •ê°’)ë¥¼ ë§¤í•‘í•¨.\n",
        "\n",
        "# tokenizerë¥¼ í†µí•´ ìƒì„±ëœ ì •ìˆ˜ ì¸ë±ìŠ¤ : í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì½ì„ ìˆ˜ ìˆë„ë¡ ìˆ«ìí™”\n",
        "# Embedding vector : ë‹¨ì–´ì˜ ì˜ë¯¸ì , ë¬¸ë§¥ì  íŠ¹ì„±ì„ ëª¨ë¸ë§"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ4zRzObNqAd"
      },
      "source": [
        "### 1-2. ì…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„ ë° í† í°í™”\n",
        "- [MASK] í† í°ì„ ì‚½ì…í•œ ë¬¸ì¥ì„ ì„¤ì •í•˜ê³ , `tokenizer.tokenize()`ë¡œ í…ìŠ¤íŠ¸ í† í°í™”\n",
        "- [MASK]ë¥¼ í™œìš©í•˜ì—¬ ë¬¸ë§¥ ì†ì—ì„œ íŠ¹ì • ë‹¨ì–´ë¥¼ ì¶”ë¡ í•˜ëŠ” ìƒí™©ì„ ë§Œë“¤ê³ , ì´ë¥¼ ëª¨ë¸ì´ í•™ìŠµí•œ íŒ¨í„´ê³¼ ë¹„êµí•˜ë„ë¡ í•¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKUXDaU1GRwy",
        "outputId": "5f398770-03f0-4833-996f-f1cbdd76fde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'ì—´', '##ì—¬', '##ëŸ', ',', 'ìš°ë¦¬', '##ëŠ”', 'ì„œë¡œ', '##ì˜', 'ì´ë¦„', '##ì„', 'ì²˜ìŒ', 'ë¶ˆë €', '##ë‹¤', '.', 'ê·¸ë¦¬ê³ ', 'ìŠ¤ë¬¼', 'í•˜ë‚˜', ',', 'ìš°ë¦°', '[MASK]', 'ì„', 'í–ˆ', '##ë‹¤', '.', '[SEP]']\n",
            "[2, 1432, 2173, 3542, 16, 3616, 2259, 4084, 2079, 3934, 2069, 3790, 6895, 2062, 18, 3673, 10514, 3657, 16, 8983, 4, 1498, 1902, 2062, 18, 3]\n"
          ]
        }
      ],
      "source": [
        "# í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥\n",
        "text = \"[CLS] ì—´ì—¬ëŸ, ìš°ë¦¬ëŠ” ì„œë¡œì˜ ì´ë¦„ì„ ì²˜ìŒ ë¶ˆë €ë‹¤. ê·¸ë¦¬ê³  ìŠ¤ë¬¼ í•˜ë‚˜, ìš°ë¦° [MASK]ì„ í–ˆë‹¤.[SEP]\"\n",
        "\n",
        "# ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ë³€í™˜\n",
        "tokenized_text = tokenizer.tokenize(text) # text(ë¬¸ì¥)ì„ í† í°ìœ¼ë¡œ ë³€í™˜í•´ tokenized_textì— ì €ì¥í•©ì‹œë‹¤!\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) # í† í°ì„ indexë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œ!\n",
        "\n",
        "print(tokenized_text) # í† í°ì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
        "print(indexed_tokens) # ì¸ë±ìŠ¤ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nrWA_XEN4I3"
      },
      "source": [
        "### 1-3. MASKëœ ìœ„ì¹˜ í™•ì¸\n",
        "- [MASK] í† í°ì˜ ìœ„ì¹˜ë¥¼ íƒì§€í•´ í•´ë‹¹ ìœ„ì¹˜ì—ì„œ ëª¨ë¸ì´ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ì§€ì •í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAOGhKOrKrRC",
        "outputId": "76218ad0-ca0c-4e85-cdbf-f5fa3f8031c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "# ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ ì°¾ê¸°\n",
        "masked_index = tokenized_text.index(\"[MASK]\") # í† í°í™”ëœ ê²°ê³¼ë¬¼ì—ì„œ index ë©”ì„œë“œë¥¼ í†µí•´ [MASK]ì˜ ìœ„ì¹˜ë¥¼ í™•ì¸í•´ë´…ì‹œë‹¤!\n",
        "print(masked_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3oqIAqDOEo_"
      },
      "source": [
        "### 1-4. ì •ìˆ˜í™”ëœ í† í° ì¸ë±ìŠ¤ë¥¼ tensorë¡œ ë³€í™˜ í›„ ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰\n",
        "- ëª¨ë¸ì€ **í…ì„œ ë°ì´í„° êµ¬ì¡°**ë¥¼ í•„ìš”ë¡œ í•˜ë¯€ë¡œ, **ì…ë ¥ê°’ì„ ë³€í™˜**í•˜ì—¬ ì í•©í•œ í˜•ì‹ìœ¼ë¡œ ë§Œë“¦.\n",
        "   - `torch.tensor(ì…ë ¥ê°’ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ì…ë ¥ê°’ ndarray)`ë¥¼ í†µí•´ tensor êµ¬ì¡°ë¡œ ë³€í™˜ ê°€ëŠ¥!\n",
        "- **ì—­ì „íŒŒë¥¼ ë¹„í™œì„±í™”**í•˜ê³ , ëª¨ë¸ì´ **ê° í† í°ì— ëŒ€í•´ ê°€ëŠ¥í•œ ëª¨ë“  ë‹¨ì–´ ì ìˆ˜ ì¶œë ¥**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2T3XriDKuJD",
        "outputId": "1b2a0c91-70f6-42a0-90f9-5fbe9d31a767"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ -6.0337,   4.5526,  -5.6312,  ...,  -7.3959,  -7.4220,  -5.8337],\n",
            "         [ -6.1762,   4.7585,  -7.3973,  ...,  -7.6209, -12.2124,  -6.0998],\n",
            "         [ -7.4194,   3.9148,  -6.1517,  ...,  -6.8162,  -9.5421,  -3.0473],\n",
            "         ...,\n",
            "         [ -7.3761,   8.6972,  -5.4904,  ...,  -8.7202,  -9.1403,  -5.8566],\n",
            "         [ -5.6347,  10.3884,  -4.3374,  ...,  -8.6900,  -7.4171,  -3.5043],\n",
            "         [ -5.6652,  10.3407,  -4.4099,  ...,  -8.7521,  -7.4286,  -3.6681]]])\n"
          ]
        }
      ],
      "source": [
        "# í† í°í™”ëœ í…ìŠ¤íŠ¸ -> ì •ìˆ˜ì¸ë±ìŠ¤(ID) -> Pytorch í…ì„œ\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text) # í† í°ì„ indexë¡œ ë³€í™˜í•˜ëŠ” ì½”ë“œ!\n",
        "tokens_tensor = torch.tensor([indexed_tokens]) # ì…ë ¥ê°’(í† í° ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸)ì„ tensor êµ¬ì¡°ë¡œ ë³€í™˜í•´ë´…ì‹œë‹¤!\n",
        "\n",
        "# ëª¨ë¸ì— í† í° í…ì„œë¥¼ ì „ë‹¬í•˜ê³  ì˜ˆì¸¡ ì‹¤í–‰\n",
        "with torch.no_grad(): # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì—†ìœ¼ë¯€ë¡œ ìë™ë¯¸ë¶„ì—°ì‚°ì€ ë”\n",
        "    outputs = model(tokens_tensor) # ì˜ˆì¸¡. ëª¨ë¸ì€ ê° í† í°ìœ„ì¹˜ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜.\n",
        "    predictions = outputs[0] # logits (ê° í† í°ìœ„ì¹˜ì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  í† í°ë“¤ì˜ ì›ì‹œì ìˆ˜)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIkvkAL5Oalc"
      },
      "source": [
        "### 1-5. MASKëœ í† í° ì˜ˆì¸¡\n",
        "- [MASK] ìœ„ì¹˜ì—ì„œ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì€ í† í°ì˜ ì¸ë±ìŠ¤ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO5B91p4KwI0",
        "outputId": "8edec0da-b576-4ca1-91d6-4ea0de32e828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: [CLS] ì—´ì—¬ëŸ, ìš°ë¦¬ëŠ” ì„œë¡œì˜ ì´ë¦„ì„ ì²˜ìŒ ë¶ˆë €ë‹¤. ê·¸ë¦¬ê³  ìŠ¤ë¬¼ í•˜ë‚˜, ìš°ë¦° [MASK]ì„ í–ˆë‹¤.[SEP]\n",
            "Masked: ['[CLS]', 'ì—´', '##ì—¬', '##ëŸ', ',', 'ìš°ë¦¬', '##ëŠ”', 'ì„œë¡œ', '##ì˜', 'ì´ë¦„', '##ì„', 'ì²˜ìŒ', 'ë¶ˆë €', '##ë‹¤', '.', 'ê·¸ë¦¬ê³ ', 'ìŠ¤ë¬¼', 'í•˜ë‚˜', ',', 'ìš°ë¦°', 'ì‚¬ë‘', 'ì„', 'í–ˆ', '##ë‹¤', '.', '[SEP]']\n",
            "Predicted token: ì‚¬ë‘\n",
            "\n",
            "Filled sentence: ì—´ì—¬ëŸ , ìš°ë¦¬ëŠ” ì„œë¡œì˜ ì´ë¦„ì„ ì²˜ìŒ ë¶ˆë €ë‹¤ . ê·¸ë¦¬ê³  ìŠ¤ë¬¼ í•˜ë‚˜ , ìš°ë¦° ì‚¬ë‘ ì„ í–ˆë‹¤ .\n"
          ]
        }
      ],
      "source": [
        "# ì˜ˆì¸¡ëœ í† í° í™•ì¸\n",
        "predicted_index = torch.argmax(predictions[0, masked_index]).item() # masked_indexì— ë“¤ì–´ê°ˆ ê²ƒìœ¼ë¡œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ì¸ë±ìŠ¤\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "\n",
        "# ì˜ˆì¸¡ëœ í† í°ìœ¼ë¡œ ë§ˆìŠ¤í¬ ì±„ìš°ê¸°\n",
        "tokenized_text[masked_index] = predicted_token\n",
        "# í† í°í™”ëœ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
        "filled_text = tokenizer.convert_tokens_to_string(tokenized_text[1:-1])\n",
        "\n",
        "print(\"Original:\", text)\n",
        "print(\"Masked:\", tokenized_text)\n",
        "print(\"Predicted token:\", predicted_token) ; print()\n",
        "\n",
        "print(\"Filled sentence:\", filled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4ncI4tBOWLh"
      },
      "source": [
        "\n",
        "### 1-6. MASKëœ ë¬¸ì¥ ë³µì›\n",
        "- ì˜ˆì¸¡ëœ í† í°ìœ¼ë¡œ [MASK]ë¥¼ ëŒ€ì²´í•˜ì—¬ ì™„ì„±ëœ ë¬¸ì¥ ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQJBSmfZK2l2",
        "outputId": "a6d75a62-f5f5-482c-e316-cdfc43709e4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "# BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
        "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
        "model = BertForMaskedLM.from_pretrained('klue/bert-base')\n",
        "\n",
        "def fill_mask(input_text):\n",
        "    # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜\n",
        "    tokenized_text = tokenizer.tokenize(input_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ ì°¾ê¸°\n",
        "    masked_index = tokenized_text.index(\"[MASK]\") # í† í°í™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ë¥¼ ì°¾ì•„ë´…ì‹œë‹¤!\n",
        "\n",
        "    # í† í°í™”ëœ í…ìŠ¤íŠ¸ -> ì •ìˆ˜ì¸ë±ìŠ¤(ID) -> Pytorch í…ì„œ\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "    # ëª¨ë¸ì— í† í° í…ì„œë¥¼ ì „ë‹¬í•˜ê³  ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor)\n",
        "        predictions = outputs[0]\n",
        "\n",
        "    # ì˜ˆì¸¡ëœ í† í° í™•ì¸\n",
        "    predicted_index = torch.argmax(predictions[0, masked_index]).item() # ìœ„ ì½”ë“œ ì°¸ê³ ! masked_indexì— ë“¤ì–´ê°ˆ ê²ƒìœ¼ë¡œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ì¸ë±ìŠ¤\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "\n",
        "    # ë§ˆìŠ¤í¬ë¥¼ ì±„ìš´ ë¬¸ì¥ ë°˜í™˜\n",
        "    tokenized_text[masked_index] = predicted_token # í† í°í™”ëœ í…ìŠ¤íŠ¸ì˜ ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜(ì¸ë±ìŠ¤ ì‚¬ìš©)ë¥¼ ì˜ˆì¸¡ëœ í† í°ìœ¼ë¡œ ì±„ì›Œë´…ì‹œë‹¤!\n",
        "    return tokenizer.convert_tokens_to_string(tokenized_text[1:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeAYIzyYPVVt"
      },
      "source": [
        "ì˜ˆì‹œ ë¬¸ì¥ ë§Œë“¤ì–´ì„œ MLMì„ ì‚¬ìš©í•´ ë³´ì„¸ìš”!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q81b8zuYK3E9",
        "outputId": "9c0af4b5-f1cb-4a9a-f9ab-57e38826d9ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filled sentence: ì‹œì¤‘ê°€ë³´ë‹¤ ì €ë ´í•˜ê²Œ ì‚¬ì„œ ë„ˆë¬´ ì¢‹ì•„ìš” ì§‘ë“¤ì´í• ë•Œ ì¼ëŠ”ë° ë„ˆë¬´ ì´ì˜ë”ë¼êµ¬ìš” ë‹¤ë¥¸ ì œí’ˆ ë„ ì‚¬ê³ ì‹¶ë„¤ìš” ì„¸ì¼ ë˜ ê¸°ë‹¤ë¦´ê²Œìš”\n"
          ]
        }
      ],
      "source": [
        "# ì˜ˆì‹œ 1: \"[CLS] ì—´ì—¬ëŸ, ìš°ë¦¬ëŠ” ì„œë¡œì˜ ì´ë¦„ì„ ì²˜ìŒ ë¶ˆë €ë‹¤. ê·¸ë¦¬ê³  ìŠ¤ë¬¼ í•˜ë‚˜, ìš°ë¦° [MASK]ì„ í–ˆë‹¤.[SEP]\"\n",
        "# ì˜ˆì‹œ 2: \"[CLS] ìœ¤í•´ë¯¼ì€ INSGIHT í•™íšŒë¥¼ ì •ë§ ì‚¬ë‘í•œë‹¤. ê·¸ëŠ” INSIGHT í•™íšŒë¥¼ ìœ„í•´ [MASK]ë¥¼ í–ˆë‹¤.[SEP]\"\n",
        "# ì´ ì˜ˆì‹œë¥¼ ì°¸ê³ í•´ì„œ [CLS](ë§¨ ì•), [MASK](ì±„ìš¸ ë¶€ë¶„), [SEP](ë§¨ ë’¤)ë¥¼ ì¶”ê°€í•´ì„œ ì—¬ëŸ¬ë¶„ë§Œì˜ ì˜ˆì‹œ ë¬¸ì¥ ë§Œë“¤ì–´ì£¼ì„¸ìš”\n",
        "input_text = \"[CLS] ì‹œì¤‘ê°€ë³´ë‹¤ ì €ë ´í•˜ê²Œ ì‚¬ì„œ ë„ˆë¬´ ì¢‹ì•„ìš” ì§‘ë“¤ì´í• ë•Œ ì¼ëŠ”ë° ë„ˆë¬´ ì´ì˜ë”ë¼êµ¬ìš” ë‹¤ë¥¸ [MASK]ë„ ì‚¬ê³ ì‹¶ë„¤ìš” ì„¸ì¼ ë˜ ê¸°ë‹¤ë¦´ê²Œìš” [SEP]\"\n",
        "filled_text = fill_mask(input_text)\n",
        "print(\"Filled sentence:\", filled_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmCHerNBVflo"
      },
      "source": [
        "# ğŸ” 2. GPT, RAG, Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJXM4PpMfBB_"
      },
      "source": [
        "## 2-1 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "\n",
        "langchain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€, pdf ë¬¸ì„œë¥¼ ì½ì„ìˆ˜ ìˆëŠ” pypdf ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í• ê²Œìš”!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHh7iUqLyHQk",
        "outputId": "78ff08c4-ac19-468e-ccab-b565fc74590c"
      },
      "outputs": [],
      "source": [
        "# â¬‡ï¸ Colabì—ì„œ í•œ ë²ˆë§Œ ì‹¤í–‰\n",
        "%pip install -qU \"langchain>=0.3\" langchain-community langchain-openai langgraph chromadb pypdf tiktoken\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWTYIcdbe4VG"
      },
      "source": [
        "## 2-2. í‚¤ê°’ ì„¤ì •"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOjJzln6fLaT"
      },
      "source": [
        "## 2-3 pdf ë¬¸ì„œ ì„ íƒ (ë¡œë“œ)\n",
        "\n",
        "\n",
        "ì—¬ëŸ¬ë¶„ë“¤ì´ í™œìš©í•˜ê³  ì‹¶ì€ ì•„ë¬´ pdf ë¬¸ì„œë¥¼ ì„ íƒí•´ë³´ì„¸ìš”!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "FH8xlYI0zEiG",
        "outputId": "f5d69229-4880-4ccc-da68-20e8421ae602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¥ ì—…ë¡œë“œí•  PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-57ac878b-1124-46f3-9d2f-d03a2e49d139\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-57ac878b-1124-46f3-9d2f-d03a2e49d139\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving KBB_SCHOLAR_ë³„ì  í‰ê°€ ì¤‘ì‹¬ ëŒ€ì¤‘ìŒì•… í‰ë¡  ë°©ì‹ ë³´í¸í™”ì— ëŒ€í•œ ë¹„íŒì  ê²€í† .pdf to KBB_SCHOLAR_ë³„ì  í‰ê°€ ì¤‘ì‹¬ ëŒ€ì¤‘ìŒì•… í‰ë¡  ë°©ì‹ ë³´í¸í™”ì— ëŒ€í•œ ë¹„íŒì  ê²€í† .pdf\n",
            "âœ… ì €ì¥ ì™„ë£Œ (Colab íŒŒì¼ëª… â†’ ì›ë³¸ëª… ë§¤í•‘):\n",
            "  doc_1_KBB_SCHOLAR_ë³„ì _í‰ê°€_ì¤‘ì‹¬_ëŒ€ì¤‘ìŒì•…_í‰ë¡ _ë°©ì‹_ë³´í¸í™”ì—_ëŒ€í•œ_ë¹„íŒì _ê²€í† .pdf  <=  KBB_SCHOLAR_ë³„ì  í‰ê°€ ì¤‘ì‹¬ ëŒ€ì¤‘ìŒì•… í‰ë¡  ë°©ì‹ ë³´í¸í™”ì— ëŒ€í•œ ë¹„íŒì  ê²€í† .pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import os, re, unicodedata\n",
        "\n",
        "os.makedirs(\"/content/pdfs\", exist_ok=True)\n",
        "print(\"ğŸ“¥ ì—…ë¡œë“œí•  PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥).\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "name_map = {}\n",
        "for i, (orig_name, data) in enumerate(uploaded.items(), 1):\n",
        "    nf = unicodedata.normalize(\"NFC\", orig_name)\n",
        "    base, ext = os.path.splitext(nf)\n",
        "    # í•œê¸€/ì˜ë¬¸/ìˆ«ì/ë°‘ì¤„/ëŒ€ì‹œ/ì ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” _\n",
        "    safe_base = re.sub(r\"[^\\w\\u3131-\\u318E\\uAC00-\\uD7A3.\\-]+\", \"_\", base).strip(\"._'â€˜â€™â€œâ€\")\n",
        "    # íŒŒì¼ëª… ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ëƒ„ (ìµœëŒ€ ~140ì ì •ë„ë¡œ)\n",
        "    safe_base = safe_base[:120] if len(safe_base) > 120 else safe_base\n",
        "    new_name = f\"doc_{i}_{safe_base}{ext.lower()}\"\n",
        "    with open(os.path.join(\"/content/pdfs\", new_name), \"wb\") as f:\n",
        "        f.write(data)\n",
        "    name_map[new_name] = orig_name\n",
        "\n",
        "print(\"âœ… ì €ì¥ ì™„ë£Œ (Colab íŒŒì¼ëª… â†’ ì›ë³¸ëª… ë§¤í•‘):\")\n",
        "for k, v in name_map.items():\n",
        "    print(f\"  {k}  <=  {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt0o0nGvbS8o"
      },
      "source": [
        "## 2-4 ë¬¸ì„œ ë¶„í•  í›„ ë²¡í„° DBì— ì €ì¥\n",
        "ë¡œë”©í•œ ë¬¸ì„œë¥¼ ë¶„í•  í•  ë•Œ,\n",
        "- chunk_sizeëŠ” `900`\n",
        "- chunk_overlapì€ `150`\n",
        "- separators(êµ¬ë¶„ì)ëŠ” `[\"\\n\\n\", \"\\n\", \" \", \"\"]`\n",
        "ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì²­í‚¹í•´ì£¼ì„¸ìš”!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfApbza30fU1",
        "outputId": "30a1f352-c9b3-4cc3-ea18-500e0c92d478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì´ ë¬¸ì„œ ì¡°ê°(í˜ì´ì§€ ê¸°ì¤€): 41\n",
            "ì´ ì²­í¬ ìˆ˜: 69\n",
            "âœ… ì¸ë±ì‹± ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "import glob\n",
        "\n",
        "# 1) PDF â†’ Documents\n",
        "pdf_paths = sorted(glob.glob(\"/content/pdfs/*.pdf\"))\n",
        "docs = []\n",
        "for p in pdf_paths:\n",
        "    try:\n",
        "        loader = PyPDFLoader(p)\n",
        "        docs.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ ë¡œë”© ì‹¤íŒ¨: {p} -> {e}\")\n",
        "\n",
        "print(f\"ì´ ë¬¸ì„œ ì¡°ê°(í˜ì´ì§€ ê¸°ì¤€): {len(docs)}\")\n",
        "\n",
        "# 2) ë¬¸ì„œ ë¶„í• \n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size= 900, # ì²­í¬ ì‚¬ì´ì¦ˆ: ë¬¸ì„œë¥¼ ëª‡ ê°œì˜ í† í° ë‹¨ìœ„ë¡œ ë‚˜ëˆŒ ê²ƒì¸ì§€ë¥¼ ì •í•©ë‹ˆë‹¤.\n",
        "    chunk_overlap= 150, # ì²­í¬ ì˜¤ë²„ë©: ë¶„í• ëœ ë ë¶€ë¶„ì—ì„œ ë§¥ë½ì´ ì´ì–´ì§ˆ ìˆ˜ ìˆë„ë¡ ì¼ë¶€ë¥¼ ê²¹ì³ì„œ(overlap) ë¶„í• í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n",
        "    separators= [\"\\n\\n\", \"\\n\", \" \", \"\"], # êµ¬ë¶„ì: ì—”í„°, ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ”\n",
        ")\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"ì´ ì²­í¬ ìˆ˜: {len(chunks)}\")\n",
        "\n",
        "# 3) ë²¡í„°ìŠ¤í† ì–´ (ì˜ì† ë””ë ‰í† ë¦¬ ì“°ë©´ ëŸ°íƒ€ì„ ì¬ì‹œì‘í•´ë„ ìœ ì§€)\n",
        "persist_dir = \"/content/chroma_pdf_db\"\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # ë¹„ìš© ì ˆì•½í˜•\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=persist_dir\n",
        ")\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(\"âœ… ì¸ë±ì‹± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF2U8Ddmhyu5"
      },
      "source": [
        "## 2-5 RA<U>Generate</U> : ìƒì„± ë¶€ë¶„ ì„ ì–¸\n",
        "\n",
        "PROMPT ë¶€ë¶„ì€ ì—¬ëŸ¬ë¶„ë“¤ì´ ì›í•˜ì‹œëŠ”ëŒ€ë¡œ ì…ë ¥í•´ë„ ì¢‹ìŠµë‹ˆë‹¤!\n",
        "\n",
        "ğŸ¤¥ ì˜ì–´ë¡œ ì…ë ¥í•˜ë©´ ëª¨ë¸ì´ ë” ì˜ ì´í•´í•œëŒ€ìš”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qaycjB50fSu"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "PROMPT = ChatPromptTemplate.from_template(\n",
        "\"\"\"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸(ì—…ë¡œë“œí•œ PDFì—ì„œ ì¶”ì¶œë¨)ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µí•˜ë¼.\n",
        "ë¶ˆí™•ì‹¤í•˜ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ë¼. í•µì‹¬ ê·¼ê±°ë¥¼ ìš”ì•½í•˜ê³ , ê° ê·¼ê±° ì˜†ì— ì¶œì²˜(íŒŒì¼/í˜ì´ì§€)ë¥¼ í‘œì‹œí•˜ë¼.\n",
        "\n",
        "[ì§ˆë¬¸]\n",
        "{question}\n",
        "\n",
        "[ì»¨í…ìŠ¤íŠ¸]\n",
        "{context}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "rag_chain = (PROMPT | llm | StrOutputParser())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMP-y_6bjw5Y"
      },
      "source": [
        "## 2-6 RAG íŒŒì´í”„ë¼ì¸ langgraphë¡œ êµ¬ì„±\n",
        "\n",
        "ë²¡í„°dbì—ì„œ ê²€ìƒ‰í•˜ëŠ” ê²ƒì´ <U>Retrieve</U>AGì´ê³ ,\n",
        "\n",
        "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìƒì„± ë‹¨ê³„ì— ë˜ì ¸ì£¼ëŠ” ê²ƒì´ R<U>Augment</U>Gì…ë‹ˆë‹¤!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuCAAX4E0fOz",
        "outputId": "341d1a13-67a5-435c-fd39-3c36b09f6b55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… LangGraph ì»´íŒŒì¼ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "from typing import TypedDict, List\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# format_docsì˜ ì—­í• ì€ ë¬¸ì„œë¥¼ í¬ë§·íŒ…í•´ì„œ ëª¨ë¸ì—ê²Œ ì œê³µí•  'context'ë¥¼ ì •ë¦¬í•˜ëŠ” ì—­í• ì…ë‹ˆë‹¤!\n",
        "def format_docs(docs):\n",
        "    # íŒŒì¼ëª…/í˜ì´ì§€ë¥¼ ë©”íƒ€ë°ì´í„°ì— ë‹´ì•„ë‘ëŠ” PyPDFLoader ê¸°ë³¸ê°’ ì‚¬ìš©\n",
        "    lines = []\n",
        "    for i, d in enumerate(docs, 1):\n",
        "        src = d.metadata.get(\"source\", \"\")\n",
        "        page = d.metadata.get(\"page\", None)\n",
        "        tag = f\"{os.path.basename(src)}\"\n",
        "        if page is not None:\n",
        "            tag += f\" p.{page+1}\"\n",
        "        lines.append(f\"[{i}] {tag}\\n{d.page_content}\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "# ê·¸ë˜í”„ ì„ ì–¸\n",
        "class QAState(TypedDict):\n",
        "    question: str\n",
        "    retrieved: List[Document]\n",
        "    answer: str\n",
        "\n",
        "def node_retrieve(state: QAState):\n",
        "    q = state[\"question\"]\n",
        "    topk = retriever.invoke(q)\n",
        "    return {\"retrieved\": topk}\n",
        "\n",
        "def node_generate(state: QAState):\n",
        "    q = state[\"question\"]\n",
        "    ctx = format_docs(state[\"retrieved\"])\n",
        "    ans = rag_chain.invoke({\"question\": q, \"context\": ctx})\n",
        "    return {\"answer\": ans}\n",
        "\n",
        "# --------- ê·¸ë˜í”„ êµ¬ì„± ---------\n",
        "# ë…¸ë“œë¼ë¦¬ ì—°ê²°í•˜ê¸°\n",
        "workflow = StateGraph(QAState)\n",
        "workflow.add_node(\"retrieve\", node_retrieve) # ì–´ë–¤ ë…¸ë“œê°€ ì¶”ê°€ë˜ì–´ì•¼ í• ê¹Œìš”?\n",
        "workflow.add_node(\"generate\", node_generate) # ì–´ë–¤ ë…¸ë“œê°€ ì¶”ê°€ë˜ì–´ì•¼ í• ê¹Œìš”?\n",
        "\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"generate\") # ì–´ë–¤ ë…¸ë“œì™€ ì–´ë–¤ ë…¸ë“œê°€ ì—°ê²°ë˜ì–´ì•¼ í• ê¹Œìš”?\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "app = workflow.compile()\n",
        "print(\"âœ… LangGraph ì»´íŒŒì¼ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNAKcKePchhL"
      },
      "source": [
        "## 2-7 ì‹¤ì œ êµ¬ë™í•´ë³´ê¸°\n",
        "\n",
        "questionì— ì—¬ëŸ¬ë¶„ë“¤ì´ ì…ë ¥í•˜ê³  ì‹¶ì€ ì§ˆë¬¸ì„ ë„£ì–´ë³´ì„¸ìš”!\n",
        "\n",
        "RAG ê³¼ì œëŠ” ì´ë ‡ê²Œ ê°„ë‹¨í•˜ê²Œ ëë‚´ê² ìŠµë‹ˆë‹¤ğŸ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC-HXC7j0wHs",
        "outputId": "dd741206-b726-4f71-b758-fa2212d72cb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â–¼ ë‹µë³€\n",
            "ì´ ë…¼ë¬¸ì˜ ì„œë¡  ë° ì„ í–‰ì—°êµ¬ ë¶€ë¶„ì„ ë‹¤ìŒê³¼ ê°™ì´ ë²”ì£¼í™”í•˜ì—¬ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "1. **í‰ë¡ ì˜ ê¶Œìœ„ì™€ ë³€í™”**\n",
            "   - í‰ë¡ ì˜ ê¶Œìœ„ì™€ ì˜í–¥ë ¥ì´ ì•½í™”ë˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ëŒ€ì¤‘ìŒì•…ë¿ë§Œ ì•„ë‹ˆë¼ ë¬¸í•™, ì˜í™”, ë¯¸ìˆ  ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë‚˜íƒ€ë‚˜ê³  ìˆë‹¤. (ì¶œì²˜: [1] p.3)\n",
            "\n",
            "2. **í‰ë¡ ì˜ ìœ„ê¸°ì™€ ì›ì¸**\n",
            "   - í‰ë¡ ì˜ ìœ„ê¸°ëŠ” í‰ë‹¨ì˜ íì‡„ì  ì„±ê²©, ë…¼ìŸì  ì„±ê²©ì˜ ìƒì‹¤, ê³µì  ì¥ì˜ ë¶€ì¬ ë“± ì—¬ëŸ¬ ì›ì¸ìœ¼ë¡œ ë¶„ì„ë˜ê³  ìˆë‹¤. (ì¶œì²˜: [1] p.3)\n",
            "\n",
            "3. **ë³„ì  í‰ê°€ì˜ ë¹„íŒì  ê³ ì°°**\n",
            "   - ë³¸ ë…¼ë¬¸ì€ ë³„ì  í‰ê°€ ë°©ì‹ì´ ëŒ€ì¤‘ìŒì•… í‰ë¡ ì—ì„œ ë³´í¸í™”ëœ í˜„ìƒì„ ë¹„íŒì ìœ¼ë¡œ ì‚´í´ë³´ë©°, ì´ë¥¼ í†µí•´ í˜„ëŒ€ ì‚¬íšŒì˜ ì‚¬ê³ ë°©ì‹ê³¼ ê°€ì¹˜ì²´ê³„ì™€ ì—°ê²°ì§€ì–´ ë…¼ì˜í•˜ê³ ì í•œë‹¤. (ì¶œì²˜: [3] p.2)\n",
            "\n",
            "4. **í‰ë¡ ì˜ ì—­ì‚¬ì  ë³€í™”**\n",
            "   - í‰ë¡ ì˜ ì—­ì‚¬ì  ë³€í™”ì™€ í˜„ì¬ì˜ ë³€í™”ê°€ ì§€ë‹Œ ì˜ë¯¸ë¥¼ ì¶”ì í•˜ë©°, í‰ë¡ ì´ ì–´ë–»ê²Œ ì „ë¬¸ì„±ì„ ì¸ì •ë°›ê³  ì¡´ì¬ ê°€ì¹˜ë¥¼ ì¦ëª…í•´ ì™”ëŠ”ì§€ë¥¼ ì‚´í´ë³¸ë‹¤. (ì¶œì²˜: [3] p.2)\n",
            "\n",
            "5. **ë¯¸ë””ì–´ í™˜ê²½ì˜ ë³€í™”**\n",
            "   - 2000ë…„ëŒ€ ì´í›„ ë¯¸ë””ì–´ í™˜ê²½ì˜ ë³€í™”ê°€ í‰ë¡ ì˜ í˜•ì‹ê³¼ ë‚´ìš©ì— ë¯¸ì¹œ ì˜í–¥ì„ ë¶„ì„í•˜ê³ , ë³„ì  í‰ê°€ì˜ ë¶€ìƒê³¼ ê·¸ë¡œ ì¸í•œ ëŒ€ì¤‘ìŒì•… í‰ë¡ ì˜ ê²½í–¥ ë³€í™”ë¥¼ ë…¼ì˜í•œë‹¤. (ì¶œì²˜: [5] p.4)\n",
            "\n",
            "ì´ì™€ ê°™ì€ ë²”ì£¼í™”ëŠ” ë…¼ë¬¸ì˜ ì„œë¡  ë° ì„ í–‰ì—°êµ¬ ë¶€ë¶„ì—ì„œ ë‹¤ë£¨ê³  ìˆëŠ” ì£¼ìš” ì£¼ì œì™€ ë…¼ì ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "question = \"ì´ ë…¼ë¬¸ì˜ ì„œë¡  ë° ì„ í–‰ì—°êµ¬ ë¶€ë¶„ì„ ë²”ì£¼í™”í•˜ì—¬ ì •ë¦¬í•´ì¤˜.\"\n",
        "final = app.invoke({\"question\": question})\n",
        "print(\"â–¼ ë‹µë³€\")\n",
        "print(final[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWRhK7jv0y81",
        "outputId": "9d9a0f68-fa8f-41a6-e9a8-9c95dbe1054c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â–¼ ë‹µë³€\n",
            "ì´ ë…¼ë¬¸ì€ \"ë³„ì  í‰ê°€ ì¤‘ì‹¬ ëŒ€ì¤‘ìŒì•… í‰ë¡  ë°©ì‹ ë³´í¸í™”ì— ëŒ€í•œ ë¹„íŒì  ê²€í† \"ë¼ëŠ” ì œëª©ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ëŒ€ì¤‘ìŒì•… í‰ë¡ ì—ì„œ ë³„ì  í‰ê°€ ë°©ì‹ì˜ ë³´í¸í™”ì— ëŒ€í•´ ë¹„íŒì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ìˆë‹¤. ì£¼ìš” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
            "\n",
            "1. **í‰ë¡ ì˜ ë³€í™”**: í‰ë¡ ì˜ ê¶Œìœ„ì™€ ì˜í–¥ë ¥ì´ ê°ì†Œí•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ëŒ€ì¤‘ìŒì•…ë¿ë§Œ ì•„ë‹ˆë¼ ë¬¸í•™, ì˜í™”, ë¯¸ìˆ  ë“± ë‹¤ì–‘í•œ ì˜ˆìˆ  ë¶„ì•¼ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” í˜„ìƒì´ë‹¤. ì´ëŸ¬í•œ ë³€í™”ëŠ” ë¯¸ë””ì–´ í™˜ê²½ì˜ ë³€í™”ì™€ ì˜ˆìˆ ì— ëŒ€í•œ ì‚¬íšŒë¬¸í™”ì  ì‹œì„ ì˜ ë³€í™”ì— ê¸°ì¸í•œë‹¤. (ì¶œì²˜: [5] p.3)\n",
            "\n",
            "2. **ë³„ì  í‰ê°€ì˜ ë¬¸ì œì **: ë³„ì  í‰ê°€ ë°©ì‹ì€ ëŒ€ì¤‘ìŒì•… í‰ë¡ ì—ì„œ ì¼ë°˜í™”ë˜ê³  ìˆì§€ë§Œ, ì´ëŠ” í‰ê°€ì˜ ì§ˆì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, ìˆ˜ì¹˜ë¡œ í™˜ì‚°ëœ í‰ê°€ê°€ ì§ˆì  í‰ê°€ë³´ë‹¤ ë” ì‹ ë¢°ë°›ëŠ” í˜„ëŒ€ ì‚¬íšŒì˜ ì‚¬ê³  ë°©ì‹ê³¼ ì—°ê²°ë˜ì–´ ìˆë‹¤. (ì¶œì²˜: [5] p.3)\n",
            "\n",
            "3. **ëŒ€ì¤‘ìŒì•… í‰ë¡ ì˜ ì—­ì‚¬ì  ê´€ì **: ëŒ€ì¤‘ìŒì•… í‰ë¡ ì˜ ì—­ì‚¬ì  ë³€í™”ì™€ ìˆ˜ì‚¬ì˜ ë¯¸í•™ì„ ì‚´í´ë³´ë©°, ë³„ì  í‰ê°€ê°€ ì–´ë–»ê²Œ ëŒ€ì¤‘ìŒì•… í‰ë¡ ì— ìë¦¬ ì¡ê²Œ ë˜ì—ˆëŠ”ì§€ë¥¼ ë¶„ì„í•œë‹¤. (ì¶œì²˜: [1] p.1)\n",
            "\n",
            "4. **ì¼ë°˜ ìˆ˜ìš©ìì™€ ì „ë¬¸ í‰ë¡ ê°€ì˜ í‰ê°€ ì°¨ì´**: ì¼ë°˜ ìˆ˜ìš©ìì™€ ì „ë¬¸ í‰ë¡ ê°€ ê°„ì˜ í‰ê°€ ë°©ì‹ê³¼ ê·¸ ê²°ê³¼ì˜ ì°¨ì´ë¥¼ ë…¼ì˜í•˜ë©°, í‰ê°€ì˜ ì£¼ê´€ì„±ê³¼ ê°ê´€ì„± ê°„ì˜ ê°ˆë“±ì„ ë‹¤ë£¬ë‹¤. (ì¶œì²˜: [4] p.9)\n",
            "\n",
            "5. **ê²°ë¡ **: ë…¼ë¬¸ì€ ë³„ì  í‰ê°€ ë°©ì‹ì´ ëŒ€ì¤‘ìŒì•… í‰ë¡ ì˜ ì§ˆì  ì €í•˜ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŒì„ ê²½ê³ í•˜ë©°, ìƒˆë¡œìš´ í‰ë¡ ì˜ ê°€ëŠ¥ì„±ì„ ëª¨ìƒ‰í•  í•„ìš”ì„±ì„ ê°•ì¡°í•œë‹¤. (ì¶œì²˜: [5] p.3)\n",
            "\n",
            "ì´ ë…¼ë¬¸ì€ ëŒ€ì¤‘ìŒì•… í‰ë¡ ì˜ í˜„ì¬ì™€ ë¯¸ë˜ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ë…¼ì˜ë¥¼ ì œê³µí•˜ë©°, ë³„ì  í‰ê°€ ë°©ì‹ì˜ ë¹„íŒì  ê²€í† ë¥¼ í†µí•´ í‰ë¡ ì˜ ìƒˆë¡œìš´ ë°©í–¥ì„±ì„ ì œì‹œí•˜ê³ ì í•œë‹¤.\n"
          ]
        }
      ],
      "source": [
        "question = \"ì´ ë…¼ë¬¸ì— ëŒ€í•´ì„œ ìš”ì•½í•´ì¤˜\"\n",
        "final = app.invoke({\"question\": question})\n",
        "print(\"â–¼ ë‹µë³€\")\n",
        "print(final[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFIJftFPpXb9",
        "outputId": "781abf26-6c74-4634-b1fd-1ca14d7cce6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â–¼ ë‹µë³€\n",
            "ì´ ë…¼ë¬¸ì—ì„œëŠ” ê¸°ì„± ëŒ€ì¤‘ìŒì•… í‰ë¡ ê³¼ ê°œì¸ ë¦¬ìŠ¤ë„ˆ í‰ë¡ ì˜ ì°¨ì´ì ê³¼ ê·¸ë¡œ ì¸í•œ ë¬¸ì œì ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì„± ëŒ€ì¤‘ìŒì•… í‰ë¡ ì€ ì „ë¬¸ê°€ì˜ ì£¼ê´€ì  í‰ê°€ê°€ ê°œì…ë˜ë©°, ì¼ë°˜ ìˆ˜ìš©ìì™€ì˜ í‰ê°€ ì°¨ì´ê°€ ì¡´ì¬í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´, ê°œì¸ ë¦¬ìŠ¤ë„ˆ í‰ë¡ ì€ ì†Œì…œ ë¯¸ë””ì–´ í”Œë«í¼ì„ í†µí•´ ë” ë§ì€ ì‚¬ëŒë“¤ì˜ ì˜ê²¬ì„ ë°˜ì˜í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "1. **ê¸°ì„± ëŒ€ì¤‘ìŒì•… í‰ë¡ ì˜ í•œê³„**: ì „ë¬¸ê°€ í‰ë¡ ê°€ëŠ” ì¼ë°˜ ìˆ˜ìš©ìë³´ë‹¤ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ìŒì•…ì„ ë¶„ì„í•˜ì§€ë§Œ, ì£¼ê´€ì´ ê°œì…ë  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©°, ê²½ì œì  ì„±ê³¼ë¥¼ ì¶©ë¶„íˆ ì„¤ëª…í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì „ë¬¸ê°€ í‰ë¡ ê³¼ ì¼ë°˜ ìˆ˜ìš©ì ê°„ì˜ ì ìˆ˜ ì°¨ì´ê°€ ë°œìƒí•©ë‹ˆë‹¤. (ì¶œì²˜: [4] p.29)\n",
            "\n",
            "2. **ê°œì¸ ë¦¬ìŠ¤ë„ˆ í‰ë¡ ì˜ ì¥ì **: ê°œì¸ ë¦¬ìŠ¤ë„ˆë“¤ì€ ì†Œì…œ ë¯¸ë””ì–´ë¥¼ í†µí•´ ìì‹ ì˜ ì˜ê²¬ì„ ì‰½ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ë‹¤ì–‘í•œ ì˜ê²¬ì„ ìˆ˜ë ´í•˜ëŠ” ë° ìœ ë¦¬í•©ë‹ˆë‹¤. íŠ¹íˆ ì¸ìŠ¤íƒ€ê·¸ë¨ íë ˆì´íŒ… ê³„ì •ì´ë‚˜ RYMê³¼ ê°™ì€ í”Œë«í¼ì€ ê°œì¸ì˜ ì£¼ê´€ì ì¸ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ í•œ í‰ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. (ì¶œì²˜: [5] p.4)\n",
            "\n",
            "3. **ì‹ ë¢°ì„± ë¬¸ì œ**: ê¸°ì„± í‰ë¡ ê°€ì˜ ì‹ ë¢°ì„±ì´ ì˜ì‹¬ë°›ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë©°, ì´ëŠ” ê°œì¸ ë¦¬ìŠ¤ë„ˆë“¤ì´ ë” ë§ì€ ì‹ ë¢°ë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì¡°ì„±í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê²Œì„ í‰ë¡ ê°€ì— ëŒ€í•œ ì‹ ë¢°ê°€ í•˜ë½í•œ ì‚¬ë¡€ê°€ ì–¸ê¸‰ë˜ë©°, ì´ëŠ” ìŒì•… í‰ë¡ ì—ë„ ìœ ì‚¬í•œ ê²½í–¥ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. (ì¶œì²˜: [4] p.29)\n",
            "\n",
            "ê²°ë¡ ì ìœ¼ë¡œ, ê¸°ì„± ëŒ€ì¤‘ìŒì•… í‰ë¡ ì€ ì „ë¬¸ê°€ì˜ ì£¼ê´€ì  í‰ê°€ê°€ ê°œì…ë˜ì–´ ì‹ ë¢°ì„±ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, ê°œì¸ ë¦¬ìŠ¤ë„ˆ í‰ë¡ ì€ ë” ë‹¤ì–‘í•œ ì˜ê²¬ì„ ë°˜ì˜í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê°œì¸ í‰ë¡ ì˜ ì‹ ë¢°ì„± ë˜í•œ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "question = \"ì´ ë…¼ë¬¸ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê¸°ì„± ëŒ€ì¤‘ìŒì•… í‰ë¡ ê³¼, ìµœê·¼ ë“œëŸ¬ë‚˜ëŠ” ê°œì¸ ë¦¬ìŠ¤ë„ˆ í‰ë¡ (ì˜ˆ: ì¸ìŠ¤íƒ€ê·¸ë¨ íë ˆì´íŒ… ê³„ì •, RYM)ì— ëŒ€í•œ ë„ˆì˜ ê´€ì ì„ ì œì‹œí•´ì¤˜.\"\n",
        "final = app.invoke({\"question\": question})\n",
        "print(\"â–¼ ë‹µë³€\")\n",
        "print(final[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCgJ30gFVuQR"
      },
      "source": [
        "# ğŸ” 3. sLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfsDR_omdNea"
      },
      "source": [
        "## 3-1. sLM(Gemma) ë¡œë“œ\n",
        "### sLM(small Language Model)ì˜ ëŒ€í‘œ ëª¨ë¸ì¸ Gemmaë¥¼ ë¡œë“œ í•´ë´…ì‹œë‹¤!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXFZFUJHgTcU"
      },
      "source": [
        "### (1) ì‚¬ì „ ì¤€ë¹„ ì‚¬í•­\n",
        "### 1. HuggingFace ë¡œê·¸ì¸\n",
        "\n",
        "https://huggingface.co/\n",
        "\n",
        "ì´ ë§í¬ë¡œ ë“¤ì–´ê°€ì„œ HuggingFaceì— ë¡œê·¸ì¸í•©ë‹ˆë‹¤.(ê³„ì •ì´ ì—†ë‹¤ë©´ íšŒì›ê°€ì… í•´ì£¼ì„¸ìš”!)\n",
        "\n",
        "### 2. í† í° ë°œê¸‰ë°›ê¸°\n",
        "\n",
        "https://huggingface.co/settings/tokens\n",
        "\n",
        "- ì´ ë§í¬ë¡œ ë“¤ì–´ê°€ì„œ 'Create new token'ì„ í´ë¦­\n",
        "- Token type: 'Read'ë¡œ í•´ì£¼ì„¸ìš”.\n",
        "- Token name: ì•„ë¬´ê±°ë‚˜ ìƒê´€ ì—†ìŠµë‹ˆë‹¤.\n",
        "- (â­ ë§¤ìš° ì¤‘ìš”!) í† í° ë§Œë“œì‹œê³  **ë°˜ë“œì‹œ í† í° ë¬¸ìì—´ (hf_... ë¡œ ì‹œì‘í•¨)ì„ ë³µì‚¬í•´ì£¼ì„¸ìš”!**  \n",
        "í•œ ë²ˆë§Œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
        "\n",
        "### 3. google colabì—ì„œ í† í° ë“±ë¡í•˜ê¸°\n",
        "- ì™¼ìª½ì— ì—´ì‡  ëª¨ì–‘ ë²„íŠ¼(ë³´ì•ˆ ë¹„ë°€)ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.\n",
        "- ì´ë¦„: colab-gemma\n",
        "- ê°’: ë³µì‚¬í•´ë‘” í† í° ë¬¸ìì—´(hf_...ë¡œ ì‹œì‘í•¨)\n",
        "- ë…¸íŠ¸ë¶ ì•¡ì„¸ìŠ¤ : ìŠ¤ìœ„ì¹˜ í‚¤ê¸°(íŒŒë€ìƒ‰)\n",
        "\n",
        "### 4. gemma-2b-it ëª¨ë¸ ì‚¬ìš© ì•½ê´€ ë™ì˜\n",
        "\n",
        "https://huggingface.co/google/gemma-2b-it\n",
        "\n",
        "- (â­ ë§¤ìš° ì¤‘ìš”!) ë°˜ë“œì‹œ 2ë²ˆ ë‹¨ê³„ì—ì„œ í† í°ì„ ë°œê¸‰ë°›ì•˜ë˜ ë°”ë¡œ ê·¸ Hugging Face ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸ëœ ìƒíƒœì—ì„œ ìœ„ ë§í¬ë¡œ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤.\n",
        "- ë§í¬ë¡œ ë“¤ì–´ê°€ì„œ 'Access Gemma on Hugging Face' ë¶€ë¶„ì˜ ì•½ê´€ì„ ì½ê³  ë™ì˜ ë²„íŠ¼ì„ í´ë¦­í•´ ì£¼ì„¸ìš”.\n",
        "- \"You have been granted access to this model\"ì´ë¼ëŠ” ë©”ì‹œì§€ê°€ ëœ¨ë©´ ì„±ê³µì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwjo5_Uucxkw"
      },
      "source": [
        "### (2) í•„ìš”í•œ íˆ´ ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_nXPEsF7UWQ",
        "outputId": "c6526151-f65b-4b76-e1ec-8891898c26c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install --upgrade -q transformers huggingface_hub peft \\\n",
        "  accelerate bitsandbytes datasets trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00df732f"
      },
      "outputs": [],
      "source": [
        "# HuggingFaceì— ë¡œê·¸ì¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# ë”°ìŒí‘œ ì•ˆì— ë³¸ì¸ì˜ ë³´ì•ˆ ë¹„ë°€ í‚¤ì˜ ì´ë¦„ì„ ì ìŠµë‹ˆë‹¤.(ì—¬ê¸°ì„  colab-gemma)\n",
        "try:\n",
        "    login(token=userdata.get(\"colab-gemma\"))\n",
        "except Exception as e:\n",
        "    print(f\"Error logging in to Hugging Face: {e}\")\n",
        "    print(\"Please make sure you have added your Hugging Face token to Colab secrets with the name 'colab-gemma'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501,
          "referenced_widgets": [
            "10f8c9e185f84a92a55a88029804e1bf",
            "5810a89706fe472fa9544b9d84495350",
            "8025ebee3947462b8d863272c7a6a9f9",
            "8e63306cbeee41f2ae80fc0b967437f7",
            "b60c133f3d344f75ad8a12daef25ffb2",
            "5386ebf457ca4bd088ae667fd3a2cc55",
            "94c4c66cbe6f42c59a64047f37f9b670",
            "464fb832925c4336bc3da24f04248dad",
            "99c33cc18559416eb8f7818a709197de",
            "b5327042a5ce4c6d8aee352ffe74e674",
            "3f409bcfef5443fcb11f2493f2a399ab",
            "142197f7693e4675a84197b252bbaee9",
            "e5c83a05e49e43cbaa43a7e5d83915ae",
            "62d7b958c8724bce88d74000b35c3a03",
            "7dc9501981654e7d908c1a7dcb3f97dc",
            "a5786d115d674de6a3ef1a35e9b5af6a",
            "5e05859b7e774bb186244797a4dc1132",
            "552cf3396f254e56ab77b7b4281b40a4",
            "9b37ec161bb945cdab494c7d6f9dab96",
            "1a72f55980644a988ce1e0a541a6d421",
            "c643f87aef79424ebcdea10f3825ecfc",
            "91236130df534840b9f8a51f169e6c71",
            "8085d64458e24273aecf8d5653703196",
            "4eb80afa6c964acc86b8e637dc1b2c9b",
            "6fbb9566cadf44a88058ea5bacc95ca2",
            "035747b97c164220ae2695ad89d0e2ce",
            "abc6b40be4804a6794bdf65a2e2098d2",
            "715709f951b44e1ab4a0b4c160389fe8",
            "d1dc12560c3a4a8797b3f60a04197138",
            "958d612ee6cd48d191656c3e59b86c6d",
            "13363b142f81412286f68f204721b1cc",
            "81eb2d6b6e684217a3020d84fda04fed",
            "e89bc1a3def645148205f857a3b2ac23",
            "0fec9e40f95d4dd0ad0a1cac834d8311",
            "df7511d5c9e848c2b2ea42a94f13b92c",
            "b5a07c2a676d44f286891c2f505af726",
            "badea16d41194fb9b40c0fa86930bbfa",
            "7a344eb85d0c41728f6c57f693e5254f",
            "5b37a178d4f54e689554a5cbb7bc4835",
            "ee77c9e8584a4cf6b0acc1f292834b80",
            "1358800b871e4cb998226216d343464d",
            "de77a4d62ab7494d9f7c3e3bd628f007",
            "9ba9fd52ae44480cbaa654c16d2003c3",
            "03c9cbf12c53490cb9c82b0b0069b484",
            "237614730668476bb0338016ad672b92",
            "8c043e0582dc4ce385bb7433238c2e77",
            "936975f09b814cc687b6b6228426f7e5",
            "6c1518332b824ab39cbbc0ddd171ef2f",
            "3d0e3e0f9167487693e373798fa3ce85",
            "06a307cdefcd4bfd8e2bc6cf9db79397",
            "a017b8750eee49e184f244cf5249a8dd",
            "3d508fe5023f4de7bade6e4dcf68bc80",
            "8a57ec02f47c4cb4b6483e41c23bc810",
            "c885824d213b42d09fd3733009cd0a02",
            "f705895efec243eab32e696c44ffa811",
            "3374a089f0c847caa8992787d5ae319d",
            "557e94711472458d9746bbcf24b559b8",
            "2af29d19155541bbbf05e0a643ba0b4a",
            "2af2f7386d9541acac6bd5706b00ad9d",
            "808b51f936474f9b9ac08dadfa9cb188",
            "e9dc0ef88480428a8bb38909571261be",
            "884414bb984349e283afe4c6c25a4487",
            "5b17651323b540c182dc16e76fca8608",
            "f0ccd5230fa847c78131855d0913bbe2",
            "56c4550240bd4736880482e7eb36587b",
            "a6bdc26307a84e69ac634ad310d2f5d0",
            "70ff2e63ba654fbbba9bd5d8b45d8728",
            "e67ee9398e3c4481b5ef5defa594af3d",
            "2e5b2f1523db44fcb64314cce5ee379d",
            "9679d5869e63406fbd6eed7ab820f61d",
            "9f831e96d85749fdb9d30eef9ef14ef4",
            "d6dc48bf49674cc18be7ec07460e222e",
            "7c3185ee1e9f452e8104f35b2be34d8b",
            "2a1235bfde814c5f85ee22659b7d4f39",
            "2a461bc90a5d47c99c7aa20abd6d340e",
            "ccfab65d96764f90900cd5be257e8596",
            "f6e7edf724f949adac84b4fc605af352",
            "697550036d9d46fe940af85d7574f42d",
            "d5588fd73dce4280814d657771f8332d",
            "90b5e557bc404a6d8dc873e1f8035b57",
            "72923a14951d47fba8faf6aa0b193587",
            "ff79b20b527849f69694e281d95e4569",
            "2be6a7c708284d8b83e4557c4f85a213",
            "5021558b7b1c4207807ddbb7747ac708",
            "d20cd79e199546659ce9d83ffa9f37be",
            "716e01804f234afba166ff95ad0690a3",
            "83a2e43e4d2c451e8942c4414d82250c",
            "112f34400f9f4f0785f35c3903ce1251",
            "b917223e0f1e460fa5245392f89e0e47",
            "2932af6cfd9c4f5ab2324518459ef237",
            "ab131fd2c560487c8bdb3b37bf436b8a",
            "148e95148587411d85e0ef6de7171a48",
            "88fb7eeb84b64bb69a942237487239f2",
            "fb0f566998ce49dfbce120a97d2d2b6e",
            "86a35e3a7f1640fe8fcd91653fcd39ff",
            "e91439a8239742e58eacd122d8e3fbc8",
            "a272aa80498447b289f878828e1425d2",
            "8062ff93f33f46a8aa57d8c2269ba182",
            "5f6fb541e1b94448b174bcba93eba31c",
            "a3322ab01e4f4a698522c2b2d853c8fe",
            "eab3faa7711c41efbba91ac42f2ac324",
            "311cc76469904ef797b04967a944a863",
            "8e2f422cc0344822abec5e1763ee3040",
            "3629ca85741143938a3b10378b5199b7",
            "493703f233024b0f9655ce53b4e28b02",
            "b2e942a78cfc4eaf92e4be31a77fd652",
            "b3c7bd579eb8458dbe16d80933bd30e2",
            "b3b463f337b44643ba9225c853f889c6",
            "c17983c808ac4ff3ba188770726646fe",
            "cfe48776d7144ebd8b3b50395be3074f",
            "672c28cc7d074354a6493c8664335dbf",
            "1b86d78b476747c990a8755f60a02e7c",
            "47ceefe813f645bd8916f10fb5e93b5e",
            "3832171a9c81489691f38c6e88ff8ae9",
            "aeb62df6c4934aa38338d5d45d9a6842",
            "691c0dea1c7144a99ceb4fb8a132894b",
            "9ebfa3410c6e43419f48e29dafcb366f",
            "2a03209e4635417aa8968b67dfd65f6d",
            "8f6eeff0eee449c5b737931837caedbb",
            "4e32cb8b3bb145b0a523da971b96f1cb",
            "f1e8605aee754d089c398a179c71dd4a"
          ]
        },
        "id": "w_z4600bwvSq",
        "outputId": "65218048-e0a9-4358-9297-222233b035cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10f8c9e185f84a92a55a88029804e1bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "142197f7693e4675a84197b252bbaee9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8085d64458e24273aecf8d5653703196",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fec9e40f95d4dd0ad0a1cac834d8311",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "237614730668476bb0338016ad672b92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3374a089f0c847caa8992787d5ae319d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70ff2e63ba654fbbba9bd5d8b45d8728",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "697550036d9d46fe940af85d7574f42d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b917223e0f1e460fa5245392f89e0e47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3322ab01e4f4a698522c2b2d853c8fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "672c28cc7d074354a6493c8664335dbf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# ê³„ì‚° ë¶€í•˜ë¥¼ ì¤„ì´ê³  ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ 'ì–‘ìí™”'í•©ë‹ˆë‹¤.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# sLM(gemma)ë¥¼ Q&A, ìš”ì•½ ë“±ì— í™œìš©í•˜ê¸° ìœ„í•´ 'ì§€ì‹œ íŠœë‹(Instruction-Tuned) ë²„ì „ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "# \"google/gemma-2b-it\" ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "# í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config, # ì–‘ìí™”(quantization)\n",
        "    device_map={\"\":\"cuda:0\"} # Changed from \"0\" to \"cuda:0\"\n",
        ")\n",
        "\n",
        "# GPUë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlmGvFMg4a2q"
      },
      "source": [
        "## 3-2. sLM í™œìš©\n",
        "- sLMì€ LLMì„ ê²½ëŸ‰í™”í•œ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ì–¸ì–´ ëª¨ë¸ì˜ ê¸°ëŠ¥ì¸ ì§ˆì˜ì‘ë‹µ(ì¶”ë¡ ), ìš”ì•½, ë¶„ë¥˜(ê°ì •ë¶„ì„) ë“±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
        "- **ì†ë„ëŠ” ë¹„êµì  ë¹ ë¥´ì§€ë§Œ, ì„±ëŠ¥ì´ ê·¸ë‹¥ ì¢‹ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**\n",
        "\n",
        "### Gemma í™œìš© ì˜ˆì‹œ (Common Use Cases)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VylD-KiV2ow"
      },
      "source": [
        "### (1) ì§ˆì˜ì‘ë‹µ(Reasoning)\n",
        "\n",
        "#### ì›í•˜ëŠ” ì§ˆë¬¸ì„ í•œ ë²ˆ ë„£ì–´ë³´ì„¸ìš”!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGU1uuui4a2q",
        "outputId": "4ef3c479-1e6f-4538-fecb-a0a77aea60c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- ì˜ˆì‹œ 1: ì§ˆì˜ì‘ë‹µ (Reasoning) ---\n",
            "**AR(Autoregressive) ëª¨ë¸**\n",
            "\n",
            "AR ëª¨ë¸ì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ê³¼ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. AR ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ í†µí•´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
            "\n",
            "1. **ì…ë ¥ ë°ì´í„°**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³¼ê±° ë°ì´í„°ë¥¼ ì¶”ì¸¡í•©ë‹ˆë‹¤.\n",
            "2. **ì˜ˆì¸¡ëœ ê°’**ì„ í˜„ì¬ ë°ì´í„°ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
            "3. **ì˜¤ì°¨**ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³¼ê±° ë°ì´í„°ì™€ í˜„ì¬ ë°ì´í„°ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•©ë‹ˆë‹¤.\n",
            "4. **ì˜¤ì°¨ë¥¼ ìµœì†Œí™”**í•˜ëŠ” ê³¼ì •ì„ í†µí•´ ëª¨ë¸ì˜ ë§¤ê°œìˆ˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.\n",
            "\n",
            "AR ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "* **ê³ í’ˆì§ˆ ì˜ˆì¸¡:** AR ëª¨ë¸ì€ ê³¼ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì˜ˆì¸¡ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
            "* **ìê¸°íšŒê·€ì„±:** AR ëª¨ë¸ì€ ê³¼ê±° ë°ì´í„°ì™€ í˜„ì¬ ë°ì´í„°ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ê¸° ë•Œë¬¸ì—, ìê¸°ì ìœ¼ë¡œ ì˜ˆì¸¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "* **ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹:** AR ëª¨ë¸ì€ ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
            "\n",
            "**MA(Moving Average) ëª¨ë¸**\n",
            "\n",
            "MA ëª¨ë¸ì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ í‰ê· ì ìœ¼ë¡œ ì¶”ì¶”í•˜ëŠ”\n"
          ]
        }
      ],
      "source": [
        "print(\"--- ì˜ˆì‹œ 1: ì§ˆì˜ì‘ë‹µ (Reasoning) ---\")\n",
        "\n",
        "# content : '~~' ë”°ìŒí‘œ ì•ˆì— ì§ˆë¬¸ì„ ì¨ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "# í•˜ê³  ì‹¶ì€ ì§ˆë¬¸ ì•„ë¬´ê±°ë‚˜ ë„£ì–´ë³´ì„¸ìš”!\n",
        "\n",
        "# \"role\": \"user\" ì…ë‹ˆë‹¤.\n",
        "\n",
        "chat = [\n",
        "    # Gemma ëª¨ë¸ì˜ ì±„íŒ… í˜•ì‹ì— ë§ì¶° 'role'ê³¼ 'content'ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "    { \"role\": \"user\", \"content\": \"ì‹œê³„ì—´ ë¶„ì„ì˜ AR, MAì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\" }\n",
        "]\n",
        "\n",
        "# 1. ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸('chat')ë¥¼ Gemma ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ê³µì‹ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# 2. ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ëª¨ë¸ì´ ì…ë ¥ë°›ì„ ìˆ˜ ìˆëŠ” í† í° ID í…ì„œë¡œ ë³€í™˜í•˜ê³ , GPU('device')ë¡œ ë³´ëƒ…ë‹ˆë‹¤.\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 3. í† í°í™”ëœ ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ í…ìŠ¤íŠ¸ \"\"ìƒì„±\"\"ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ìµœëŒ€ 256ê°œì˜ ìƒˆ í† í° ìƒì„±)\n",
        "# ìƒì„±ì„ ìœ„í•´ 'generate' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "outputs = model.generate(**inputs, max_new_tokens=256) # **inputsì—ì„œ **ëŠ” ë¹ˆì¹¸ì´ ì•„ë‹™ë‹ˆë‹¤.\n",
        "\n",
        "# 4. 'outputs'ì—ëŠ” 'ì…ë ¥'ê³¼ 'ë‹µë³€'ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "# ë”°ë¼ì„œ ì›ë³¸ ì…ë ¥ í† í°ì˜ ê¸¸ì´(ì…ë ¥, 'inputs.input_ids.shape[1]')ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ê·¸ ì´í›„ì— 'ìƒˆë¡œ ìƒì„±ëœ' í† í°(ë‹µë³€)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "new_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
        "\n",
        "# 5. ì¶”ì¶œëœ í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ë¡œ \"\"ë””ì½”ë”©\"\"í•©ë‹ˆë‹¤.\n",
        "# ë””ì½”ë”©ì„ ìœ„í•´ 'decode' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "# 6. ìµœì¢… ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jqUC9Arb6pf"
      },
      "source": [
        "### (2) ìš”ì•½ (Summarization)\n",
        "\n",
        "#### ì›í•˜ëŠ” ê¸€ì„ ë„£ì–´ì„œ ìš”ì•½í•´ ë³´ì„¸ìš”!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPxJeg244a2q",
        "outputId": "58d2c780-544d-4130-e156-07f035ebd717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ì˜ˆì‹œ 2: ìš”ì•½ (Summarization) ---\n",
            "1. A virtual asset exchange ETFê°€ ìƒì¥ì„ í–ˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "\n",
            "2. ì´ìœ ëŠ” ì‹œê¸°ì™€ ì •ì±… ë¶ˆí™•ì‹¤ì„±ì´ ì»¤ì§€ê³  ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
            "\n",
            "\n",
            "3. ETF í™•ëŒ€ê°€ ì¥ê¸°ì ìœ¼ë¡œëŠ” ì‹œì¥ ì•ˆì •í™”ì— ë„ì›€ì´ ëœë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- ì˜ˆì‹œ 2: ìš”ì•½ (Summarization) ---\")\n",
        "\n",
        "# text_to_summarize = \"\"\"~~~\"\"\" ë”°ìŒí‘œ ì•ˆì— ìš”ì•½í•˜ê³  ì‹¶ì€ ê¸€ì„ ë„£ì–´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\"\"\n",
        "# ìš”ì•½í•˜ê³  ì‹¶ì€ ê¸€ ì•„ë¬´ê±°ë‚˜ ë„£ì–´ë³´ì„¸ìš”!\n",
        "\n",
        "text_to_summarize = \"\"\"\n",
        "'í¬ë¦½í†  ìœˆí„°' ì•ŒíŠ¸ì½”ì¸â€¦ETFë¡œ ìŠ¹ë¶€ìˆ˜\n",
        "â€˜íƒ€ì´ë° ì•…ì¬â€™â€¦ ê°€ìƒìì‚° ì „ë°˜ ì¹¨ì²´ ì˜í–¥\n",
        "ì¡°ì •ì¥ì—ë„ ì½”ì¸ ETF ì¤„ëŒ€ê¸° [ì©ë„ë¦¬ì¦˜]\n",
        "<ì•µì»¤>\n",
        "ì§€ë‚œì£¼ ë¯¸êµ­ ì…§ë‹¤ìš´ í•´ì œì™€ ë§ë¬¼ë ¤ ë‚˜ìŠ¤ë‹¥ì— ì¹´ë‚˜ë¦¬ ìºí”¼íƒˆì˜ ë¦¬í”Œ, XRP ìƒì¥ì§€ìˆ˜í€ë“œ(ETF)ê°€ ìƒì¥ì„ í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê¸°ëŒ€ì™€ ë‹¬ë¦¬ ì£¼ì¶¤í•˜ëŠ” ëª¨ìŠµì„ ë³´ì˜€ëŠ”ë°ìš”. ì¶œì‹œë¥¼ ì•ë‘” ë‹¤ë¥¸ ê°€ìƒìì‚° ETFì—ë„ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì„ê¹Œ ì‹œì¥ì˜ ê´€ì‹¬ì´ ì ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ì†Œì‹, ì¦ê¶Œë¶€ ì´ë¯¼ì¬ ê¸°ìì™€ í•¨ê»˜í•©ë‹ˆë‹¤. ì´ ê¸°ì, ê¸°ëŒ€ê°€ ì»¸ë˜ XRP ETFê°€ í˜ì„ ì˜ ëª» ì“°ëŠ” ì´ìœ , ë­¡ë‹ˆê¹Œ?\n",
        "\n",
        "<ê¸°ì>\n",
        "ê°€ì¥ í° ì›ì¸ì€ ì‹œê¸°ì…ë‹ˆë‹¤. ì§€ê¸ˆ ê°€ìƒìì‚° ì‹œì¥ì€ ì´ë¥¸ë°” â€˜í¬ë¦½í†  ìœˆí„°â€™, ì¦‰ ì¡°ì •ì¥ì— ëŒ€í•œ ìš°ë ¤ê°€ ì»¤ì§€ê³  ìˆìŠµë‹ˆë‹¤. ìœ„í—˜ìì‚°ì— ëŒ€í•œ ì„ í˜¸ê°€ ì „ë°˜ì ìœ¼ë¡œ ì¤„ì–´ë“  ë°ë‹¤, ë¯¸êµ­ ì—°ì¤€ì˜ ë§¤íŒŒì  ë°œì–¸ê³¼ íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì˜ ë ˆì„ë• ìš°ë ¤ë¡œ ì •ì±… ë¶ˆí™•ì‹¤ì„±ì´ ì»¤ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì— ìœ ë™ì„± ê°ì†Œì™€ ê°€ìƒìì‚° íŠ¸ë ˆì´ë”© ê¸°ì—…,(DAT) ë“± ì´ë¥¸ë°” â€˜í° ì†â€™ë“¤ì˜ ë§¤ë„ì™€ ì²­ì‚° ë¬¼ëŸ‰ì´ ìŸì•„ì§€ë©´ì„œ ì•…ìˆœí™˜ì´ ì´ì–´ì§€ê³  ìˆë‹¤ëŠ” ë¶„ì„ì…ë‹ˆë‹¤.\n",
        "\n",
        "<ì•µì»¤>\n",
        "ë¹„íŠ¸ì½”ì¸ê³¼ ì´ë”ë¦¬ì›€ ê°™ì€ ëŒ€í‘œ ê°€ìƒìì‚° ETFë“¤ë„ ìƒí™©ì´ ë¹„ìŠ·í•©ë‹ˆê¹Œ?\n",
        "\n",
        "<ê¸°ì>\n",
        "ë¹„ìŠ·í•©ë‹ˆë‹¤. ë¹„íŠ¸ì½”ì¸ í˜„ë¬¼ì€ 9ë§Œ ë‹¬ëŸ¬ì„ ê¹Œì§€ ë°€ë¦¬ë©° ì˜¬í•´ ìƒìŠ¹ë¶„ì„ ê±°ì˜ ë°˜ë‚©í–ˆìŠµë‹ˆë‹¤. ì—…ê³„ì—ì„œëŠ” ë‹¨ê¸° ë³´ìœ ìë“¤ì˜ ì†ì‹¤ ê·œëª¨ê°€ 2022ë…„ FTX ì‚¬íƒœ ì´í›„ ìµœëŒ€ ìˆ˜ì¤€ì´ë¼ëŠ” ì§„ë‹¨ë„ ë‚˜ì˜µë‹ˆë‹¤. ì´ì™€ í•¨ê»˜ ê´€ë ¨ ETFì—ì„œ ì§€ë‚œ 12ì¼(í˜„ì§€ì‹œê°)ë¶€í„° ë¸”ë™ë¡, í”¼ë¸ë¦¬í‹°, ë¹„íŠ¸ì™€ì´ì¦ˆ, ì•„í¬ ë“± ì£¼ìš” ETF ìš´ìš©ì‚¬ë“¤ì´ ë§¤ë„ë¥¼ ì´ì–´ê°€ë©° ìê¸ˆ ìœ ì¶œì´ í™•ëŒ€ë˜ê³  ìˆìŠµë‹ˆë‹¤. í•˜ë£¨ ë™ì•ˆ ì•½ 8ì–µ 7ì²œë§Œ ë‹¬ëŸ¬ê°€ ë¹ ì ¸ë‚˜ê°€ëŠ” ë“± ETF ì¶œë²” ì´í›„ ì—­ëŒ€ ë‘ ë²ˆì§¸ë¡œ í° ê·œëª¨ì˜ ìœ ì¶œì´ ë°œìƒí•˜ê¸°ë„ í–ˆìŠµë‹ˆë‹¤. ì•ŒíŠ¸ì½”ì¸ ETF íˆ¬ìì‹¬ë¦¬ì—ë„ ì°¬ë¬¼ì„ ë¼ì–¹ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "<ì•µì»¤>\n",
        "ê·¸ëŸ°ë°ë„ ì—¬ì „íˆ ìƒì¥ì„ ì¤€ë¹„ ì¤‘ì¸ ê°€ìƒìì‚° ETFê°€ ë§ë‹¤ê³ ìš”. ì´ìœ ê°€ ë­¡ë‹ˆê¹Œ?\n",
        "\n",
        "<ê¸°ì>\n",
        "ì•½ì„¸ë¼ê³  í•´ì„œ ìê¸ˆì´ ì•ˆ ë“¤ì–´ì˜¤ëŠ” ê±´ ì•„ë‹™ë‹ˆë‹¤. ì‹¤ì œë¡œ ë¦¬í”Œ ETFëŠ” ì²«ë‚ ì—ë§Œ 3,500ì–µ ì› ë„˜ëŠ” ìê¸ˆì´ ëª°ë ¸ìŠµë‹ˆë‹¤. ì´ëŸ° ì´ìœ ë¡œ ì›”ê°€ì—ì„œëŠ” ë¸”ë™ë¡ì˜ ë¦¬í”Œ ETF ì°¸ì—¬ ê°€ëŠ¥ì„±ì´ ê±°ë¡ ë˜ê¸°ë„ í–ˆìŠµë‹ˆë‹¤. ë¦¬í”Œë³´ë‹¤ ë¨¼ì € ìƒì¥í•œ ì†”ë¼ë‚˜ ETFì—ëŠ” 10ì›” ë§ë¶€í„° ìê¸ˆì´ ê¾¸ì¤€íˆ ë“¤ì–´ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "<ì•µì»¤>\n",
        "ê²°êµ­ ETF í™•ëŒ€ê°€ ì¥ê¸°ì ìœ¼ë¡œëŠ” ì‹œì¥ ì•ˆì •í™”ì— ë„ì›€ì´ ëœë‹¤ê³  ë³¼ ìˆ˜ ìˆì„ê¹Œìš”?\n",
        "\n",
        "<ê¸°ì>\n",
        "ê·¸ë ‡ìŠµë‹ˆë‹¤. ê°€ìƒìì‚° ì‹œì¥ì´ ì œë„ê¶Œì˜ ì‹ ë¢°ë¥¼ ì–»ëŠ” ê²Œ í•µì‹¬ì¸ë°ìš”.íˆ¬ììë“¤ì´ í˜„ë¬¼ë¡œ ê±°ë˜í•  ìˆ˜ ìˆëŠ” ê°€ìƒìì‚°ì„ êµ³ì´ ETF í˜•íƒœë¡œ ê±°ë˜í•˜ëŠ” ì´ìœ ëŠ”, ìë³¸ì‹œì¥ íˆ¬ììë“¤ì´ ë³„ë„ì˜ ê°€ìƒìì‚° ê±°ë˜ì†Œ ê°€ì…ì´ë‚˜ ë³µì¡í•œ ì ˆì°¨, ê·¸ë¦¬ê³  ì½”ì¸ ê´€ë ¨ ë¶ˆì•ˆê° ì—†ì´ íˆ¬ìë¥¼ í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ì„­ë‹ˆë‹¤. ì¦‰, ê°€ìƒìì‚°ì„ ETFë¼ëŠ” ê·¸ë¦‡ì— ë‹´ì•„ ìë³¸ì‹œì¥ì—ì„œ ì†ì‰½ê²Œ ê±°ë˜í•˜ë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹¤ë¥¸ ì˜ˆë¥¼ ë“¤ì–´ë³´ë©´ ë‹¬ëŸ¬ì™€ ê°™ì€ ë²•ì •í™”íë¥¼ ê°€ìƒìì‚°ì´ë¼ëŠ” ê·¸ë¦‡ì— ë‹´ì•„ ê°€ìƒìì‚° ì‹œì¥ì—ì„œ ê±°ë˜í•  ìˆ˜ ìˆë„ë¡ ë§Œë“  ê²ƒì´ ë°”ë¡œ ìŠ¤í…Œì´ë¸”ì½”ì¸ì…ë‹ˆë‹¤. ì´ë“¤ì€ ì„œë¡œ ì‹œì¥ì˜ ìœ ë™ì„±ì„ ê³µê¸‰í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ìµœê·¼ ë‹¤ìˆ˜ì˜ ETF ì¶œë²”ê³¼ ìŠ¤í…Œì´ë¸”ì½”ì¸ ë“±ì¥ì€ ì´ëŸ¬í•œ íë¦„ì˜ ê¸°ë°˜ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ìƒì¥ì„ ì•ë‘” ì²´ì¸ë§í¬ í˜„ë¬¼ ETFë¥¼ ë¹„ë¡¯í•´ ì•„ë°œë€ì²´, ìŠ¤í…”ë¼, í´ì¹´ë‹·, ë„ì§€ì½”ì¸, ì‹œë°”ì´ëˆ„ ë“± ê°€ìƒìì‚° ETFê°€ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ì½”ì¸ì„ ë¬¶ì€ â€˜ì§€ìˆ˜í˜• ETFâ€™ ì¶œì‹œë„ ì˜ˆì •ë¼ ìˆìŠµë‹ˆë‹¤. ê²°êµ­ ì´ëŸ° ë‹¤ì–‘í•œ ETFë“¤ì´ ì‹œì¥ ë¬¸í„±ì„ ë„˜ìœ¼ë ¤ëŠ” ë°°ê²½ì—ëŠ”, ì œë„ê¶Œ í¸ì…ì„ í†µí•œ ì‹ ë¢° íšŒë³µê³¼ ìê¸ˆ ìœ ì…ì˜ ì„ ìˆœí™˜ì„ ê¸°ëŒ€í•˜ëŠ” ì‹œì¥ ì‹¬ë¦¬ê°€ ìë¦¬í•˜ê³  ìˆë‹¤ëŠ” ë¶„ì„ì…ë‹ˆë‹¤.\n",
        "\n",
        "<ì•µì»¤>\n",
        "ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ì¦ê¶Œë¶€ ì´ë¯¼ì¬ ê¸°ìì˜€ìŠµë‹ˆë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "# content : '~~~'ì— ìš”ì²­ì‚¬í•­ì„ ì ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ìœ„ ê¸€ì„ ì„¸ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ë‹¬ë¼ê³  ìš”ì²­í–ˆìŠµë‹ˆë‹¤.\n",
        "# \"role\": \"user\" ì…ë‹ˆë‹¤.\n",
        "\n",
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": f\"Could you summarize the following text in three sentence?\\n\\nText:\\n{text_to_summarize}\" }\n",
        "]\n",
        "\n",
        "# ì—¬ê¸°ë¶€í„°ëŠ” ê° ì‚¬ë¡€ ëª¨ë‘ ì½”ë“œê°€ ë™ì¼í•©ë‹ˆë‹¤.\n",
        "\n",
        "# 1. ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸('chat')ë¥¼ Gemma ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ê³µì‹ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# 2. ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ëª¨ë¸ì´ ì…ë ¥ë°›ì„ ìˆ˜ ìˆëŠ” í† í° ID í…ì„œë¡œ ë³€í™˜í•˜ê³ , GPU('device')ë¡œ ë³´ëƒ…ë‹ˆë‹¤.\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 3. í† í°í™”ëœ ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ í…ìŠ¤íŠ¸ \"\"ìƒì„±\"\"ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ìµœëŒ€ 256ê°œì˜ ìƒˆ í† í° ìƒì„±).\n",
        "# ìƒì„±ì„ ìœ„í•´ 'generate' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "outputs = model.generate(**inputs, max_new_tokens=256) # **inputsì—ì„œ **ëŠ” ë¹ˆì¹¸ì´ ì•„ë‹™ë‹ˆë‹¤.\n",
        "\n",
        "# 4. 'outputs'ì—ëŠ” 'ì…ë ¥'ê³¼ 'ë‹µë³€'ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "# ë”°ë¼ì„œ ì›ë³¸ ì…ë ¥ í† í°ì˜ ê¸¸ì´(ì…ë ¥, 'inputs.input_ids.shape[1]')ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ê·¸ ì´í›„ì— 'ìƒˆë¡œ ìƒì„±ëœ' í† í°(ë‹µë³€)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "new_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
        "\n",
        "# 5. ì¶”ì¶œëœ í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ë¡œ \"\"ë””ì½”ë”©\"\"í•©ë‹ˆë‹¤.\n",
        "# ë””ì½”ë”©ì„ ìœ„í•´ 'decode' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "# 6. ìµœì¢… ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_UiOd02cuK2"
      },
      "source": [
        "### (3) ë¶„ë¥˜ (Classification)\n",
        "\n",
        "ì›í•˜ëŠ” ë¬¸ì¥ì„ ë„£ê³  ê°ì • ë¶„ì„ì„ í•´ë³´ì„¸ìš”!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN5y1c0v4a2q",
        "outputId": "fd7ff32e-f372-4c66-be3e-207b76d6d6f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ì˜ˆì‹œ 3: ë¶„ë¥˜ (Classification) ---\n",
            "Neutral: ê¸°ëŒ€í–ˆë˜ ê±° ë³´ë‹¨ ë³„ë¡œì¸ ê±° ê°™ì•„ìš”.\n",
            "\n",
            "Negative: ì²˜ìŒì— ì´ë¬¼ì§ˆ í•˜ìë¡œ êµí™˜í•˜ê³  ë°›ì•˜êµ¬, ë¹ ë¥¸ ì²˜ë¦¬ ì£¼ì…¨ì§€ë§Œ ê°€ê²©ì— ë¹„í•´ ê·¸ëƒ¥ ê·¸ëŸ° ê±° ê°™ì•„ìš” !\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- ì˜ˆì‹œ 3: ë¶„ë¥˜ (Classification) ---\")\n",
        "\n",
        "# content: '~~~' ë”°ìŒí‘œ ì•ˆì— ìš”ì²­ì‚¬í•­ì„ ì ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "# ì—¬ê¸°ì„œëŠ” ê¸ì •, ë¶€ì •, ì¤‘ë¦½ìœ¼ë¡œ ê°ì •ë¶„ì„ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "# \\n\\nText: ì—¬ê¸°ì— ê°ì •ë¶„ì„í•  ë¬¸ì¥ì„ ì ì–´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "# ì›í•˜ëŠ” ë¬¸ì¥ì„ ë„£ê³  ê°ì • ë¶„ì„ì„ í•´ë³´ì„¸ìš”!\n",
        "\n",
        "# \"role\": \"user\" ì…ë‹ˆë‹¤.\n",
        "\n",
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": \"Classify the text into neutral, negative, or positive. Generate only the class, nothing else.\\n\\nText: ê¸°ëŒ€í–ˆë˜ ê±° ë³´ë‹¨ ë³„ë¡œì¸ ê±° ê°™ì•„ìš”.. ã… ã…  ì²˜ìŒì— ì´ë¬¼ì§ˆ í•˜ìë¡œ êµí™˜í•˜ê³  ê¸°ë‹¤ë ¸ë‹¤ê°€ ë°›ì•˜êµ¬, ë¹ ë¥¸ ì²˜ë¦¬ ì£¼ì…¨ì§€ë§Œ ê°€ê²©ì— ë¹„í•´ ê·¸ëƒ¥ ê·¸ëŸ° ê±° ê°™ì•„ìš” ! \" }\n",
        "]\n",
        "\n",
        "# 1. ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸('chat')ë¥¼ Gemma ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ê³µì‹ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# 2. ë³€í™˜ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ëª¨ë¸ì´ ì…ë ¥ë°›ì„ ìˆ˜ ìˆëŠ” í† í° ID í…ì„œë¡œ ë³€í™˜í•˜ê³ , GPU('device')ë¡œ ë³´ëƒ…ë‹ˆë‹¤.\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# 3. í† í°í™”ëœ ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ í…ìŠ¤íŠ¸ \"\"ìƒì„±\"\"ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ìµœëŒ€ 256ê°œì˜ ìƒˆ í† í° ìƒì„±).\n",
        "# ìƒì„±ì„ ìœ„í•´ 'generate' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "outputs = model.generate(**inputs, max_new_tokens=256) # **inputsì—ì„œ **ëŠ” ë¹ˆì¹¸ì´ ì•„ë‹™ë‹ˆë‹¤.\n",
        "\n",
        "# 4. 'outputs'ì—ëŠ” 'ì…ë ¥'ê³¼ 'ë‹µë³€'ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "# ë”°ë¼ì„œ ì›ë³¸ ì…ë ¥ í† í°ì˜ ê¸¸ì´(ì…ë ¥, 'inputs.input_ids.shape[1]')ë¥¼ ê¸°ì¤€ìœ¼ë¡œ, ê·¸ ì´í›„ì— 'ìƒˆë¡œ ìƒì„±ëœ' í† í°(ë‹µë³€)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "new_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
        "\n",
        "# 5. ì¶”ì¶œëœ í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ë¡œ \"\"ë””ì½”ë”©\"\"í•©ë‹ˆë‹¤.\n",
        "# ë””ì½”ë”©ì„ ìœ„í•´ 'decode' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "# 6. ìµœì¢… ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCExY4TXqUcy"
      },
      "source": [
        "\n",
        "#### sLM í™œìš©ì‚¬ë¡€ ê²°ê³¼ ìš”ì•½\n",
        "\n",
        "- sLMì€ ì§ˆì˜ì‘ë‹µì´ë‚˜ ìš”ì•½ì€ ì–´ëŠ ì •ë„ ìˆ˜í–‰í•˜ì§€ë§Œ, í€„ë¦¬í‹°ê°€ ì•„ì‰½ìŠµë‹ˆë‹¤.\n",
        "- ê°ì • ë¶„ì„ì—ì„œ 'ë„ˆë¬´ ì¬ë¯¸ê°€ ì—†ë‹¤'ë¥¼ ì¤‘ë¦½ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„ ë¶„ë¥˜ ì‘ì—…ì—ì„œë„ í•œê³„ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "- ì´ë ‡ê²Œ ë²”ìš©ì ì¸ í™œìš©ì—ì„œëŠ” í•œê³„ê°€ ë‚˜íƒ€ë‚˜ì§€ë§Œ, ë¯¸ì„¸ì¡°ì •(Fine-Tuning)ì„ í†µí•´ íŠ¹ì • ì‘ì—…ì— íŠ¹í™”í•˜ë©´ í•´ë‹¹ ì‘ì—…ì—ëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ë©´ì„œë„ ì—¬ì „íˆ ê²½ëŸ‰í™”ëœ ëª¨ë¸ì˜ ì¥ì ì„ ëˆ„ë¦´ ìˆ˜ ìˆëŠ” ê²ƒì´ sLMì˜ íŠ¹ì§•ì…ë‹ˆë‹¤!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwtpRrJivCh7"
      },
      "source": [
        "# ğŸ¤©ğŸ¤© ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤ ğŸ¤©ğŸ¤©\n",
        "â­ï¸â­ï¸ 2-2ì˜ API KEY ì…€ì„ ì§€ìš°ê³  ê³¼ì œ ì œì¶œí•´ì£¼ì„¸ìš”!! â­ï¸â­ï¸"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
