{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8806036",
   "metadata": {},
   "source": [
    "> ### Outline\n",
    "\n",
    "- 안정적 시계열 분석\n",
    "    - AR, MA, ARMA\n",
    "    - 전제조건\n",
    "    - 정상성, 가역성\n",
    "- 차수 결정법\n",
    "- 불안정 시계열 분석\n",
    "    - ARIMA, SARIMA\n",
    "- 다변량 시계열 분석\n",
    "- 시계열 딥러닝\n",
    "    - RNN, Attention, Foundation 개념 및 장단점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5970f3",
   "metadata": {},
   "source": [
    "> ## 1. 단변량 시계열 분석  \n",
    "\n",
    "> ### 1-1. 안정적 시계열 모델  \n",
    "\n",
    "1. 자기 회귀 모델(AR)  \n",
    "**과거 자신의 y 값들을 독립변수**로 삼아서 현재 값을 예측  \n",
    "\n",
    "$AR(p)$: p차 자기 회귀 모형  \n",
    "- 과거 p개 시점의 데이터들의 **선형 조합**을 이용해 예측한 모델  \n",
    "\n",
    "$AR(p):  y_{t}=c +\\phi_1y_{t-1}+\\phi_2y_{t-2}+...+\\phi _py_{t-p}+\\varepsilon _t$\n",
    "\n",
    "- 보통의 선형회귀식에서 $x_n$이 들어가는 자리에 y의 과거 값들($y_{t-n}$)이 들어가게 됨  \n",
    "- $y_t$의 현재 값은 1시점 이전의 값의 영향을 받는다. 이 때, 1시점은 1달, 1년, 1초, 1시간 등등 다양!\n",
    "\n",
    "- OLS가 시계열에 적절하지 않은 이유  \n",
    "    - OLS는 데이터 포인트끼리 **독립적**이라고 가정하기 때문에, 자기상관성이 있는 시계열 데이터에는 적절하지 않다.\n",
    "\n",
    "- AR은 MLE(최우도추정)기반으로 $\\phi$(회귀계수와 비슷한)을 추정  \n",
    "    - MLE: 현재 이 데이터 결과가 나왔을 때, 이 결과를 만들어냈을 확률이 가장 높은 파라미터($\\phi$)는 무엇인지를 역으로 추적하는 방법\n",
    "\n",
    "2. 이동 평균 모델(MA)  \n",
    "**과거 시점의 잔차($\\varepsilon$)를 독립변수**로 삼아서 현재값을 예측\n",
    "\n",
    "- 잔차($\\varepsilon$): 실제 값과 예측 값의 차이  \n",
    "- 잔차들은 독립적이고 백색잡음임\n",
    "\n",
    "  $MA(q)$: q차 이동 평균 모형  \n",
    "- 과거 q개의 예측 오차의 선형 결합으로 예측한 모델  \n",
    "- 보통의 선형회귀식에서 $x_n$이 들어가는 자리에 y의 과거 **오차**들($\\varepsilon_{t-n}$)이 들어가게 됨  \n",
    "- $y_t$의 현재 값은 1시점 이전의 오차($\\varepsilon_{t-1}$)의 영향을 받는다!\n",
    "\n",
    "\n",
    "> AR, MA 모형의 전제 조건  \n",
    "\n",
    "1. 정상성(데이터의 정상성이 아닌 모델의 정상성)  \n",
    "    - AR 모델의 정상성 조건  \n",
    "    특성 방정식: $1 - \\phi_1 z - \\phi_2 z^2 - \\cdots - \\phi_p z^p = 0$  \n",
    "    모든 해의 절댓값 > 1  \n",
    "    (보통 AR(1), AR(2) 까지만 활용하게 됨)  \n",
    "\n",
    "    - MA 모델의 정상성 조건  \n",
    "    잔차(백색잡음)는 정상성을 만족하므로, 잔차의 조합인 MA모델은 항상 정상성을 만족한다.  \n",
    "\n",
    "2. 가역성  \n",
    "MA 모형을 AR 모형의 형태로 재표현할 수 있는가  \n",
    "-> 오차항 ($varepsilon$)을 과거 관측값 ($y_{t-n}$) 형태로 표현할 수 있어야 함!  \n",
    "    - AR 모형의 가역성 조건  \n",
    "    처음부터 만족함  \n",
    "\n",
    "    - MA 모형의 가역성 조건  \n",
    "    특성 방정식: : $1 + \\theta_1 z + \\cdots + \\theta_q z^q = 0$  \n",
    "    모든 해의 절댓값 > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a2770",
   "metadata": {},
   "source": [
    "> ARMA 모형  \n",
    "\n",
    "AR(p), MA(q) 결합 -> ARMA(p,q)  \n",
    "즉, 과거 p개의 관측값 + 과거 q개의 오차를 활용해서 예측하는 방법  \n",
    "-> 정상성(AR 모델의 조건)과 가역성(MA 모델의 조건)을 모두 만족해야 함  \n",
    "\n",
    "ARMA(p,0) -> AR(p)와 같음  \n",
    "ARMA(0,q) -> MA(q)와 같음  \n",
    "ARMA(0,0) -> 백색잡음과 같음\n",
    "\n",
    "> 차수 결정 방법: 자기 상관 함수  \n",
    "\n",
    "두 그래프를 통해, 현재 값이 과거 값과 어떤 관계가 있는지 파악 -> AR, MA, ARMA의 적절한 차수(p,q)를 결정  \n",
    "\n",
    "- <span style=\"background-color:#fff5b1\">자기 상관 계수 (ACF) </span>  \n",
    "ARMA(p,q) 모형에서, 사용할 q개의 오차를 결정하는 방법  \n",
    "두 시점 사이의 상관관계를 파악(간접 영향까지)  \n",
    "\n",
    "$ACF(k)=Corr(y_t,y_{t-k})$  \n",
    "\n",
    "-> 특정 시차(k)에서, 두 관측치 $y_t$ 와 $y_{t-k}$ 사이의 상관계수를 구한다.  \n",
    "이때, t와 t-k 사이의 관측치에 있는 간접적인 영향까지 모두 반영된다!\n",
    "\n",
    "- <span style=\"background-color:#fff5b1\">부분 자기 상관 계수(PACF) </span>  \n",
    "ARMA(p,q) 모형에서, 사용할 p개의 시점을 결정하는 방법  \n",
    "두 시점 사이의 **직접적인 영향만 파악**\n",
    "\n",
    "$e_t = y_t - (\\beta_1y_{t-1} + ... + \\beta_{k-1}y_{t-(k-1)})$일 때, \n",
    "\n",
    "$PACF(k) = Corr(e_t, e_{t-k})$  \n",
    "\n",
    "특정 시차(k)에 대해서, 두 시점(t와 t-k) 사이의 순수한 상관 관계를 나타내는 함수  \n",
    "(중간의 간접적인 영향은 파악하지 않음)\n",
    "\n",
    "\n",
    "> 그래프를 보고 어떻게 결정해야 할까?  \n",
    "\n",
    "- AR(p) 모형\n",
    "    - **ACF : 천천히 감소 or 진동 감소**하는 모양   \n",
    "        AR 모델은 과거 **값**이 계속 영향,  \n",
    "        현재의 값이 **여러 시차의 과거 값과 약하게 연결**\n",
    "        \n",
    "    - **PACF : p 이후 시점부터 0으로 절단**(신뢰구간 안에 들어옴)  \n",
    "        **시점 p까지만의 과거만이 현재에 영향**,  \n",
    "        그 이후 시차는 더 이상 직접적인 영향 X\n",
    "        \n",
    "- MA(q) 모형\n",
    "    - **ACF: q 이후 시점부터 0으로 절단**(신뢰구간 안에 들어옴)  \n",
    "        MA 모델은 q시점까지만 과거 **오차항**의 영향,  \n",
    "        q 이후 시점부터는 더 이상 오차의 영향이 없음\n",
    "        \n",
    "    - **PACF: 천천히 감소 or 진동 감소**하는 모양  \n",
    "        현재값 = 과거 **오차항**의 조합 이므로, 오차항들이 만들어내는 간접 상관이 존재,  \n",
    "        멀리 떨어진 시점 간에도 상관성 존재\n",
    "\n",
    "- ARMA(p,q) 모형  \n",
    "    - 두 그래프의 특징 구분이 명확하지 않음(둘 다 특정 시점(p,q) 이후 신뢰 구간에 들어옴)  \n",
    "\n",
    "![예시](https://blog.kakaocdn.net/dna/sw4Si/btq5ya53u0n/AAAAAAAAAAAAAAAAAAAAAFLOQjb1wVHR6gKvDwKN4COEr4ephVXWMZhs0nwI9Y5o/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1764514799&allow_ip=&allow_referer=&signature=y4Aukakg4Pqpn5IDpBJyXgMDSUk%3D)  \n",
    "\n",
    "-> ACF는 1 이후로 컷오프, PACF는 2 이후로 컷오프 되는 그림  \n",
    "-> AR(2) or MA(1) or ARMA(2,1) 를 활용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c30782",
   "metadata": {},
   "source": [
    "> ### 1-2. 불안전 시계열을 다룰 때  \n",
    "\n",
    "1. 비정상 시계열 데이터(추세, 계절성이 존재)  \n",
    "2. 정상성 확보(차분 or 계절성 차분)  \n",
    "3. -> 정상 시계열로 만듦  \n",
    "4. ARMA로 '안정적 시계열' 분석  \n",
    "\n",
    "> ARIMA(p,d,q) 모델  \n",
    "ARMA + 차분 모델!  \n",
    "\n",
    "ARIMA(p,1,q) -> 1번 차분하고 정상성을 만족하게 된 ARMA(p,q)  \n",
    "ARIMA(0,0,0) -> 백색잡음  \n",
    "ARIMA(p,0,0) -> 자기회귀(AR(p))  \n",
    "ARIMA(0,0,q) -> 이동평균(MA(q))  \n",
    "\n",
    "상수가 없는 ARIMA(0,1,0) -> 랜덤워크  \n",
    "상수가 있는 ARIMA(0,1,0) -> 표류를 포함하는 랜덤워크  \n",
    "\n",
    "- 상수항이 있는 랜덤워크  \n",
    ": 랜덤워크의 특성을 가지나, 특정한 방향성(상향성, 하향성)이 존재하는 랜덤워크  \n",
    "\n",
    "> SARIMA(Seasonal ARIMA)  \n",
    "ARIMA + 계절성 차분 모델  \n",
    "\n",
    "$SARIMA(p,d,q)(P,D,Q)_m$  \n",
    "\n",
    "(p,d,q) -> AR의 차수, 차분의 정도, MA의 차수  \n",
    "(P,D,Q) -> 계절성 AR의 차수, 계절성 차분의 정도, 계절성 MA의 차수  \n",
    "m -> 계절성 주기의 길이  \n",
    "\n",
    "- 차분할 때는 계절성 차분을 먼저 진행 -> 비계절성 차분을 진행  \n",
    "\n",
    "> 하이퍼파라미터 결정 방법  \n",
    "\n",
    "- d, D 결정: ADF로 정상성 확보 여부 판단 후 결정  \n",
    "- p, q 결정:  \n",
    "    - ACF, PACF 그래프를 보고 결정: 대부분 명확하지 않은 경우가 많음  \n",
    "    - 그럴 때에는 AIC, BIC 점수로 평가하여 그리드 서치  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e85893",
   "metadata": {},
   "source": [
    "> ## 2. 다변량 시계열 분석  \n",
    "\n",
    "> VAR  \n",
    "자신의 과거 값과 다른 변수들의 과거 값 함께 사용  \n",
    "\n",
    "- 단변량은 자신의 과거만 사용하고($X_t$가 $X_{t-1}$ 사용)  \n",
    "- 다변량은 서로의 과거를 사용($X_t$가 $X_{t-1}, Y_{t-1}$ 사용, $Y_t$가 $X_{t-1}, Y_{t-1}$ 사용)  \n",
    "\n",
    "- 적합한 경우:  \n",
    "    - 여러 경제 지표가 서로 영향을 주고받는 경우 (GDP, 소비, 투자 등)  \n",
    "    - 금융 시장에서 여러 자산의 가격이 상호작용하는 경우  \n",
    "    - 변수 간 상호 의존성이 중요한 경우  \n",
    "\n",
    "- 부적합한 경우:  \n",
    "    - 변수가 너무 많은 경우 (파라미터 폭발)  \n",
    "    - 변수 간 관계가 명확하지 않은 경우  \n",
    "    - 장기 예측이 필요한 경우  \n",
    "\n",
    "- VAR의 사용 변수 고르는 방법  \n",
    "Granger 인과 검정을 통해, p-value가 0.05 아래라면 귀무가설(유의미한 변수가 아니다)를 기각하고, 그 변수를 포함시킬 수 있다.  \n",
    "- 시차(p)는 보통 AIC 점수를 통해 고른다\n",
    "\n",
    "> VAR 모델 활용 분석 방법  \n",
    "\n",
    "1. 충격 반응 함수(IRF)  \n",
    "한 시계열에 특정 시점에서 충격이 발생할 때, 다른 시계열에, **시간에 따라**, **어떤 영향**을 주는지 분석  \n",
    "\n",
    "예: 금리를 1% 올렸더니(충격), 3개월 뒤에 주가가 떨어졌다가 6개월 뒤에 회복\n",
    "\n",
    "2. 예측오차 분산 분해(Variance Decomposition)  \n",
    "어떤 시계열이 상대적으로 어떤 영향을 끼치고 있는지의 **중요도**를 도출하는 분석  \n",
    "각 변수가 미래의 움직임을 예측하는 데에 어느 정도의 기여를 하는지 평가하는 방법  \n",
    "(Feature importance와 비슷)\n",
    "\n",
    "예: 주가가 변동하는 걸 볼 때, 자기 탓(관성)이 70%고, 금리 충격 탓이 30%\n",
    "\n",
    "- VAR의 장단점  \n",
    "    - 장점: 변수 간의 양방향 분석 지원, 도메인 지식 없이도 여러 변수를 함께 예측, 동시성 문제 없음(과거값만 사용하기 때문)  \n",
    "    - 단점: 변수가 많아지면 파라미터 폭발 문제, 장기적 정보 손실  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64898ec2",
   "metadata": {},
   "source": [
    "> ## 3. 딥러닝 시계열 분석  \n",
    "\n",
    "- 등장 배경: 전통 통계 모델의 한계  \n",
    "데이터가 많으면 처리 속도 느려짐, 크기가 고정된 데이터만 가능, 비선형 패턴 포착이 어려움  \n",
    "\n",
    "> ### 3-1. RNN 계열  \n",
    "\n",
    "1. 심플 RNN  \n",
    "- 적합: 10~100 시점의 짧음~중간 시계열, 순차적 의존성이 강한 데이터(주가, 센서 측정), 자기상관이 큰 데이터  \n",
    "- 부적합: 매우 긴 시계열(장기 의존성 문제), 병렬 처리가 필요한 큰 데이터  \n",
    "\n",
    "2. LSTM  \n",
    "- 적합: 100~1000 시점의 장기 시계열, 중요한 과거 이벤트가 먼 미래에 영향을 줄 때  \n",
    "- 부적합: 계산 자원이 부족할 때, 빠른 처리가 필요할 때\n",
    "\n",
    "3. GRU  \n",
    "- 적합: LSTM처럼 장기 시계열을 다루지만, 학습 속도가 중요한 경우, 데이터가 상대적으로 적음(과적합에 강함), 실시간 처리가 필요한 경우(계산량 부담 적음)  \n",
    "- 부적합: 더 장기적인 이벤트를 포착해야 할 때\n",
    "\n",
    "> ### 3-2. 트랜스포머 계열\n",
    "\n",
    "- 적합: 1000시점 이상의 매우 긴 시계열, 전역 패턴(연간 계절성, 장기 트렌드)이 중요할 때, 대규모 데이터(병렬 연산 가능), 다변량 시계열(변수 간 상호작용을 잘 학습)  \n",
    "- 부적합: 짧은 시계열(과적합 위험), 계산 자원이 부족할 때  \n",
    "\n",
    "- 파생 모델 종류\n",
    "    - Vanilla Transformer: 기초적 트랜스포머, 짧은~중간 길이에 적합  \n",
    "    - Informer  \n",
    "        ProbSparse Attention(중요한 쿼리만 선택)이 핵심  \n",
    "        계산 복잡도가 적어 긴 시계열 처리 가능 -> 장기 예측에 강함  \n",
    "    - Autoformer  \n",
    "        요소분해(Decomposition) + 자기상관(Auto-Correlation)이 핵심\n",
    "        추세와 계절성을 분리해 학습하고, 주기성 포착에 강함 -> 주기 패턴이 뚜렷한 데이터에 적합\n",
    "    - Temporal Fusion Transformer (TFT)  \n",
    "        다중 시점 예측 + 외생 변수 활용이 핵심  \n",
    "        **해석이 가능**하고, 정적/동적 변수 모두 사용함  \n",
    "        여러 외생 변수를 고려해야 하는 경우에 적합  \n",
    "\n",
    "> ### 3-3. Foundation Model\n",
    "거대한 데이터로 사전 학습하여, 다양한 문제를 해결할 수 있는 범용 모델  \n",
    "\n",
    "사전 학습 후, 범용적 모델을 파인튜닝으로 특화  \n",
    "\n",
    "- LLM 기반 모델  \n",
    "    - Without Adaptation: LLM이 이미 시계열 지식을 가지고 있다고 가정하고 프롬프트 엔지니어링에 집중  \n",
    "    - Adapt LLM: LLM을 시계열 데이터로 파인튜닝  \n",
    "    - Adapt to LLM: 시계열을 텍스트로 변환 -> LLM 활용\n",
    "    - 장점  \n",
    "        Zero-shot: 학습 없이도 상식적인 추론으로 예측이 가능  \n",
    "    - 단점  \n",
    "        토큰화 문제: 숫자를 언어처럼 쪼개다 보니 시계열의 연속성을 잃을 수 있음  \n",
    "        비용/속도: 모델이 너무 커서 무겁고, API 비용이 발생할 수 있음  \n",
    "\n",
    "- 자체 시계열 전용 대형 모델  \n",
    "시계열 전요응로 처음부터 학습한 모델  \n",
    "예: TimeGPT, TimesFM, Moirai  \n",
    "    - 장점  \n",
    "        시계열의 특성(주기, 추세)을 이미 다 알고 있어서, 데이터만 넣으면 바로 고성능 예측이 가능  \n",
    "        파인튜닝 없이도 다양한 도메인에 바로 적용 가능  \n",
    "    - 단점  \n",
    "        블랙박스 문제: 내부가 공개되지 않은 경우(TimeGPT 등)가 있음  \n",
    "        특정 도메인의 아주 특이한 패턴은 사전 학습에 없으면 못 예측할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b42c7f",
   "metadata": {},
   "source": [
    "> ### 3-4. 딥러닝 모델 선택할 때  \n",
    "\n",
    "- 트레이드오프 4가지  \n",
    "1. 단순함 <-> 성능  \n",
    "2. 해석 가능성 <-> 정확도  \n",
    "3. 학습 시간 <-> 예측 성능  \n",
    "4. 데이터의 크기  \n",
    "\n",
    "| 상황 | 추천 모델 | 이유 |\n",
    "| --- | --- | --- |\n",
    "| 짧은 시계열 (< 100) | 통계 모델, RNN | 데이터가 적으면 단순한 모델이 과적합 방지에 유리 |\n",
    "| 중간 길이 (100~1000) | LSTM, GRU | 장기 의존성 학습 가능, 적절한 복잡도 |\n",
    "| 매우 긴 시계열 (1000+) | Informer, Autoformer | 긴 시퀀스의 전역 패턴 효율적 학습 |\n",
    "| 장기 의존성 중요 | LSTM, Transformer | GRU보다 먼 과거 정보 유지에 강함 |\n",
    "| 주기 패턴 뚜렷 | Autoformer | 추세/계절성 분해로 주기 포착 우수 |\n",
    "| 외생 변수 많음 | TFT | 정적/동적 변수 융합 + 해석 가능 |\n",
    "| 대규모 데이터 | Transformer 계열 | 병렬 연산으로 학습 효율적 |\n",
    "| 범용 예측 필요 | Foundation Model | 사전 학습된 지식으로 빠른 적용 |\n",
    "\n",
    "- 데이터포인트가 적을 때는 딥러닝보다 통계적 모델이 더 적합  \n",
    "- GRU는 LSTM보다 빠르지만, 장기 의존성 문제에는 더 약함  \n",
    "- 해석이 중요한(상사에게 설명해야 하는, 블랙박스 문제가 있으면 취약한) 비즈니스 문제에는 TFT를 고려  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
