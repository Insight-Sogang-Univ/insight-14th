{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4012a86b",
   "metadata": {},
   "source": [
    "## Focus on Image 사전과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00659971",
   "metadata": {},
   "source": [
    "### 1. 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d0771",
   "metadata": {},
   "source": [
    "- 인간의 신경망을 따라 만들었음\n",
    "- 여러 input이 주어지면 인공신경망을 통과해서 하나의 output이 출력되는 구조\n",
    "- 흐름:\n",
    "    복수의 input -> 각각의 가중치와 내적 계산 (가중합) -> 활성함수 -> output 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddfdb76",
   "metadata": {},
   "source": [
    "- input (이전 layer의 output)과 가중합해서 output을 내는 hidden layer의 개수가 하나면 단층, 여러 개면 다층 퍼셉트론\n",
    "- 단층 퍼셉트론은 layer가 총 2개인데 (hidden layer는 1개), 이러면 비선형성을 부여를 못해서 XOR게이트를 구현하지 못함\n",
    "- 따라서, 다층 퍼셉트론(MLP)으로 문제 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc48ea2",
   "metadata": {},
   "source": [
    "### 2. 딥러닝 모델의 학습\n",
    "- Foward Propagation, Back Propagation (순전파, 역전파)\n",
    "- 순전파: 예측 수행\n",
    "    - 각 hidden layer, 그리고 마지막 출력 전마다 활성화 함수를 적용\n",
    "    - 활성화 함수 종류\n",
    "        1. 계단함수: 단층 퍼셉트론에서 사용. 직각으로 생긴 로지스틱회귀 모양\n",
    "        2. 시그모이드: 0~1 부드럽게. 연속성, 비선형성\n",
    "        3. ReLU: max(0,x), 계산이 적음, 비선형성, 미분하면 0 or 1이어서 역전파가 잘 됨 (기울기 소실 해결)\n",
    "        4. Tanh: -1 ~ 1 시그모이드\n",
    "        5. softmax: 개별 클래스의 확률을 구할 때 사용. 손실 함수가 예측의 확률(softmax 함수)과 정답 확률과의 차이(크로스 엔트로피 손실 함수)를 바로 계산할 수 있어 학습 효율이 높다고 함\n",
    "- 역전파: 예측값과 실제값의 loss를 계산하고, input -> output의 gradient를 계산해서 각 파라미터의 가중치를 조절함 (학습, chain rule 활용)\n",
    "    - 그런데 가중치가 (0,1)이면 기울기 소실 발생 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b602d",
   "metadata": {},
   "source": [
    "- 손실함수:\n",
    "    - MSE\n",
    "        - 회귀\n",
    "    - BCE: 이진 크로스 엔트로피\n",
    "        - 이진 클래스 분류\n",
    "    - CEE: 크로스 엔트로피\n",
    "        - 다중 클래스 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3885e",
   "metadata": {},
   "source": [
    "- 학습 과정을 조금 더 딥하게 살펴보자면, 각 batch만큼, iteration번 학습함 (epoch).\n",
    "- 이때, 손실함수를 최소화(by 가중치, 편향 조절)해야하는데, 미분을 통한 gradient descent를 사용.\n",
    "    1. batch gradient descent\n",
    "        - 한 번에 전체를 학습해버림\n",
    "        - 때문에 수렴이 안정적으로 되기는 하나, 비효율적임. \n",
    "    2. SGD\n",
    "        - 배치 크기를 1로 만들음\n",
    "        - 랜덤으로 하나의 데이터를 샘플링하고, 그 데이터를 통해 파라미터 최적화\n",
    "        - 빨리빨리 업데이트되기는 하는데, 파라미터값이 불안정함 -> 정확도가 낮을 수도 있음\n",
    "    3. Mini Batch gradient descent\n",
    "        - 위 두개를 섞음. 즉, 적절한 batch를 통해 비용과 정확도 두 마리의 토끼를 잡는 구조\n",
    "        - batch size를 잘 결정해야함. 보통 2^n으로 한다고 함. (default=2^5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d2460",
   "metadata": {},
   "source": [
    "- 경사하강법을 노가다로 할 수는 없으니, 알아서 찾아주는게 옵티마이저\n",
    "- GD의 방향과 속력을 조절함. (속도 조절. 속도 = 방향&크기)\n",
    "    - 모멘텀: 관성을 부여함. local minima를 빠져나갈 수 있게 함\n",
    "    - RMSProp: 속력을 조절함. 기울기가 극단적이라도 안정적인 속력을 갖도록 조절\n",
    "    - Adam: 위 두개 짬뽕. 각 파라미터의 1차 모멘트(기울기의 평균)와 2차 모멘트(기울기의 제곱 평균)을 사용하여 학습률을 조정한다고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149bb22",
   "metadata": {},
   "source": [
    "- 데이터 증강 & 전이 학습 (fine-tuning)\n",
    "    - 데이터 증강: 데이터에 변화를 줘서 robust하게 예측할 수 있도록 도와줌 (과적합 방지)\n",
    "    - fine-tuning: pre-training한 걸 기반으로 fine-tuning. 성능이 좋아지고 내가 하는 작업에 커스텀할 수 있음. 또한, 기존에 있던 걸 활용하기 때문에 계산 비용, 성능 등에 다 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ee1c1",
   "metadata": {},
   "source": [
    "### 3. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1424f",
   "metadata": {},
   "source": [
    "- 딥러닝을 통한 이미지 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eccfda",
   "metadata": {},
   "source": [
    "- 픽셀: 픽셀\n",
    "- 채널: 색공간 (ex. RGB, 그레이스케일)\n",
    "    - rgb는 채널 값(차원?)이 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1a7c5",
   "metadata": {},
   "source": [
    "- 이미지 인식을 왜 MLP 안쓰고 CNN으로 할까?\n",
    "    1. 이미지 구조 유지\n",
    "        - 픽셀 & 0~255 사이의 값들의 행렬\n",
    "    2. 이미지의 위치, 크기, 회전 등이 달라져도 인식 good\n",
    "        - 이미지의 위치별 특징을 학습하고, 차원 축소를 하기 때문\n",
    "    3. 파라미터 폭발\n",
    "        - MLP는 완전연결 구조이기때문에, 대량의 픽셀을 가지는 이미지를 MLP를 통해 학습하면 파라미터가 너무 많아짐\n",
    "        - 과적합도 많이 됨\n",
    "        - CNN은 부분연결 구조이기 때문에 위 문제가 해결됨\n",
    "    4. 공간 정보 손실\n",
    "        - flatten하는 과정에서 정보 손실이 많이 되지만, CNN은 공간 정보, 특징 등을 학습하고 flatten하기 때문에 괜찮다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642b4d3",
   "metadata": {},
   "source": [
    "- 흐름\n",
    "    1. 이미지를 픽셀화\n",
    "    2. Feature Learning: 위치별 특징  \n",
    "        2-1. Covolution layer  \n",
    "            2-1-1. filter 적용  \n",
    "                - 가중치를 가지는 특정 크기를 가지는 filter 행렬과, 픽셀의 행렬을 내적해서 계산  \n",
    "                - 이때, 가장자리의 정보도 포함하기 위해 padding 활용 (가장자리에 0 추가. 차원 늘어남, 정보 보존)  \n",
    "            2-1-2. stride  \n",
    "                - filter를 움직여서 적용하는데, 몇 픽셀만큼 움직이게 할거냐를 결정  \n",
    "                - 계속 움직여서 계속 내적 계산  \n",
    "                - 결과적으로 filter map (activation map)이 나옴. (내적(가중합)의 결과)  \n",
    "          \n",
    "        2-2. Pooling layer  \n",
    "            - 최대한 filtermap의 정보를 보존하면서 차원을 축소함.  \n",
    "            - 픽셀의 영역 내에서 최댓값 or 평균값을 계산해서 차원을 축소함. (픽셀 개수가 줄어듧)  \n",
    "            - 노이즈 축소, 과적합 방지. but 정보 손실 가능성이 높아짐.  \n",
    "    3. Classification  \n",
    "        - 위에서 만든 벡터들을 flatten  \n",
    "        - 이 flatten된 벡터를 통해 이미지를 분류  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8942c",
   "metadata": {},
   "source": [
    "- AlexNet, VGGNet, Resnet 등이 있다.\n",
    "- 요즘은 인간보다 뛰어나짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02b1f7",
   "metadata": {},
   "source": [
    "### 4. 이미지 딥러닝 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb63097",
   "metadata": {},
   "source": [
    "1. 이미지 분류\n",
    "2. 객체 탐지\n",
    "    - 객체를 둘러싸는 bounding box 찾기\n",
    "    - YOLO, Fast-RNN, SSD 등\n",
    "3. 이미지 캡셔닝\n",
    "    - 이미지 내의 객체들 간의 관계 파악\n",
    "    - attention, rnn을 통해 단어나 문장을 생성할 수 있다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
