{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5edaf1a3",
   "metadata": {},
   "source": [
    "1. **언어 모델**\n",
    "    - 통계기반 모델\n",
    "        - SLM\n",
    "        - N-gram\n",
    "        - PPL\n",
    "    - 딥러닝기반 모델\n",
    "        - BERT\n",
    "            - 특징\n",
    "            - 사전학습(1) : Masked Language Model(MLM)\n",
    "            - 사전학습(2) : Next Sentence Prediction(NSP)\n",
    "        - GPT\n",
    "            - LLM의 한계\n",
    "        - RAG\n",
    "            - RAG의 구조\n",
    "        - Langchain\n",
    "            - Langchain의 아이디어, 특징\n",
    "    - sLM\n",
    "        - 모델 압축\n",
    "        - sLM의 장단점\n",
    "        - sLM과 LLM의 결합\n",
    "2. **생성 모델**\n",
    "    - 분류모델과 생성모델의 차이\n",
    "    - 생성모델의 분류\n",
    "        - AE\n",
    "        - VAE\n",
    "        - GAN\n",
    "            - 구조와 장단점\n",
    "        - 확산모델\n",
    "            - 순확산과 역확산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be912a5e",
   "metadata": {},
   "source": [
    "## 언어모델\n",
    "\n",
    "- 단어 스퀀스에 확률을 할당해 가장 자연스러운 단어 시퀀스를 찾는 모델.\n",
    "    - 이전 단어들을 통해 다음 단어들을 예측\n",
    "\n",
    "- 전통적인 통계 기반 모델과, 인공신경망을 활용한 모델로 나눌 수 있다\n",
    "\n",
    "- 활용 예시\n",
    "    - 기계 번역: P(나는 버스를 탔다) > P(나는 버스를 태운다)\n",
    "    - 오타 교정: \"선생님이 교실로 부리나케\" → P(달려갔다) > P(잘려갔다)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2abd8e",
   "metadata": {},
   "source": [
    "### SLM (통계적 언어 모델)\n",
    "\n",
    "- 분포 가설 (Distributional Hypothesis)에 근거\n",
    "    - 분포 가설: 비슷한 문맥에 함께 등장하는 단어들은 비슷한 의미를 가진다.\n",
    "    - 예시: \"I eat pizza\", \"I eat chicken\", \"I eat pasta\"에서 pizza, chicken, pasta는 모두 '음식'이라는 카테고리에 속한다고 학습함\n",
    "\n",
    "1. 조건부 확률\n",
    "    - 앞의 단어들과 뒤의 단어의 조건부 확률을 계산한다\n",
    "    - 문장의 확률은 단어들이 순차적으로 등장할 확률의 곱으로 계산함\n",
    "    - 예: P(오늘, 날씨가, 좋다) = P(오늘) × P(날씨가|오늘) × P(좋다|오늘, 날씨가)\n",
    "    - 확률이 높을수록 자연스러운 문장이라고 판단함\n",
    "\n",
    "2. 카운트 기반\n",
    "    - 확률을 구할 때, 빈도를 통해 확률을 계산한다.\n",
    "    - 예를 들어, '오늘 날씨가'가 총 250번 등장했는데, 그중 150번이 '오늘 날씨가 좋다'의 형태였다면, 확률은 150/250 = 60%이다.\n",
    "    - 카운트 기반으로 확률을 구하는 건 직관적이지만, 명확한 한계를 가진다.\n",
    "\n",
    "- 희소 문제: 데이터의 양이 부족해 언어를 정확하게 모델링하지 못함.\n",
    "    - 상황 1: 분자가 0인 경우\n",
    "        - 학습 데이터에 '오늘 날씨가 좋다'가 없으면 확률이 0%로 계산됨\n",
    "        - 자연스러운 문장임에도 불가능한 문장으로 판단하는 문제 발생\n",
    "    - 상황 2: 분모가 0인 경우\n",
    "        - '오늘 날씨가'라는 표현 자체가 없으면 확률 자체를 계산할 수 없음\n",
    "        - 새로운 문맥에 대해 예측 불가능한 상태가 됨\n",
    "    - 모델의 퀄리티가 학습 데이터의 범위에 매우 종속적이게 되기 때문에, 그 학습 데이터가 인간이 사용하는 언어의 분포를 정확하게 반영하지 않는다면 모델도 그만큼 성능이 떨어지게 된다.\n",
    "    - 이 문제로 인해 인공신경망 기반 모델로 패러다임이 넘어감\n",
    "\n",
    "## N-gram\n",
    "\n",
    "- 이전 N-1개의 단어만 고려해 다음 단어의 확률을 계산하는 통계 기반 언어 모델\n",
    "- 전체 문맥을 모두 보는 대신 일부 최근 단어들만 사용하여 희소 문제를 완화하려는 아이디어\n",
    "- N-gram 배경\n",
    "    - 참고하는 단어가 많을수록 코퍼스에서 해당 시퀀스를 찾기 어려워짐\n",
    "    - 예: P(좋다 | 오늘 날씨가)를 계산하려면 '오늘 날씨가'가 반드시 존재해야 함\n",
    "    - 해결: 참고하는 단어 수를 줄임 → P(좋다 | 오늘 날씨가) ≈ P(좋다 | 날씨가)\n",
    "- N-gram 종류\n",
    "    - N=1 (Unigram): 단어 하나만 보고 확률 계산\n",
    "    - N=2 (Bigram): 바로 직전 단어만 고려\n",
    "    - N=3 (Trigram): 직전 두 단어 고려\n",
    "- 확률 계산 방식\n",
    "    - N-gram 모델에서는 다음 단어를 예측할 때 n-1개의 단어만 참고함\n",
    "    - 예: 3-gram 모델은 바로 앞의 2개 단어만 참고하여 다음 단어 예측\n",
    "- 장점\n",
    "    - 구현이 매우 간단함\n",
    "    - 학습과 추론 속도가 빠름\n",
    "    - 초창기 NLP 시스템에서 폭넓게 사용됨\n",
    "- 한계\n",
    "    - N이 커질수록 필요한 데이터가 기하급수적으로 늘어남\n",
    "    - 문맥을 길게 기억하지 못함\n",
    "    - 희소 문제가 여전히 존재함\n",
    "    - N을 선택하는 것의 Trade-off 문제\n",
    "        - N을 크게 하면: 더 많은 문맥 고려하지만 희소 문제 심해짐\n",
    "        - N을 작게 하면: 희소 문제는 줄지만 예측 정확도가 떨어짐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c3ccc",
   "metadata": {},
   "source": [
    "- 언어 모델이 문장을 얼마나 잘 예측하는지 평가하는 대표 지표\n",
    "- 값이 낮을수록 모델이 문장을 더 잘 예측한다는 의미\n",
    "- 핵심 개념\n",
    "    - 문장 전체의 확률을 기반으로 모델의 \"혼란도\"를 측정하는 값\n",
    "    - 모델이 다음 단어를 얼마나 헷갈려 하는지를 수치화한 것\n",
    "    - 예측이 정확하면 → 문장 확률 ↑ → PPL ↓\n",
    "    - 예측이 불확실하면 → 문장 확률 ↓ → PPL ↑\n",
    "- 직관적 의미\n",
    "    - PPL = 1 → 모델이 다음 단어를 완벽히 맞춤\n",
    "    - PPL = 50 → 다음 단어 후보 50개 중 하나를 고르는 수준으로 혼란스러움\n",
    "    - PPL이 낮을수록 언어 모델의 품질이 좋음\n",
    "- 활용\n",
    "    - N-gram 모델 성능 비교\n",
    "    - RNN, LSTM, Transformer 등 딥러닝 언어 모델의 학습 상태 확인\n",
    "    - 모델이 문장 구조를 얼마나 잘 이해하는지 평가할 때 사용됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6430e93f",
   "metadata": {},
   "source": [
    "### 딥러닝 기반 언어 모델\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40960fbd",
   "metadata": {},
   "source": [
    "- 단어를 벡터로 임베딩하고, 신경망이 문맥적 패턴을 직접 학습해 다음 단어를 예측하는 방식\n",
    "- 통계 모델과 달리, 긴 문맥·복잡한 의미 관계·비선형 패턴까지 학습할 수 있다는 점이 핵심\n",
    "- 이 계열의 대표 모델이 BERT와 GPT\n",
    "\n",
    "#### BERT\n",
    "- 특징\n",
    "    - 양방향(Bidirectional) 모델로, 앞뒤 문맥을 모두 보고 단어의 의미를 파악함\n",
    "    - 빈칸을 중심으로 양쪽 문맥을 동시에 고려해 의미를 이해하는 구조\n",
    "    - 문맥 이해 능력이 매우 뛰어남\n",
    "    - 문장 분류, 감정 분석, QA 등 이해 기반(NLU) 작업에서 강함\n",
    "    - Transformer Encoder 구조 기반\n",
    "    - 통계 모델과 비교하면, 단순 빈도 대신 단어 의미·문장 관계·문맥 패턴을 실제로 학습함\n",
    "- 사전학습(1): Masked Language Model (MLM)\n",
    "    - 입력 문장에서 일부 단어를 [MASK]로 가리고 가려진 단어를 예측하도록 학습함\n",
    "    - 예: \"오늘 날씨가 [MASK] .\" → 모델은 앞뒤 문맥을 보고 \"좋다\", \"맑다\", \"춥다\" 같은 단어를 예측함\n",
    "    - 문맥 양방향 이해를 가능하게 하는 핵심 훈련 방식\n",
    "- 사전학습(2): Next Sentence Prediction (NSP)\n",
    "    - 두 문장이 실제로 이어지는 문장인지 판별하는 학습\n",
    "    - A 다음에 B가 실제로 이어지는 문장인지, 아니면 무관한 문장인지 판별\n",
    "    - 문장 간 논리적 연결 관계를 이해하게 됨\n",
    "    - 문서 분류, QA, 문장 매칭 같은 작업에서 유용함\n",
    "\n",
    "#### GPT\n",
    "- 오로지 다음 단어 예측(언어 생성)에 최적화된 단방향(autoregressive) 모델\n",
    "- Transformer Decoder 구조를 사용함\n",
    "- 주어진 앞 문맥만 보고 다음 단어 생성\n",
    "- 문장 생성 능력 매우 뛰어남\n",
    "- 요약, 번역, 대화 등 생성 기반(NLG) 작업에서 강함\n",
    "\n",
    "#### LLM의 한계\n",
    "- 크기가 커질수록 능력이 좋아지지만, 다음 한계를 갖고 있음\n",
    "    - 최신 지식 반영 어려움 (학습 데이터 시점 고정)\n",
    "    - 환각(Hallucination) 발생\n",
    "    - 외부 정보와 직접 연결되지 않음\n",
    "    - 모델 크기 증가 → 비용, 메모리, 추론 시간 모두 증가\n",
    "- 이 한계가 RAG, LangChain, sLM 같은 기술 등장으로 이어짐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c68d1c",
   "metadata": {},
   "source": [
    "### RAG (Retrieval-Augmented Generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56893f72",
   "metadata": {},
   "source": [
    "- LLM의 한계를 보완하기 위해 외부 지식 검색을 결합한 생성 방식\n",
    "- 모델이 알고 있는 정보만으로 답변하는 것이 아니라, 필요한 정보를 외부 DB(문서, 위키, 사내 자료 등)에서 검색해 가져온 뒤 그 내용을 기반으로 응답을 생성함\n",
    "- 검색 + 생성을 한 흐름에서 수행하는 구조\n",
    "\n",
    "#### RAG의 구조\n",
    "- Retriever(검색기)\n",
    "    - 사용자의 질문을 벡터로 변환함\n",
    "    - 임베딩 DB에서 관련 문서를 검색함\n",
    "    - 질문과 가장 유사한 문서들을 top-k 형태로 가져옴\n",
    "- Generator(생성기: LLM)\n",
    "    - 검색된 문서와 질문을 함께 입력받음\n",
    "    - 외부 정보 기반으로 더 정확하고 최신성을 가진 답을 생성함\n",
    "\n",
    "#### 왜 필요한가?\n",
    "- LLM 단독으로는 다음 문제가 생김\n",
    "    - 학습 cutoff 때문에 최신 정보 부족\n",
    "    - 사실 기반 응답이 부족해 환각 발생\n",
    "    - 사내 문서, 개인 문서 같은 폐쇄형 데이터에 접근 불가\n",
    "- RAG는 이 문제를 해결함\n",
    "    - 최신 정보 반영 가능\n",
    "    - 검색한 실제 데이터를 근거로 답변\n",
    "    - LLM의 파라미터를 재학습할 필요 없음\n",
    "    - 비용적 효율도 높음\n",
    "- LLM은 \"생각하는 역할\", 검색 시스템은 \"아는 역할\"을 담당함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcb706",
   "metadata": {},
   "source": [
    "### LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5228d92",
   "metadata": {},
   "source": [
    "- LLM을 단순 질의응답이 아니라 '애플리케이션 형태'로 확장하기 위한 프레임워크\n",
    "- LLM이 외부 도구·DB·API와 연결되어 직접 행동하고 작업 흐름을 자동화하도록 만들기 위한 구조\n",
    "\n",
    "#### LangChain의 핵심 아이디어\n",
    "- LLM이 단순히 텍스트만 생성하는 것이 아니라, 필요한 도구를 스스로 선택해 사용하며 목적을 수행하도록 만든다\n",
    "- 모델에게 \"지식\"뿐 아니라 \"행동 능력\"을 부여하는 것이 핵심\n",
    "\n",
    "#### LangChain의 주요 특징\n",
    "- 체인(Chain)\n",
    "    - 여러 단계의 프롬프트 또는 작업을 하나의 흐름으로 연결함\n",
    "    - 예: 질문 → 검색 → 요약 → 정답 생성\n",
    "    - 모든 단계를 코드로 묶어 자동화 가능함\n",
    "- 에이전트(Agent)\n",
    "    - LLM이 스스로 어떤 도구를 사용할지 결정함\n",
    "    - 예:\n",
    "        - 검색이 필요하면 Retriever 호출\n",
    "        - 코드 실행이 필요하면 파이썬 REPL 호출\n",
    "        - 계산이 필요하면 Calculator 도구 사용\n",
    "    - LLM이 \"계획 + 실행\"을 자체적으로 수행함\n",
    "- 도구(Tools) 연동\n",
    "    - DB, API, 문서 검색기, 코드 실행기 등 거의 모든 시스템과 연결 가능함\n",
    "    - 챗봇이 실제로 업무 도구를 쓰는 형태를 구현할 수 있음\n",
    "- RAG 파이프라인 구축 용이\n",
    "    - 임베딩, 벡터스토어, 리트리버, LLM 응답 등 전체 구조를 쉽게 구성할 수 있음\n",
    "    - RAG 앱을 빠르게 개발할 때 사실상 표준처럼 사용됨\n",
    "\n",
    "#### 핵심 요약\n",
    "- LangChain은 언어 모델을 \"지능형 소프트웨어\"로 만드는 도구\n",
    "- LLM + 도구 + 워크플로우 자동화\n",
    "- 검색기·DB·코드 실행기와 연결 가능\n",
    "- RAG, Agent, Pipeline 등 다양한 방식으로 확장 가능\n",
    "- LLM을 사용한 애플리케이션 제작을 실용적으로 가능하게 한 프레임워크\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758a1fa",
   "metadata": {},
   "source": [
    "### sLM (small Language Model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2075acdf",
   "metadata": {},
   "source": [
    "- 파라미터 수가 적고, 경량화·최적화된 언어 모델\n",
    "- 거대 모델(LLM)과 달리, 작고 빠르며 특정 작업에 특화된 성능을 내기 위해 설계된 모델\n",
    "- 최근 AI 실사용 흐름에서 \"LLM + sLM 결합\" 방식이 강하게 부상 중\n",
    "\n",
    "#### 모델 압축\n",
    "- sLM은 보통 다음과 같은 방식으로 LLM을 줄여서 만듦\n",
    "    - 지식 증류(Knowledge Distillation): 큰 모델의 출력을 작은 모델이 모방하도록 학습함\n",
    "    - 양자화(Quantization): 파라미터 비트를 줄여 메모리 사용량 감소\n",
    "    - 프루닝(Pruning): 중요도가 낮은 뉴런·연결 제거해 경량화\n",
    "    - 로우랭크 근사(LoRA 등): 학습 파라미터를 분리해 계산량을 줄임\n",
    "- 이런 기법으로 작은 모델이지만 실제 환경에서 충분한 성능을 발휘하도록 조정함\n",
    "\n",
    "#### sLM의 장단점\n",
    "- 장점\n",
    "    - 속도가 매우 빠름\n",
    "    - 디바이스 온프레미스·로컬(PC, 모바일 등)에서 실행 가능\n",
    "    - 비용이 낮아 배포·운영에 유리함\n",
    "    - 특정 작업(예: 요약, 분류, 질의응답)에 최적화 가능\n",
    "- 단점\n",
    "    - LLM 대비 지식량 부족\n",
    "    - 복잡한 추론·창의적 작업은 어려움\n",
    "    - 새로운 상황·문맥에 대한 일반화 성능이 낮음\n",
    "    - 환각 가능성 존재\n",
    "- 정밀한 추론보다는 빠른 처리와 실용성이 핵심 목표\n",
    "\n",
    "#### sLM과 LLM의 결합\n",
    "- 최근 가장 중요한 트렌드 중 하나\n",
    "- LLM은 고차 reasoning / 복잡한 계획 담당\n",
    "- sLM은 속도 빠른 실행, 반복 작업, 실제 서비스 추론 담당\n",
    "- 일반적인 구조:\n",
    "    - LLM이 계획을 세움\n",
    "    - sLM이 계획된 세부 작업을 빠르게 실행\n",
    "    - 필요하면 다시 LLM이 보정함\n",
    "- 예시: \"LLM이 감독(Director)이고, sLM은 실제로 움직이는 작업자(Worker) 역할을 맡는다\"\n",
    "- 중요한 사고는 LLM이 하고, 대량의 반복 처리나 실제 서비스 응답은 sLM이 담당함\n",
    "- 이 방식은 성능·비용·속도 균형을 맞추기 때문에 차세대 AI 애플리케이션의 기본 구조로 간주됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7643f887",
   "metadata": {},
   "source": [
    "## 생성 모델\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c1783",
   "metadata": {},
   "source": [
    "### 분류 모델 vs 생성 모델\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e37c11",
   "metadata": {},
   "source": [
    "#### 분류 모델(Classification Model)\n",
    "- 입력 데이터를 보고 미리 정의된 레이블을 선택하는 모델\n",
    "- 예: 스팸/햄 분류, 감정 분석, 이미지 카테고리 분류 등\n",
    "- 목표는 \"무엇인지 판별하는 것\"\n",
    "- 입력 → 특징 추출 → 레이블 확률 출력 구조를 가짐\n",
    "\n",
    "#### 생성 모델(Generative Model)\n",
    "- 새로운 데이터를 직접 만들어내는 모델\n",
    "- 예: 텍스트 생성, 이미지 생성, 음성 생성, 샘플 생성 등\n",
    "- 목표는 \"데이터 분포를 학습하고 새로운 샘플 생성\"\n",
    "- 단순 분류가 아니라 데이터 자체를 복원하거나 창조함\n",
    "- 분류 모델은 \"정답을 고르는 모델\", 생성 모델은 \"아예 새로운 데이터를 만들어내는 모델\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90d995",
   "metadata": {},
   "source": [
    "### 생성 모델의 분류\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b1059",
   "metadata": {},
   "source": [
    "- 생성 모델은 학습 방식·구조에 따라 여러 종류로 나뉨\n",
    "- 대표적으로 AE, VAE, GAN, Diffusion이 있음\n",
    "\n",
    "#### AE (Autoencoder)\n",
    "- 입력을 압축(인코딩)했다가 다시 복원(디코딩)하는 구조\n",
    "- 목표는 \"입력 데이터의 중요한 구조를 학습하는 것\"\n",
    "- 장점: 구조 단순, 계산 효율적\n",
    "- 한계: 너무 단순해 복잡한 분포를 잘 생성하지 못함\n",
    "- 생성 모델이라기보다는 \"특징 추출·차원 축소\"용에 가까움\n",
    "\n",
    "#### VAE (Variational Autoencoder)\n",
    "- AE의 한계를 개선한 확률 기반 생성 모델\n",
    "- 입력을 '하나의 점'으로 압축하는 AE와 달리, 잠재공간을 확률분포로 모델링하여 더 다양한 샘플 생성이 가능함\n",
    "- 특징:\n",
    "    - 잠재공간을 정규분포로 가정\n",
    "    - 샘플링 과정이 수학적으로 안정적\n",
    "    - 매끄러운 latent space 조작 가능\n",
    "- 장점:\n",
    "    - 안정적 학습\n",
    "    - 다양한 생성 가능\n",
    "- 단점:\n",
    "    - 생성물이 다소 흐릿함(특히 이미지)\n",
    "\n",
    "#### GAN (Generative Adversarial Network)\n",
    "- 가장 혁신적인 생성 모델 중 하나\n",
    "- Generator(생성기)와 Discriminator(판별기)가 서로 경쟁하며 학습함\n",
    "- 구조:\n",
    "    - 생성기: 가짜 데이터를 만듦\n",
    "    - 판별기: 가짜/진짜 구분\n",
    "    - 생성기는 판별기를 속이기 위해 점점 더 정교한 데이터를 생성함\n",
    "- 장점:\n",
    "    - 매우 선명하고 고품질의 샘플 생성 가능\n",
    "    - 이미지 생성에서 특히 강력함\n",
    "- 단점:\n",
    "    - 학습이 불안정함\n",
    "    - 모드 붕괴(mode collapse) 문제 발생\n",
    "        - 생성기가 특정 패턴만 반복적으로 생성하는 문제\n",
    "- GAN은 사기꾼(Generator)과 경찰(Discriminator)이 서로 경쟁하며 동시에 성장하는 구조\n",
    "\n",
    "#### 확산 모델(Diffusion Model)\n",
    "- 최근 가장 강력한 이미지 생성 기술\n",
    "- Stable Diffusion, DALL·E 등이 이 방식 기반\n",
    "- 핵심 아이디어:\n",
    "    - 원본 데이터에 점점 노이즈를 추가(순확산)\n",
    "    - 완전한 노이즈에서 다시 원본 데이터를 복원(역확산)\n",
    "- 순확산(Forward Diffusion):\n",
    "    - 이미지에 단계적으로 노이즈를 넣어 완전히 무작위로 만듦\n",
    "    - 모델은 이 \"노이즈 추가 과정\"을 학습함\n",
    "- 역확산(Reverse Diffusion):\n",
    "    - 랜덤 노이즈를 입력으로 받고\n",
    "    - 노이즈를 단계적으로 제거하며 이미지를 생성함\n",
    "    - 결국 \"노이즈 → 이미지\" 변환 함수가 모델에 학습되는 것\n",
    "- 장점:\n",
    "    - 매우 안정적, 고해상도 생성 품질\n",
    "- 단점:\n",
    "    - 계산량이 많고 느림\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78028ff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
