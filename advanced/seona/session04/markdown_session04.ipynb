{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd60275",
   "metadata": {},
   "source": [
    "## 1. 언어 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29af11",
   "metadata": {},
   "source": [
    "### 1.1 언어 모델이란?\n",
    "- 언어 모델: 단어 시퀀스에 확률 할당 -> 가장 자연스러운 단어 시퀀스를 찾는 모델\n",
    "- ex. `나는 버스에` 다음에 올 단어로 `탔다` 예측해주기\n",
    "\n",
    "- 통계 기반\n",
    "    - 등장 빈도를 기반으로 확률 계산\n",
    "    - N-gram, Perplexity 등\n",
    "- 인공 신경망 기반\n",
    "    - 단어를 벡터로 표현 후, 신경망 통해 다음 단어 예측\n",
    "    - RNN, LSTM, GRU, Transformer 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc367e8",
   "metadata": {},
   "source": [
    "### 1.2 SLM (통계적 언어 모델)\n",
    "\n",
    "> #### Statistic Language Model\n",
    "- **분포 가설**\n",
    "    - 비슷한 문맥에서 함께 나타는 단어들 -> 비슷한 의미를 가질 것이다~!\n",
    "    - **조건부 확률**과 **카운트기반 확률** 통해 분포 가설 설명 가능\n",
    "- 조건부 확률\n",
    "    $$P(B|A) = \\frac{P(A,B)}{P(A)} \\\\ P(A, B) = P(A) \\times \\frac{P(A,B)}{P(A)} = P(A) \\times P(B|A)$$\n",
    "    - `오늘`, `날씨가`, `좋다`가 문장에서 동시에 나타날 확률\n",
    "        - $P(오늘, 날씨가, 좋다) = P(오늘) \\times P(날씨가|오늘) \\times P(좋다|오늘, 날씨가)$\n",
    "    - 이렇게 계산 확률이 높을수록, 해당 문장이 자연스럽다고 판단 가능\n",
    "\n",
    "- 카운트 기반\n",
    "    - 이전 단어 시퀀스의 **등장 빈도**를 기반으로 다음 단어의 등장 확률 계산\n",
    "    - `오늘 날씨가` 뒤에 `좋다`라는 단어가 등장할 확률 ($P(좋다|오늘\\ 날씨가)$)\n",
    "        - (`오늘 날씨가 좋다`가 등장한 횟수) / (`오늘 날씨가`가 등장한 횟수)\n",
    "    - 한계: **희소 문제**\n",
    "        - 데이터의 부족으로 언어를 정확하게 모델링하지 못하는 문제\n",
    "\n",
    "\n",
    "> #### N-gram\n",
    "- 앞 단어를 모두를 참고하는 이전 방식과 달리\n",
    "- 앞 단어 n-1개만 참고하는 확률 계산 방식\n",
    "- 연속된 n개의 단어 시퀀스를 활용한다\n",
    "- 한계\n",
    "    - 여전한 희소 문제\n",
    "    - n 선택의 Trade-off 문제\n",
    "    - 문맥 파악의 한계\n",
    "\n",
    "\n",
    "> #### Perplexity(PPL)\n",
    "- 언어 모델 성능 평가 지표\n",
    "- 낮을수록 좋다\n",
    "$$PPL(W) = P(w_1, w_2, \\dots, W_N)^{-\\frac{1}{N}}$$\n",
    "- W: 평가하려는 문장\n",
    "- N: 문장 W에 포함된 총 단어 수\n",
    "- P(W): 언어 모델이 문장 W에 부여하는 확률. 높을수록 문장이 자연스럽다고 판단 -> PPL값이 낮아짐\n",
    "- 문장(W)이 희소하거나 or 문장의 길이(N)가 작아질수록  \n",
    "    -> PPL이 커진다(모델이 예측할 때 혼란스러워 한다 = 모델이 문맥을 더 잘 이해 못한다)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327b05d",
   "metadata": {},
   "source": [
    "### 1.3 딥러닝 기반 언어 모델\n",
    "\n",
    "> #### LLM (대규모 언어 모델)\n",
    "- 방대한 양의 데이터를 학습하여\n",
    "- 다양한 작업 수행이 가능한 인공지능 모델\n",
    "- 작동 방식\n",
    "    1. **대규모 학습**\n",
    "        - 방대한 양의 데이터 학습 통해\n",
    "        - 단어 간의 관계, 문맥적 의미, 문법 구조, 세상의 지식 등을 터득\n",
    "    2. **예측 및 생성**\n",
    "        - 학습한 패턴 기반으로\n",
    "        - 다음에 올 가장 확률 높은 단어를 순서대로 예측하며\n",
    "        - 답변을 생성한다\n",
    "- 사용 사례: 텍스트 생성 / 기계 번역 / 질의응답 / 문서 요약 / 감정 분석\n",
    "\n",
    "\n",
    "> #### BERT\n",
    "- 트랜스포머(Transformer)의 인코더(Encoder) 부분을 활용해 만든 양방향(Bidirectional) 언어모델\n",
    "- 특징\n",
    "    - **양방향**\n",
    "        - 특정 단어의 좌우 문맥(앞뒤 문맥)을 동시에 고려한다\n",
    "    - **트랜스포머 기반**\n",
    "        - 트랜스포머 구조의 인코더만을 활용한다\n",
    "    - **사전학습 + 파인 튜닝** \n",
    "        - 대량의 데이터로 일반적인 언어능력을 학습하고(사전학습),\n",
    "        - 사전 학습된 모델을 기반으로 특정 과제에 맞게 나중에 살짝 조정한다(파인 튜닝) \n",
    "\n",
    "- BERT 구조\n",
    "    - 트랜스포머의 인코더를 쌓아 올린 구조\n",
    "- 처리 과정\n",
    "    1. input\n",
    "    2. 임베딩\n",
    "    3. 트랜스포머 인코더 (base는 12개, large는 24개)\n",
    "    4. ouput\n",
    "- 임베딩\n",
    "    - Token 임베딩 (단어 자체의 의미)\n",
    "    - Segment 임베딩 (A or B 문장인지 구분)\n",
    "    - Position 임베딩 (문장에서 몇번째 문장인지)\n",
    "    - 위 3개의 임베딩을 합쳐서 입력 벡터를 만들어 사용한다\n",
    "- 사전학습(1): MLM\n",
    "    - 입력 텍스트의 15%를 랜덤 마스킹 후\n",
    "    - 가려진 단어가 무엇인지 맞추면서\n",
    "    - 사전 학습 진행\n",
    "- 사전학습(2): NSP\n",
    "    - 두 문장이 이어지는지(`IsNext`) 안 이어지는지(`NotNext`)를 학습\n",
    "    - 문장 간 관계 이해 능력 기르기 위해\n",
    "        - 질문-답, 문장 전개 등의 과제에서 중요\n",
    "    \n",
    "- BERT 이후\n",
    "    - RoBERTa\n",
    "        - BERT의 사전 훈련 과정을 개선\n",
    "        - 주요 차이점\n",
    "            - 더 거대한 데이터셋을 활용\n",
    "            - NSP 제거\n",
    "            - 배치크기 및 학습률 최적화\n",
    "            - 마스킹 패턴을 동적으로 변경 -> 다양한 패턴 학습\n",
    "    - ALBERT\n",
    "        - BERT의 경량화 버전\n",
    "        - 주요 차이점\n",
    "            - 파라미터 공유:  레이어 간 파라미터 공유 -> 모델 크기 감소\n",
    "            - 임베딩 행렬 분해: 임베딩 행렬을 더 작은 2개의 행렬로 분해 -> 계산 비용과 메모리 요구량 감소\n",
    "\n",
    "> #### GPT\n",
    "- GPT의 역사\n",
    "    1. GPT-1\n",
    "        - 비지도 데이터 통해 사전 훈련 후\n",
    "        - 다양한 NLP 과제에 맞게 지도 학습으로 파인 튜닝\n",
    "    2. GPT-2\n",
    "        - 지도학습으로 파인 튜닝하는 단계 제거 -> 언어모델의 다양한 작업 수행 가능성 제시\n",
    "    3. GPT-3\n",
    "        - zero-shot, one-shot, few-shot 실험\n",
    "        - prompting 도입\n",
    "    4. GPT-4\n",
    "        - 멀티모달 기능 도입\n",
    "            - 이미지, 텍스트 동시 처리\n",
    "    \n",
    "- GPT의 아키텍쳐\n",
    "    - 트랜스포머의 디코더만 사용\n",
    "    - 디코더의 Self-Attention은 Masked Attention으로 동작한다\n",
    "        - 미래 단어를 볼 수 없도록 마스킹\n",
    "        - -> 생성하려는 단어의 왼쪽에 위치한 단어만 참고 가능\n",
    "    - 동작원리\n",
    "        - 입력된 텍스트이 앞부분 단어 기반으로\n",
    "        - 다음에 올 단어의 확률 분포 계산\n",
    "        - 확률이 가장 높은 단어를 선택하여\n",
    "        - 문장을 순차적으로 생성\n",
    "    - GPT vs BERT\n",
    "        - BERT: 양방향, 이해 중심\n",
    "        - GPT: 단방향, 생성 중심\n",
    "- LLM의 한계\n",
    "    - 할루시네이션\n",
    "    - 업데이트 비용\n",
    "\n",
    "> #### RAG\n",
    "- 관련 정보를 검색하여 찾고 -> 이를 LLM에 넣어 최종 답변 생성\n",
    "- 구조 \n",
    "    1. 질의 인코더\n",
    "        - 질문 이해 위한 언어 모델\n",
    "        - 질문을 벡터 형태로 인코딩\n",
    "    2. 지식 검색기\n",
    "        - 인코딩된 질문 바탕으로\n",
    "        - 외부 지식 베이스에서 관련 정보 검색\n",
    "    3. 지식 증강 생성기\n",
    "        - 검색된 지식 바탕으로 답변 생성하는 언어 모델\n",
    "        - 기존의 LLM보다 정확하고 풍부한 답변 생성 가능\n",
    "- 장점\n",
    "    - 풍부한 정보 제공\n",
    "    - 실시간 정보 반영\n",
    "    - 환각 방지\n",
    "- CAG\n",
    "    - RAG 역시 여전히 답변의 품질과 정확도 문제 존재\n",
    "    - 이러한 RAG의 한계 보완 -> CAG\n",
    "    - 잘못된 컨텍스트가 모델 성능에 미치는 부정적 영향 최소화 & 문서의 신뢰도 스스로 판단, 활용할 수 있게 훈련\n",
    "\n",
    "> #### LangChain\n",
    "- LLM + RAG 구현을 쉽게 할 수 있도록 도와주는 프레임워크\n",
    "- 특징\n",
    "    1. 추상화\n",
    "        - LLM 서비스 만들 때 필요한 각종 작업을 간소화해줌\n",
    "    2. 표준화\n",
    "        - 비슷한 기능 갖춘 요소들을 똑같은 형식의 컴포넌트로 표준화\n",
    "            - 다양한 AI 모델들 -> 하나의 형식으로\n",
    "            - 다양한 원본 데이터 -> 표준화된 컴포넌트로\n",
    "    3. 체이닝\n",
    "        - 주요 컴포넌트를 쉽게 연결해서 LLM 서비스 로직 쉽게 파악\n",
    "        - Input -> 컴포넌트 -> Output\n",
    "- LangGraph\n",
    "    - LangChain에서는 복잡한 로직 구현 어렵다는 한계 존재\n",
    "    - LangGraph를 통해 비선형적이고 동적인 프로세스 구현 가능\n",
    "        - 노드\n",
    "            - 특정 작업 수행 단위\n",
    "            - 랭체인의 컴포넌트 그대로 가져와 노드로 사용 가능\n",
    "        - 엣지\n",
    "            - 노드와 노드 연결하는 선\n",
    "    - 사이클, 루프를 만들 수 있다\n",
    "- LangSmith\n",
    "    - LLM에서는 최종 답변이 어떻게 도출되는지 파악 어려움 -> 랭스미스 등장\n",
    "    - 핵심 기능\n",
    "        - 디버깅 및 추적\n",
    "            - LLM 어플리케이션의 모든 실행과정 단계별로 시각화\n",
    "        - 테스트 및 평가\n",
    "            - 질문과 정답 데이터셋 통해 LLM 애플리케이션의 성능 자동 평가\n",
    "        - 모니터링\n",
    "            - 실제 사용자들 모니터링\n",
    "        - 프롬프트 허브\n",
    "            - 다양한 프롬프트 저장, 버전 관리, 테스트 공간 제공\n",
    "- 외부 API 활용\n",
    "    - API를 통해 LLM 외부 지식 활용해 답변"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a139e",
   "metadata": {},
   "source": [
    "### 1.4 sLM\n",
    "- LLM보다 규모와 범위가 작은\n",
    "- 자연어 콘텐츠를 처리, 이해, 생성할 수 있는 AI 모델델\n",
    "\n",
    "> #### 1. 모델 압축\n",
    "1. 가지치기\n",
    "    - 신경망에서 불필요한 매개변수 제거\n",
    "    - 가중치 or 뉴런 자체 or 신경망 계층의 매개변수 -> 가지치기\n",
    "\n",
    "2. 양자화\n",
    "    - 고정밀 데이터 -> 저정밀 데이터로 변환\n",
    "    - 계산 부하 감소 & 추론 속도 향상\n",
    "3. 지식 증류\n",
    "    - 증류: 사전 학습 내용 -> 더 작은 모델로 이전\n",
    "\n",
    "> #### 2. sLM의 예\n",
    "- DistilBERT\n",
    "- Gemma\n",
    "- GPT-4o mini\n",
    "등.\n",
    "\n",
    "\n",
    "> #### 3. sLM의 장점\n",
    "- 프라이버시\n",
    "    - 로컬 환경에서 실행 가능 -> 보안에 유리\n",
    "- 비용 절감\n",
    "    - 비용이 낮고, 에너지 소비가 적다\n",
    "- 효율성 & 맞춤화\n",
    "    - 특정 작업에 최적화가 쉬우며\n",
    "    - 실시간 처리 같은 분야에서 높은 성능\n",
    "\n",
    "\n",
    "> #### 4. sLM의 한계\n",
    "- 편향\n",
    "- 제한된 일반화\n",
    "- 할루시네이션\n",
    "- 성능과 용량의 한계\n",
    "\n",
    "> #### 5. LLM과 sLM의 결합 (결합 추론)\n",
    "- SMART 프레임워크\n",
    "    - sLM이 먼저 문제를 풀고\n",
    "    - 막히는 부분이 있으면 LLM의 도움 받아 해결\n",
    "- 작동 원리\n",
    "    1. sLM이 먼저 문제를 풀어 문제에 대한 단계별 풀이 초안 작성\n",
    "    2. 각 단계를 평가하고 점수를 매겨 평가\n",
    "    3. 점수가 낮은 어려운 문제는 LLM에게 요청하여 정확한 내용으로 수정\n",
    "- 장점\n",
    "    - 높은 정확도 달성하면서\n",
    "    - LLM 토큰 사용량을 90%까지 절약 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8da35",
   "metadata": {},
   "source": [
    "## 2. 생성 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a76255b",
   "metadata": {},
   "source": [
    "### 2.1 분류 모델 vs 생성 모델\n",
    "\n",
    "> #### 생성 모델\n",
    "- 학습 데이터의 분포를 따르는 유사한 데이터 생성하는 모델\n",
    "- 입력 데이터의 확률 분포 파악하고자 함\n",
    "\n",
    "> #### 생성 모델 분류\n",
    "- 명시적 확률밀도 모델\n",
    "    - 학습 데이터의 분포 기반 생성\n",
    "    - Tractable Density\n",
    "        - 정확히 계산 가능\n",
    "        - 확률 분포 직접 표현 & likelihood 계산 가능한 경우\n",
    "    - Approximate Density\n",
    "        - 근사적 likelihood 기반\n",
    "        - 정확한 likelihood 계산 어렵\n",
    "- 암시적 확률밀도 모델\n",
    "    - 학습 데이터의 분포와 상관 없이 생성\n",
    "    - Markov Cahin\n",
    "        - GSN\n",
    "    - Direct\n",
    "        - GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be8ab5",
   "metadata": {},
   "source": [
    "### 2.2 AE vs VAE\n",
    "\n",
    "> #### AE (Auto Encoder)\n",
    "- 입력과 동일한 출력 만드는 것이 목적인 신경망\n",
    "- 차원 축소, 특징 추출, 노이즈 제거, 이상 탐지 등에 사용\n",
    "\n",
    "- 구조\n",
    "    - 인코더\n",
    "        - 고차원 입력 데이터 -> 잠재 표현으로 변환하는 네트워크\n",
    "    - 디코더\n",
    "        - 잠재 표현을 풀어서 -> 입력을 재복원하여 출력하는 네트워크\n",
    "    - 잠재 표현\n",
    "        - 원본 데이터를 저차원으로 압축하여 정보 저장하는 벡터\n",
    "\n",
    "> #### VAE (Variational Auto Encoder)\n",
    "- 데이터를 잠재 공간으로 인코딩 후 -> 잠재 공간에서 다시 디코딩하여 -> 원본 데이터와 유사한 결과 생성\n",
    "- 이미지 생성, 텍스트 생성, 신호 처리, 이미지 보간 등에 사용\n",
    "- 구조\n",
    "    - 인코더\n",
    "        - 고차원 입력 데이터 -> 분포로 변환하는 네트워크\n",
    "    - 잠재 벡터\n",
    "        - 원본 데이터를 저차원으로 압축하여 정보 저장하는 벡터\n",
    "    - 디코더\n",
    "        - 샘플링된 잠재 표현을 풀어서 -> 입력을 재복원하여 출력하는 네트워크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b0864",
   "metadata": {},
   "source": [
    "### 2.3 GAN\n",
    "\n",
    "- 생성자 신경망과 판별자 신경망이 경쟁하면서 -> 훈련을 통해 정교하게 작업 수행하는 신경망 모델\n",
    "    - 생성자: 진짜 분포에 가까운 가짜분포 생성\n",
    "    - 판별자: 가짜 분포에 속하는지 진짜 분포에 속하는지 결정\n",
    "- 이미지 생성, 영상 합성 등에 사용\n",
    "\n",
    "> #### GAN 적용 사례\n",
    "- 가짜 이미지 생성\n",
    "- 가짜 오바마 연설 영상\n",
    "- Eye In-Painting\n",
    "\n",
    "> #### GAN 장단점\n",
    "- 진짜 같은 가짜 생성 가능\n",
    "- 한계\n",
    "    - 학습의 불안정\n",
    "    - 끝없는 숨바꼭질\n",
    "    - Mode collapsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87acb2",
   "metadata": {},
   "source": [
    "### 2.4 확산 모델\n",
    "\n",
    "- diffusion model\n",
    "    - 입력 이미지에 noise 추가하고\n",
    "    - 여러 단계에 걸쳐 noise 제거함으로써\n",
    "    - 유사한 확률 분포를 가진 결과 이미지 생성\n",
    "\n",
    "> #### 순확산\n",
    "- 데이터에 점진적으로 노이즈 추가하는 과정\n",
    "1. 원본 데이터\n",
    "2. 노이즈 추가\n",
    "2. 완전한 노이즈 데이터: 완전히 무작위 노이즈가 된다!\n",
    "\n",
    "\n",
    "> #### 역환산\n",
    "- 노이즈 데이터에서 원본 데이터 재구성 과정\n",
    "1. 노이즈 데이터\n",
    "2. 노이즈 제거: 순확산 과정의 반대 방향으로 진행\n",
    "3. 원본 데이터 복원\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
