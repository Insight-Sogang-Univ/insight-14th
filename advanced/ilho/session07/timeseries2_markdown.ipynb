{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7651106",
   "metadata": {},
   "source": [
    "\n",
    "## 1. 단변량 시계열 분석\n",
    "\n",
    "### 1.1 안정적 시계열 분석\n",
    "\n",
    "#### ① 자기 회귀 모델 (AR, Autoregressive Model)\n",
    "* **개념**: 과거의 **자기 자신 데이터**를 독립변수로 삼아 현재 값을 예측.\n",
    "* **수식 ($AR(p)$)**:\n",
    "    $$y_{t}=c +\\phi_1y_{t-1}+\\dots+\\phi_py_{t-p}+\\varepsilon _t$$\n",
    "* **특징**:\n",
    "    * 데이터의 자기상관성 활용.\n",
    "    * OLS와 달리 예측값을 다시 예측에 사용하는 구조 내재 (MLE 추정).\n",
    "* **정상성 조건**: 특성 방정식의 모든 해의 절댓값이 1보다 커야 함.\n",
    "\n",
    "#### ② 이동 평균 모델 (MA, Moving Average)\n",
    "* **개념**: 과거의 **예측 오차**를 독립변수로 삼아 현재 값을 예측.\n",
    "* **수식 ($MA(q)$)**:\n",
    "    $$y_t = \\mu +\\varepsilon_t+ \\theta_1\\varepsilon_{t-1}+\\dots+\\theta_q\\varepsilon_{t-q}$$\n",
    "* **특징**:\n",
    "    * 과거의 충격이 현재까지 영향을 미친다고 가정.\n",
    "    * 항상 정상성을 만족함 (백색잡음의 결합이므로).\n",
    "* **가역성 조건**: 모델의 유일성 보장을 위해 MA 모형을 AR 형태로 표현 가능해야 함.\n",
    "\n",
    "#### ③ AR, MA 모델의 전제 조건\n",
    "* **정상성 (Stationarity)**: AR 모형은 필수 (단위근 검정이 아닌 특성 방정식으로 모델 자체의 정상성 판단).\n",
    "* **가역성 (Invertibility)**: MA 모형은 필수 (현재 값을 과거의 관측값으로 표현 가능 여부).\n",
    "\n",
    "#### ④ ARMA (Autoregressive Moving Average) 모형\n",
    "* **개념**: AR(p)와 MA(q)를 결합. 과거 관측값과 과거 오차를 모두 활용.\n",
    "* **특수 형태**:\n",
    "    * ARMA(p, 0) = AR(p)\n",
    "    * ARMA(0, q) = MA(q)\n",
    "    * ARMA(0, 0) = 백색 잡음\n",
    "\n",
    "#### ⑤ 차수 결정 방법: 자기 상관 함수 (ACF, PACF)\n",
    "* **ACF (자기 상관 계수)**: $y_t$와 $y_{t-k}$ 사이의 **간접 영향 포함** 모든 상관관계. (MA 차수 q 결정에 활용)\n",
    "* **PACF (부분 자기 상관 계수)**: 두 시점 사이의 다른 시점 영향을 제거한 **직접적인 상관관계**. (AR 차수 p 결정에 활용)\n",
    "\n",
    "\n",
    "\n",
    "**[차수 결정 가이드]**\n",
    "\n",
    "| 구분 | AR(p) | MA(q) | ARMA(p, q) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **ACF** | 천천히 감소 / 진동 감소 | **q+1 시점부터 절단 (0)** | 천천히 감소 |\n",
    "| **PACF** | **p+1 시점부터 절단 (0)** | 천천히 감소 / 진동 감소 | 천천히 감소 |\n",
    "\n",
    "### 1.2 불안정 시계열\n",
    "\n",
    "#### ① ARIMA (Autoregressive Integrated Moving Average)\n",
    "* **구조**: $ARMA(p, q) + \\text{차분(d)} \\rightarrow ARIMA(p,d,q)$.\n",
    "* **특수 형태**:\n",
    "    * ARIMA(0,1,0): 랜덤워크 (예측 불가, 방향성 X).\n",
    "    * ARIMA(0,1,0) + 상수: 표류가 있는 랜덤워크 (방향성 존재).\n",
    "\n",
    "#### ② SARIMA (Seasonal ARIMA)\n",
    "* **구조**: $ARIMA(p,d,q) + \\text{계절성(Seasonal) 차분} \\\\ \\rightarrow SARIMA(p,d,q)(P, D, Q)_m$\n",
    "* **파라미터**:\n",
    "    * $(p, d, q)$: 비계절성 패턴.\n",
    "    * $(P, D, Q)_m$: 계절성 패턴 ($m$: 주기).\n",
    "* **특징**: ACF에서 주기 $m$의 배수 지점에서 값이 급등하는 현상 발생 시 사용.\n",
    "\n",
    "#### ③ 불안정 시계열 하이퍼파라미터 결정\n",
    "1.  **d, D 결정**: ADF 검정 등을 통해 정상성이 확보되는 차분 횟수 찾기 (주로 0 또는 1).\n",
    "2.  **p, q, P, Q 결정**: ACF, PACF 그래프로 대략적 범위 추정 후, **Grid Search** 수행.\n",
    "3.  **최종 선택**: **AIC, BIC** 값이 가장 낮은 모델 선택.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 다변량 시계열 분석\n",
    "\n",
    "### ① VAR (Vector Autoregression)\n",
    "* **핵심**: 자신의 과거 값뿐만 아니라 **다른 변수들의 과거 값**도 함께 사용하여 예측 (양방향 영향).\n",
    "* **추천 상황**: 경제 지표(GDP-소비), 금융 자산 등 변수 간 상호 의존성이 높을 때.\n",
    "* **변수 선택 (Granger Causality Test)**:\n",
    "    * \"X의 과거가 Y의 미래 예측에 도움이 되는가?\" 검정.\n",
    "    * p-value < 0.05 이면 변수 채택.\n",
    "    * *주의: 실제 인과관계가 아닌 예측적 인과관계를 의미.*\n",
    "\n",
    "### VAR 분석 도구\n",
    "1.  **충격 반응 함수 (IRF)**: 한 변수의 충격이 다른 변수에 시간에 따라 어떤 반응을 일으키는지 분석.\n",
    "2.  **예측오차 분산 분해 (Variance Decomposition)**: 미래 예측 오차의 원인이 각 변수로부터 얼마나 기인했는지 중요도 산출.\n",
    "\n",
    "### 장단점\n",
    "* **장점**: 도메인 지식 없이 변수 간 관계 활용 가능, 양방향 분석.\n",
    "* **단점**: 변수 증가 시 파라미터 폭발(Overfitting), 장기 예측 성능 저하.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 딥러닝을 활용한 시계열 분석\n",
    "\n",
    "### ⓪ 등장 배경\n",
    "* 전통 통계 모델의 한계(속도, 비선형성, 데이터 크기 제약) 극복.\n",
    "\n",
    "### ① RNN 계열\n",
    "1.  **RNN**: 기본 순환 구조. 긴 시계열에서 기울기 소실 문제 발생.\n",
    "2.  **LSTM**: 게이트(Gate) 추가로 장기 의존성(Long-term dependency) 학습 가능.\n",
    "3.  **GRU**: LSTM 경량화 모델. 학습 속도가 빠름.\n",
    "\n",
    "\n",
    "\n",
    "### ② Transformer 계열\n",
    "* **특징**: Attention 메커니즘 활용, 병렬 처리 가능, 긴 시계열 및 대규모 데이터에 적합.\n",
    "* **모델 종류**:\n",
    "    * **Vanilla Transformer**: 기본 모델, 시계열 적용.\n",
    "    * **Informer**: ProbSparse Attention으로 긴 시계열 효율적 처리 (LSTF).\n",
    "    * **Autoformer**: 추세/계절성 분해(Decomposition) 구조 적용, 주기성 포착 우수.\n",
    "    * **TFT (Temporal Fusion Transformer)**: 외생 변수 활용 용이, 해석 가능성.\n",
    "\n",
    "### ③ Foundation Model\n",
    "* **개념**: 거대 데이터로 사전 학습된 범용 모델 (Fine-tuning하여 사용).\n",
    "* **유형**:\n",
    "    * **LLM 기반**: 텍스트 모델을 시계열에 적용 (Prompting, Fine-tuning).\n",
    "    * **자체 거대 모델**: 시계열 전용으로 처음부터 학습 (TimeGPT 등).\n",
    "\n",
    "### ④ 딥러닝 모델 선택 가이드\n",
    "* **짧은 시계열 (<100)**: 통계 모델 (ARIMA), RNN (과적합 방지).\n",
    "* **중간 길이 (100~1000)**: LSTM, GRU.\n",
    "* **매우 긴 시계열 (1000+)**: Informer, Autoformer (전역 패턴 학습).\n",
    "* **주기성 뚜렷**: Autoformer.\n",
    "* **외생 변수 많음 & 해석 필요**: TFT.\n",
    "* **범용/빠른 적용**: Foundation Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2ad92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
