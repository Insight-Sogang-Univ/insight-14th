{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7651106",
   "metadata": {},
   "source": [
    "# 1. 과거를 기억하는 신경망: RNN (Recurrent Neural Network)\n",
    "\n",
    "## 1-1. 순차(Sequence) 데이터란?\n",
    "\n",
    "### 순차 데이터(Sequence Data)의 정의\n",
    "- 순서에 의미가 있으며, 순서가 달라질 경우 의미가 손상되는 데이터\n",
    "- 순차 데이터 세부 분류\n",
    "    - 순차 데이터 (Sequence Data): 가장 넓은 범위\n",
    "    - 시간적 순차 데이터 (Temporal Sequence Data): 순서가 ‘시간의 흐름’을 나타내는 경우\n",
    "    - 시계열 데이터 (Time Series): 시간적 순차 데이터 중 ‘일정한 시간 간격’으로 기록되는 경우\n",
    "\n",
    "### 순차 데이터의 예시\n",
    "- 1. 음성 및 오디오 데이터: 소리 파형은 시간에 따라 연속적으로 변화\n",
    "- 2. 자연어: 문장, 단락 등 단어의 순서가 중요한 데이터\n",
    "- 3. 생물학적 서열 데이터: DNA, 단백질 서열\n",
    "- 4. 비디오 데이터: 여러 이미지 프레임이 순서대로 나열된 데이터\n",
    "\n",
    "### 기존 정형 데이터와의 차이점\n",
    "- 일반적인 정형 데이터의 특징\n",
    "    - 샘플(행)의 독립성: 행의 순서가 바뀌어도 의미 불변\n",
    "    - 특성(열) 순서의 독립성: ‘나이, 키’ 순서와 ‘키, 나이’ 순서의 정보는 동일\n",
    "- 순차 데이터의 특징\n",
    "    - 요소의 순서 의존성: 순서 자체가 핵심 정보\n",
    "    - 자기상관성: 이전 시점의 데이터가 다음 시점의 데이터에 영향을 미침\n",
    "- 결론: 순차 데이터의 특징(과거 정보 기억, 순서 의미 학습)을 고려하기 위해 특별한 신경망 필요\n",
    "\n",
    "## 1-2. RNN의 구조와 원리\n",
    "\n",
    "### RNN(Recurrent Neural Network)의 정의\n",
    "- 순환하는 구조를 가진, 순차 데이터 처리에 특화된 인공 신경망 (순환 신경망)\n",
    "- RNN의 핵심\n",
    "    - 1. 시퀀스 데이터를 입력받아 순서 정보를 유지하며 처리\n",
    "    - 2. 순환하는 은닉층이 매 시점의 은닉 상태를 업데이트\n",
    "        - 은닉층 (Hidden Layer): 물리적 요소 (가중치, 활성화 함수 포함). 정보 처리 '규칙' 또는 '함수'.\n",
    "        - 은닉 상태 (Hidden State): 특정 시점(t)의 결과값 (벡터). 과거 정보를 요약한 '메모리' 또는 '문맥'.\n",
    "    - 3. 이전 시점의 값을 현재 시점으로 전달\n",
    "\n",
    "### RNN의 구조\n",
    "- 용어 설명\n",
    "    - $x_t$: 시간 스텝 t에서의 입력 (Input)\n",
    "    - $h_t$: 시간 스텝 t에서의 은닉 상태 (Hidden State)\n",
    "    - $o_t$: 시간 스텝 t에서의 출력 (Output)\n",
    "    - $U$ (입력 가중치): $x_t$가 $h_t$에 영향을 주는 가중치 (현재 정보 처리)\n",
    "    - $V$ (순환 가중치): $h_{t-1}$이 $h_t$에 영향을 주는 가중치 (과거 정보 전달)\n",
    "    - $W$ (출력 가중치): $h_t$가 $o_t$에 영향을 미치는 가중치 (결과 생성)\n",
    "- 기존 신경망과의 차이점: V(순환 가중치)의 유무\n",
    "- 정보 흐름\n",
    "    - $t$ 시점에 새로운 입력 $x_t$와 전달받은 과거 정보 $h_{t-1}$을 함께 사용\n",
    "    - 현재 시점의 은닉 상태 $h_t$를 업데이트하고 출력 $o_t$를 계산\n",
    "    - 결론: 은닉 상태(Hidden State)의 순환을 통해 과거 정보를 다음 시점으로 계속 전달\n",
    "- RNN 아키텍처\n",
    "    - 1. One to One: Vanilla RNN\n",
    "    - 2. One to Many: 이미지 캡셔닝\n",
    "    - 3. Many to One: 감정 분석, 시계열 데이터 예측\n",
    "    - 4. Many to Many (Delayed): 기계 번역\n",
    "    - 5. Many to Many (Real-time): Video recognition\n",
    "\n",
    "### RNN의 핵심 원리: 가중치 공유\n",
    "- 정의: 동일한 가중치(U, V, W)를 시퀀스의 모든 시점에서 반복적으로 적용\n",
    "- $X_1$, $X_2$, ... $X_n$을 처리할 때까지 모두 동일한 파라미터(U, W, b)를 공유\n",
    "- 가중치 공유의 이점\n",
    "    - 1. 학습 파라미터의 수 감소\n",
    "        - 시퀀스 길이가 길어져도 학습해야 할 파라미터 수가 일정하게 유지됨\n",
    "        - 모델이 더 빠르고 가벼워짐\n",
    "    - 2. 일반화 능력 향상\n",
    "        - '데이터를 처리하는 규칙'을 각각 배우는 것이 아니라, 하나의 일반적 규칙을 학습\n",
    "        - 훈련 데이터에 없던 새로운 길이의 시퀀스나 다양한 패턴에 유연하게 대응 가능\n",
    "\n",
    "## 1-3. RNN의 한계 및 장기 의존성 문제 (The Long-Term Dependency Problem)\n",
    "\n",
    "### RNN의 한계점\n",
    "- 역전파 과정에서 chain rule에 의해 미분값이 반복적으로 곱해짐 (등비수열 형태)\n",
    "- 기울기 소실 (Vanishing Gradients)\n",
    "    - 정의: 시퀀스 뒤쪽의 오차가 앞쪽까지 제대로 전달되지 않아 먼 과거의 정보를 학습하지 못하는 현상\n",
    "    - 원인: 반복적으로 곱해지는 기울기(미분값)의 크기가 1보다 작을 때 (기울기가 0으로 수렴)\n",
    "    - 결과: 중요한 과거 정보를 반영할 수 없게 됨\n",
    "- 기울기 폭주 (Exploding Gradients)\n",
    "    - 정의: 오차가 역전파 과정에서 비정상적으로 커지는 문제\n",
    "    - 원인: 반복적으로 곱해지는 기울기(미분값)의 크기가 1보다 클 때 (기울기가 무한대로 발산)\n",
    "    - 결과: 가중치가 비정상적으로 큰 값으로 업데이트되어 학습 불능 상태 (NaN, Infinity) 발생\n",
    "    - 해결 방법: Gradient Clipping (기울기의 상한선(threshold)을 정해 발산 방지)\n",
    "- 느린 훈련 시간\n",
    "    - 원인: 계산 과정이 순차적으로 이루어져야 하는 구조적 한계 ($h_t$ 계산을 위해 $h_{t-1}$이 필요)\n",
    "    - 결과: 전체 시퀀스를 한 번에 병렬 처리 불가능\n",
    "\n",
    "### 장기 의존성 문제 (Long-Term Dependency Problem)\n",
    "- 정의: RNN의 구조적 한계로 인해, 시퀀스 앞 부분의 중요한 정보를 잊어버려 맥락 파악 능력이 급격히 저하되는 현상\n",
    "- '단기 기억력'은 준수하나, '장기 기억력'에 문제 발생\n",
    "- 예시: 문장이 길어질수록 문장 첫 부분의 \"모스크바\"라는 핵심 정보를 잊어버려 다음 단어 예측이 어려워짐\n",
    "- 결론: RNN의 기억력 한계를 극복하고, 중요한 정보는 오래 기억하는 새로운 모델(LSTM, GRU)의 필요성 대두\n",
    "\n",
    "# 2. 똑똑하게 기억하고 잊는 법: LSTM & GRU\n",
    "\n",
    "## 2-1. 게이트(Gate)로 정보의 흐름을 제어하다: LSTM\n",
    "\n",
    "### LSTM (Long Short Term Memory)\n",
    "- 정의: RNN의 기울기 소실 문제(장기 의존성 문제)를 해결하기 위한 모델 구조\n",
    "- RNN 구조와의 비교\n",
    "    - RNN은 $h_t$ (은닉 상태) 1개의 층만 순환\n",
    "    - LSTM은 $c_t$ (셀 상태)와 $h_t$ (은닉 상태) 2개의 순환되는 층을 사용\n",
    "        - $c_t$ (Cell State): 장기 기억\n",
    "        - $h_t$ (Hidden State): 단기 기억\n",
    "- 핵심 아이디어: \"GATE\"를 통해 기억할 내용과 잊어버릴 내용을 선택\n",
    "\n",
    "### LSTM의 구조 (핵심)\n",
    "- 1. Forget gate (망각 게이트): 과거 정보를 얼마나 잊어버릴지 결정\n",
    "    - $h_{t-1}$과 $x_t$를 받아 sigmoid를 통과 (0~1 사이 값)\n",
    "    - 0에 가까울수록 많이 잊고, 1에 가까울수록 많이 기억\n",
    "- 2. Input gate (입력 게이트): 현재 정보를 얼마나 사용할지(기억할지) 결정\n",
    "    - sigmoid 함수가 현재 입력 $x_t$를 얼마나 사용할지 결정 (0~1)\n",
    "    - tanh 함수가 새로운 후보값 벡터($\\tilde{C}_t$)를 생성\n",
    "- 3. Final memory cell (Cell State 업데이트)\n",
    "    - 이전 값($C_{t-1}$)을 얼마나 잊을지(f)와 이번 입력값($\\tilde{C}_t$)을 얼마나 기억할지(i)를 조합하여 현재 셀 상태 $C_t$를 업데이트\n",
    "- 4. Output gate (출력 게이트): 다음 층으로 어떤 정보를 전달할지 결정\n",
    "    - 업데이트된 $C_t$와 sigmoid/tanh를 거쳐 현재 시점의 은닉 상태(단기 기억) $h_t$를 생성\n",
    "\n",
    "### LSTM의 장점 & 단점\n",
    "- 장점: Vanishing Gradient Problem(기울기 소실 문제)을 효과적으로 완화\n",
    "    - Gate를 통해 중요한 정보는 잊지 않고(f=1) 보존하여 전달 가능\n",
    "- 단점: 복잡한 구조로 인해 RNN보다 학습 파라미터가 많아지고 계산 비용이 높음\n",
    "\n",
    "## 2-2. 더 단순하고 효율적인 구조: GRU (Gated Recurrent Unit)\n",
    "\n",
    "### GRU (Gated Recurrent Units)\n",
    "- 정의: LSTM의 구조를 메모리 셀 없이 게이트 수를 줄여 간소화한 모델\n",
    "\n",
    "### GRU의 구조 (LSTM과의 차이점)\n",
    "- 1. Forget gate와 Input gate를 **Update gate** 하나로 통합\n",
    "    - Update gate($z$): 과거 정보와 현재 정보의 반영 비율을 결정 (z와 1-z로 조절)\n",
    "- 2. **Reset gate** 사용\n",
    "    - Reset gate($r$): 과거 정보를 얼마나 무시할지 결정\n",
    "- 3. Cell State($c_t$)가 없고, Hidden State($h_t$) 하나로 장기/단기 기억 모두 관리\n",
    "- 4. 게이트 수가 3개(LSTM)에서 2개(GRU)로 줄어듦\n",
    "    - 학습 파라미터가 적어 계산 효율이 높고 학습 시간이 단축됨\n",
    "\n",
    "### GRU의 장점 & 단점\n",
    "- 장점: LSTM과 유사하거나 더 좋은 성능을 보이면서도, 구조가 단순하고 계산 효율성이 좋음\n",
    "- 단점: 긴 시퀀스 처리에서의 한계는 여전히 존재할 수 있음\n",
    "\n",
    "## 2-3. LSTM vs GRU: 언제 무엇을 선택할까?\n",
    "- 특정 모델이 절대적으로 우수하지 않으며, 상황에 맞는 선택이 중요\n",
    "- LSTM (Long Short-Term Memory)\n",
    "    - 구조: 3개 게이트 (입력, 삭제, 출력), 2개 상태 (Cell, Hidden)\n",
    "    - 특징: 더 복잡, 파라미터 많음, 학습 속도 느림\n",
    "    - 적합한 경우: 데이터가 매우 풍부하고, 시퀀스가 매우 길며, 복잡한 패턴 학습이 필요할 때 (예: 기계 번역)\n",
    "- GRU (Gated Recurrent Unit)\n",
    "    - 구조: 2개 게이트 (업데이트, 리셋), 1개 상태 (Hidden)\n",
    "    - 특징: 단순, 파라미터 적음, 학습 속도 빠름\n",
    "    - 적합한 경우: 데이터가 적거나, 계산 자원이 제한적이거나, 빠른 학습 속도가 필요할 때 (예: 실시간 음성 인식)\n",
    "- 결론: 일반적으로 실험을 통해 두 모델을 비교 후 선택\n",
    "\n",
    "# 3. 문장을 입력받아 문장을 출력하다: Seq2Seq\n",
    "\n",
    "## 3-1. Seq2Seq의 기본 구조: 인코더와 디코더\n",
    "\n",
    "### Seq2Seq (Sequence-to-Sequence)\n",
    "- 정의: 한 시퀀스를 다른 시퀀스로 변환하는 작업을 수행하는 딥러닝 모델\n",
    "- 주 사용처: 기계 번역, 챗봇, 요약\n",
    "- 특징 1: 입력 아이템(단어)의 개수와 출력 아이템의 개수가 달라도 됨\n",
    "- 특징 2: 인코더(Encoder)와 디코더(Decoder)라는 두 모듈로 구성됨 (Encoder-Decoder 모델)\n",
    "\n",
    "### 인코더와 디코더\n",
    "- 1. 인코더 (Encoder)\n",
    "    - 역할: 입력된 시퀀스(원본 문장)를 읽고 압축하여 디코더에게 넘겨줄 문맥 정보(Context Vector)를 준비\n",
    "    - 작동: t 시점의 입력 단어와 t-1 시점의 은닉 상태를 RNN/LSTM/GRU에 입력하여 t 시점의 은닉 상태를 생성\n",
    "- 2. 디코더 (Decoder)\n",
    "    - 역할: 인코더가 압축한 정보(Context Vector)를 바탕으로 원하는 시퀀스(번역 문장)를 생성\n",
    "    - 작동: 인코더의 Context Vector를 디코더의 첫 번째 은닉 상태로 사용. t 시점에 나온 출력값이 t+1 시점의 입력값으로 사용됨\n",
    "- 인코더와 디코더 유닛: RNN, LSTM, GRU 등을 여러 개 조합한 형태\n",
    "\n",
    "### 컨텍스트 벡터 (Context Vector)\n",
    "- 정의: 인코더의 마지막 시점 은닉 상태(hidden state)\n",
    "- 역할: 입력 시퀀스(입력 문장) 전체의 의미 정보를 요약한 벡터\n",
    "- 특징: float형의 벡터 (예: 256, 512, 1024 차원)\n",
    "- 정보 흐름: 인코더의 마지막 은닉 상태(Context Vector)가 디코더의 첫 번째 은닉 상태로 전달됨\n",
    "\n",
    "## 3-2. Seq2Seq의 한계: 병목 현상 (Bottleneck)\n",
    "\n",
    "### 병목 현상 (Bottleneck) 정의\n",
    "- 문제점: 인코더가 입력 시퀀스의 모든 정보를 **고정된 길이의 Context Vector** 하나에 압축해야 함\n",
    "- 결과: 입력 시퀀스가 길어질 경우, 모든 정보를 하나의 벡터에 담지 못하고 정보 손실이 발생\n",
    "- 예시: 짧은 문장(\"나는 사과를 먹는다\")과 긴 문단(수필)을 모두 \"단어 3개\"로 요약해야 할 때, 긴 문단의 정보는 대부분 손실됨\n",
    "- 결론: 고정된 크기의 Context Vector가 정보 전달의 병목이 되어 모델 성능이 하락\n",
    "\n",
    "### 그 외 한계점\n",
    "- 1. 기울기 소실 문제 (Vanishing Gradient): LSTM/GRU로 완화했으나, 시퀀스가 매우 길어지면 구조적 한계로 인해 여전히 발생 가능\n",
    "- 2. 병렬화 불가능 문제: RNN 기반이므로 순차적 처리 필요. 학습 시간이 길어짐\n",
    "- 해결책: 어텐션(Attention) 메커니즘 도입"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2ad92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
