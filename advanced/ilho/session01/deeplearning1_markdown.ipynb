{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7651106",
   "metadata": {},
   "source": [
    "### 1. 인공신경망의 시작: 퍼셉트론\n",
    "\n",
    "#### 1-1. 인공신경망이란? (Artificial Neural Network)\n",
    "\n",
    "* **인공신경망(ANN)**\n",
    "    * 정의: 뇌의 신경망(뉴런) 구조를 모방한 머신러닝 모델.\n",
    "    * 생물학적 뉴런: 신호 수용(가지돌기) → 신호 합산(세포체) → 기준(역치) 초과 시 신호 전달(축삭돌기).\n",
    "    * 인공 뉴런: 입력($x_n$) 수신 → 가중치($w_n$) 적용 → 가중합 계산 → 활성화 함수 통과 → 출력.\n",
    "* **퍼셉트론 (Perceptron)**\n",
    "    * 정의: 초기 형태의 인공신경망.\n",
    "    * 작동 절차: 입력 → 가중합 → 활성화 함수 → 출력.\n",
    "    * 학습(Training): 원하는 출력값을 내보내도록 가중치를 조정하는 작업.\n",
    "* **주요 용어**\n",
    "    * **가중치(Weight)**: 입력값이 출력값에 주는 영향력(중요도).\n",
    "    * **활성화 함수(Activation Function)**: 가중합이 특정 기준(임계값)을 넘는지 여부에 따라 출력 신호를 결정.\n",
    "        * 초기: 계단 함수(Step Function).\n",
    "        * 발전: Sigmoid, Tanh, **ReLU**(현재 주력), Leaky ReLU 등.\n",
    "    * **임계값(Threshold, $\\theta$)**: 활성화 기준치.\n",
    "    * **편향(Bias)**: 임계값을 수식($b = -\\theta$)으로 표현. 현재 딥러닝에서 보편적으로 사용.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1-2. 단층 퍼셉트론과 그 한계 (Single-Layer Perceptron)\n",
    "\n",
    "* **단층 퍼셉트론(SLP)**\n",
    "    * 구조: 입력층과 출력층, 2개의 층으로만 구성.\n",
    "    * 기능: **선형 분리(Linear Separation)** 가능한 문제 해결 (예: AND, NAND, OR 게이트).\n",
    "* **한계**\n",
    "    * 문제점: **XOR 게이트**와 같은 **비선형** 문제 해결 불가능.\n",
    "    * 이유: SLP는 '직선' 하나로만 영역을 나눌 수 있는 선형 분류기임.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1-3. 다층 퍼셉트론 (Multi-Layer Perceptron, MLP)\n",
    "\n",
    "* **다층 퍼셉트론(MLP)**\n",
    "    * 목적: SLP의 한계(XOR 문제) 극복.\n",
    "    * 구조: 입력층과 출력층 사이에 1개 이상의 **은닉층(Hidden Layer)**을 추가.\n",
    "    * 기능: 은닉층을 통해 비선형 영역 분리 가능. (XOR을 NAND, OR 게이트 조합으로 해결)\n",
    "* **딥러닝으로의 발전**\n",
    "    * **심층 신경망 (DNN, Deep Neural Network)**: 은닉층이 2개 이상인 신경망 (MLP 확장).\n",
    "    * **딥러닝 (Deep Learning)**: 심층 신경망을 학습시키는 기술.\n",
    "    * 포함 관계: 퍼셉트론 ⊂ MLP ⊂ DNN ⊂ ANN.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 딥러닝 모델은 어떻게 학습할까?\n",
    "\n",
    "#### 2-1. 순전파와 역전파 (Forward & Backward Propagation)\n",
    "\n",
    "* **딥러닝 학습 알고리즘 기본 구조 (반복)**\n",
    "    1.  **(1) 순전파 (Forward Propagation)**: 입력층 → 출력층 방향으로 예측값 계산. (비유: 문제 풀이)\n",
    "    2.  **(2) 오차 계산 (손실 함수)**: 예측값과 실제값 비교. (비유: 채점)\n",
    "    3.  **(3) 역전파 (Backward Propagation)**: 계산된 오차를 출력층 → 입력층 방향으로 전달. (비유: 오답 원인 분석)\n",
    "    4.  **(4) 가중치 업데이트 (옵티마이저)**: 오차를 줄이는 방향으로 가중치 수정. (비유: 오답노트 기반 재학습)\n",
    "* **연쇄 법칙 (Chain Rule)**\n",
    "    * 정의: 합성 함수의 미분법. 역전파 과정에서 각 가중치가 최종 오차에 얼마나 기여했는지(기울기) 계산하는 핵심 원리.\n",
    "    * **기울기 소실 문제 (Vanishing Gradient)**: 층이 깊어질수록, 역전파 시 기울기가 0에 가까워져 초기 층의 가중치 학습이 멈추는 현상. (주로 Sigmoid 함수 등에서 발생)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2-2. 손실 함수 (Loss Function)\n",
    "\n",
    "* **정의**: 모델의 **예측값**과 **실제값** 사이의 차이(오차)를 수치화하는 함수.\n",
    "* **목적**: 딥러닝 학습은 이 손실 함수의 값을 최소화하는 방향으로 가중치를 찾는 과정.\n",
    "* **주요 종류**\n",
    "    * **평균 제곱 오차 (MSE)**: **회귀 문제**(연속값 예측, 예: 집값)에 사용.\n",
    "    * **이진 크로스 엔트로피 (BCE)**: **이진 분류**(0 또는 1, 예: 스팸/정상)에 사용.\n",
    "    * **크로스 엔트로피 오차 (CEE)**: **다중 클래스 분류**(여러 개 중 하나, 예: 숫자 0~9)에 사용. (Softmax 함수와 짝)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2-3. 활성화 함수 (Activation Function)\n",
    "\n",
    "* **정의**: 뉴런의 가중합(선형 값)을 **비선형 값**으로 변환하여 다음 층으로 전달.\n",
    "* **목적**: 모델에 비선형성을 부여. (활성화 함수가 없으면 층을 아무리 깊게 쌓아도 선형 모델과 동일)\n",
    "* **주요 종류**\n",
    "    * **계단 함수**: 단층 퍼셉트론에서 사용 (0 또는 1).\n",
    "    * **시그모이드 (Sigmoid)**: (0~1) 출력. (단점: 기울기 소실 문제 유발).\n",
    "    * **ReLU (Rectified Linear Unit)**\n",
    "        * 특징: 입력이 0 이하면 0, 0보다 크면 입력값 그대로 출력.\n",
    "        * 장점: **기울기 소실 문제 완화**, 계산 속도 빠름.\n",
    "        * 용도: 현재 딥러닝 **은닉층**에서 가장 보편적으로 사용.\n",
    "    * **Tanh (쌍곡탄젠트)**: (-1~1) 출력.\n",
    "    * **Softmax**:\n",
    "        * 특징: 출력값들의 총합이 1이 되도록 각 클래스에 대한 '확률'로 변환.\n",
    "        * 용도: **다중 클래스 분류** 문제의 **출력층**에서 사용.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2-4. 경사 하강법과 옵티마이저 (Gradient Descent & Optimizer)\n",
    "\n",
    "* **경사 하강법**: 손실 함수를 최소화하기 위해, 손실 함수의 기울기(경사) 반대 방향으로 가중치를 점진적으로 업데이트하는 방법.\n",
    "* **학습 단위**\n",
    "    * **배치 (Batch)**: 학습 시 한 번에 처리하는 데이터 묶음.\n",
    "    * **미니 배치 경사 하강법 (Mini-Batch GD)**: 데이터를 N개(배치 크기)씩 묶어 학습. (속도와 안정성의 균형, 현재 표준 방식)\n",
    "    * **이포크 (Epoch)**: 전체 데이터셋을 1회 학습(순회)한 횟수.\n",
    "* **옵티마이저 (Optimizer)**: 경사 하강법을 더 효율적이고 안정적으로 수행하기 위한 알고리즘 (가중치 업데이트 전략).\n",
    "    * **모멘텀 (Momentum)**: '관성'을 적용, 이전 업데이트 방향을 참고하여 지역 최솟값(Local minimum) 탈출 도움.\n",
    "    * **RMSProp**: 학습률(보폭)을 동적으로 조절.\n",
    "    * **Adam (Adaptive Moment Estimation)**: 모멘텀과 RMSProp의 장점을 결합. 현재 가장 보편적으로 사용됨.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2-5. 학습을 향상시키는 기법: 데이터 증강과 전이 학습\n",
    "\n",
    "* **데이터 증강 (Data Augmentation)**\n",
    "    * 정의: 기존 학습 데이터(특히 이미지)에 회전, 반전, 확대/축소 등 인위적 변형을 가해 데이터 양을 늘리는 기법.\n",
    "    * 목적: 모델의 일반화 성능 향상, **과적합(Overfitting) 방지**.\n",
    "* **전이 학습 (Transfer Learning)**\n",
    "    * 정의: 대규모 데이터셋으로 미리 학습된 강력한 모델(**Pre-trained Model**)을 가져와, 새로운 (상대적으로 작은) 데이터셋 문제에 맞게 활용하는 기법.\n",
    "    * **미세 조정 (Fine-Tuning)**: 사전 학습 모델을 기반으로, 새로운 데이터에 맞게 모델 일부 또는 전체를 재학습.\n",
    "    * 장점: **학습 시간 및 비용 절감**, 적은 데이터로도 높은 성능 확보.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 합성곱 신경망 (CNN, Convolutional NN)\n",
    "\n",
    "* **이미지 데이터**: 픽셀(Pixel)의 집합. (흑백: H x W x 1, 컬러: H x W x 3 채널).\n",
    "* **MLP의 이미지 처리 한계**\n",
    "    1.  **공간 정보 손실**: 2D/3D 이미지를 1차원 벡터로 펼치면서 위치 정보 상실.\n",
    "    2.  **파라미터 폭증**: 모든 픽셀과 뉴런을 완전 연결(FC)하여 가중치 수가 기하급수적으로 증가 (과적합 위험).\n",
    "    3.  **위치 변화 취약**: 객체 위치가 조금만 바뀌어도 완전히 다른 입력으로 인식.\n",
    "* **CNN (합성곱 신경망)**\n",
    "    * 정의: 이미지의 공간 정보(Spatial Info)를 유지한 채 학습하는 데 특화된 딥러닝 모델.\n",
    "    * 구조: [**특징 추출 영역**: (Conv + Pool) 반복] → [**분류 영역**: (Flatten + FC Layer)]\n",
    "\n",
    "---\n",
    "\n",
    "#### 3-1. 왜 이미지 처리에 MLP가 아닌 CNN을 사용할까?\n",
    "\n",
    "* (요약) CNN은 MLP와 달리,\n",
    "    1.  **공간 정보 보존**: 이미지를 2D/3D 그대로 입력받아 처리.\n",
    "    2.  **파라미터 효율성**: 필터(커널)가 이미지를 훑으며 **가중치를 공유**(Parameter Sharing)하고, **부분 연결**(Local Connectivity)을 통해 파라미터 수 대폭 감소.\n",
    "    3.  **위치 변화 둔감**: 풀링(Pooling) 등을 통해 객체의 위치가 다소 변해도 동일한 특징으로 인식 가능.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3-2. 합성곱(Convolution)과 풀링(Pooling)\n",
    "\n",
    "* **[특징 추출 영역]**\n",
    "* **합성곱 층 (Convolution Layer)**:\n",
    "    * **필터 (Filter / 커널)**: 이미지의 특정 특징(수직선, 수평선, 질감 등)을 감지하는 작은 가중치 행렬.\n",
    "    * **합성곱 연산**: 필터가 이미지 위를 이동(Stride)하며, 픽셀 값과 곱해져 **특징 맵(Feature Map)**(활성화 맵) 생성.\n",
    "    * **패딩 (Padding)**: 이미지 가장자리에 0 값을 추가하여, 외곽 특징 정보 손실 방지 및 출력 크기 조절.\n",
    "    * **스트라이드 (Stride)**: 필터의 이동 보폭(칸 수).\n",
    "* **풀링 층 (Pooling Layer)**:\n",
    "    * 목적: 특징 맵의 크기를 줄여(Downsampling) 계산량을 감소시키고, 과적합 방지.\n",
    "    * 동작: 특징 맵을 일정 영역으로 나눠, 주요 값만 추출.\n",
    "    * 종류: **Max Pooling**(영역 내 최댓값 추출, 주로 사용), Average Pooling(평균값 추출).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3-3. 대표적인 CNN 아키텍처 살펴보기\n",
    "\n",
    "* **AlexNet (2012)**\n",
    "    * 최초로 GPU를 활용한 딥러닝 모델, 딥러닝 붐 야기.\n",
    "    * 특징: 8층 구조, **ReLU** 활성화 함수, **Dropout**(과적합 방지) 도입.\n",
    "* **VGGNet (2014)**\n",
    "    * 특징: 3x3의 작은 필터를 반복적으로 쌓아 층의 깊이(16, 19층)를 늘림.\n",
    "    * 구조가 단순하여 전이 학습의 기반(Baseline) 모델로 많이 사용됨.\n",
    "* **ResNet (2015)**\n",
    "    * 특징: **잔차 연결(Residual Connection / Skip Connection)** 도입.\n",
    "    * 입력값을 출력값에 더해주는 '지름길'을 만들어, **기울기 소실 문제**를 획기적으로 해결.\n",
    "    * 100층 이상의 매우 깊은 신경망 학습을 가능하게 함 (예: ResNet-152).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 이미지 딥러닝 응용\n",
    "\n",
    "#### 4-1. 이미지 분류 (Image Classification)\n",
    "\n",
    "* 아이디어: 이미지를 입력받아, 사전 정의된 클래스(예: '개', '고양이') 중 하나로 분류.\n",
    "* 모델: AlexNet, VGG, ResNet, Vision Transformer (ViT) 등.\n",
    "\n",
    "#### 4-2. 객체 탐지 (Object Detection)\n",
    "\n",
    "* 아이디어: 이미지 내 객체의 **위치**(경계 상자, Bounding Box)와 **클래스**를 동시에 찾아내는 작업.\n",
    "* 모델: YOLO, Faster R-CNN, SSD 등.\n",
    "\n",
    "#### 4-3. 이미지 캡셔닝 (Image Captioning)\n",
    "\n",
    "* 아이디어: 이미지의 내용을 자연어 문장으로 설명.\n",
    "* 모델 적용:\n",
    "    1.  **CNN** (예: ResNet): 이미지의 특징(Feature) 추출.\n",
    "    2.  **RNN** (순환 신경망): 추출된 특징을 기반으로 순차적인 문장 생성."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2ad92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
