{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f813b7c4",
   "metadata": {},
   "source": [
    "# 군집화 사전과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22851f",
   "metadata": {},
   "source": [
    "## 군집화 <br>\n",
    "#### <span style='background-color: #fff5b1'>1. 머신러닝: 비지도 학습</span>   \n",
    "- 인공지능의 한 분야   \n",
    "- 컴퓨터가 스스로 학습하도록 하는 알고리즘 or 기술 개발   \n",
    "→ 데이터 예측 or 의사결정을 돕는 기술   \n",
    "<BR>  \n",
    "\n",
    "|  | 정답 | 방식 | 예시 |   \n",
    "| ---- | ---- | ---- | ---- |  \n",
    "| 지도 학습 | 주어짐 | 기존의 정답과 예측값이 같아지도록 기계 학습 | 회귀, 분류 |   \n",
    "| <u>비지도 학습</u> | 주어지지 않음 | 데이터 속 패턴 or 데이터 간 유사도를 기계 학습 | **군집화** |    \n",
    "\n",
    "**비지도 학습**   \n",
    "- 데이터에 내재된 구조 파악 후 학습   \n",
    "- 정답 레이블 X → <u>에이전트</u>가 해야 하는 작업 정의 X → 모델 성능 명확히 측정 X   \n",
    "- 표현학습 [피처학습]으로 데이터셋의 고유 패턴 식별    \n",
    "=> 목표가 명확하지 않고, 데이터에서 새로운 특성을 찾고자 할 때 적합    \n",
    "\n",
    "**비지도 학습이 적합한 케이스**   \n",
    "(1) 패턴이 알려지지 않은 경우    \n",
    "(2) 패턴이 계속 변하는 경우    \n",
    "(3) 열린 문제를 해결하고 지식을 일반화해야 하는 경우    \n",
    "ex. 고객 세그먼트 분석, 이상 거래 탐지, 자연어 처리 등   \n",
    "<br>   \n",
    "\n",
    "**<u>에이전트</u>   \n",
    "- 인간 개입 없이 작업을 수행하는 SW 시스템   \n",
    "ex. 자율 주행 자동차, 로봇 등     \n",
    "- 지도 학습 환경에서 정답을 바탕으로 학습 & 행동   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181995c",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 2. 군집화</span>   \n",
    "= 종속 변수를 따로 설정하지 않고, 데이터 내부의 패턴을 인식해 비슷한 특성을 가진 그룹으로 나누는 비지도 학습 기법    \n",
    "=> 각 그룹의 구성 파악하며 데이터 구조 이해   \n",
    "- 군집 = 유사한 데이터의 집합    \n",
    "\n",
    "**목표**   \n",
    "=> 데이터 간 유사성을 최대한 잘 유지하며, 서로 다른 그룹은 구분될 수 있도록    \n",
    "(1) 응집도 최대화: 같은 군집에 속하는 데이터끼리 최대한 비슷하도록   \n",
    "(2) 분리도 최대화: 서로 다른 군집은 최대한 분리되도록   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f48000",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 3. 군집화 과정</span>   \n",
    "(1) **피처 선택 / 추출**: 군집화에 사용할 데이터의 피처 선택   \n",
    "(2) **군집화 알고리즘 선택**: 데이터 특성과 목표에 맞는 군집화 알고리즘 선택   \n",
    "(3) **군집 유효성 검증**: 군집화 성능 평가   \n",
    "(4) **결과 해석**: 각 군집의 특성 분석 및 해석   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac735fc8",
   "metadata": {},
   "source": [
    "## 군집화를 위한 데이터 준비<br>\n",
    "#### <span style='background-color: #fff5b1'> 1. 주요 고려사항</span>   \n",
    "(1) **변수 유형 이해**   \n",
    "- 변수 종류와 개수에 따라 군집화 알고리즘이 달라지기 때문에 사용할 피처의 종류와 특성 명확히 이해  \n",
    "\n",
    "(2) **거리 / 유사도 정의와 측정**    \n",
    "- 군집화는 데이터 간 거리 / 유사도 기반으로 그룹을 형성하기 때문에 거리 측정 방법 정의가 중요    \n",
    "\n",
    "(3) **차원 축소**    \n",
    "- 모델 성능 향상을 위해 유사한 변수를 묶어 처리하는 방식   \n",
    "- 변수가 많을수록 모델 복잡성↑ 효율성↓ 성능↓ => 유사한 변수 통합, 필요 없는 변수 제거해 변수 개수 감소   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9089bb",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 2. 변수 유형</span>   \n",
    "**연속형 변수**    \n",
    "- 수치로 측정 가능한 변수 ex. 키, 몸무게, 나이, 소득 등   \n",
    "- 거리 측정: 유클리디안 거리, 맨하탄 거리 등   \n",
    "- 적합한 알고리즘: K-means, 계층적 군집화 등     \n",
    "- 스케일링 필수   \n",
    "\n",
    "**명목형 변수**   \n",
    "- 범주로 구분되는 변수 ex. 성별, 지역, 직업 등   \n",
    "- 거리 측정: 해밍 거리, 자카드 거리 등   \n",
    "- 적합한 알고리즘: K-modes, 계층적 군집화    \n",
    "- 원-핫 인코딩 또는 더미 변수 변환으로 처리    \n",
    "\n",
    "**혼합형 변수**   \n",
    "- 연속형 + 명목형   \n",
    "- 적합한 알고리즘: K-prototypes, Gower distance 기반 계층적 군집화    \n",
    "- 변수 유형에 맞는 거리 함수 조합해 처리    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c87080",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 3. 거리 / 유사도 정의와 측정</span>   \n",
    "**거리 측정 방법**    \n",
    "- 데이터 포인트나 군집 간 유사성 / 비유사성을 수치로 계산하는 방법    \n",
    "- 가까울 수록 유사하고, 멀수록 다름    \n",
    "\n",
    "(1) **유클리디안 거리**: 가장 일반적인 측정법으로, 직선 거리 의미. k-means에서 사용    \n",
    "(2) **맨하탄 거리**: |각 차원의 차이|의 합으로, 이상치에 덜 민감    \n",
    "(3) **코사인 유사도**: 벡터 간 각도로, 텍스트 데이터나 고차원 데이터에 효과적   \n",
    "<br>\n",
    "\n",
    "**연결 방법**    \n",
    "- 계층적 군집화에서 군집 간 거리를 계산  \n",
    " \n",
    "(1) **단일 연결법 / 최단 연결법 (Single linkage)**   \n",
    "    = 군집 간 가장 가까운 두 점 간의 거리   \n",
    "    - 고립된 군집 발견에 중점    \n",
    "    - 한 클러스터에 이상치가 속하면, 가장 가까운 다른 클러스터와도 가까운 거리에 있게 되어 연쇄적으로 잘못된 클러스터링을 유발하는 등 이상치에 취약   \n",
    "\n",
    "(2) **완전 연결법 / 최장 연결법 (Complete linkage)**   \n",
    "    = 군집 간 가장 먼 두 점 간 거리를 계산   \n",
    "    - 군집의 한 관측값을 뽑았을 때 나타나는 거리의 최댓값으로 측정해 가장 유사성이 큰 군집으로 병합    \n",
    "    - 군집들의 내부 응집성에 중점    \n",
    "    - 이상치에 민감      \n",
    "    \n",
    "(3) **평균 연결법 (Average linkage)**    \n",
    "    = 두 군집의 모든 점 쌍 사이 거리의 평균    \n",
    "    = 모든 항목의 거리 평균    \n",
    "    - (1), (2) 보다 이상치에 덜 민감     \n",
    "    - 계산 많아질 수 있음     \n",
    "\n",
    "(4) **중심 연결법 (Centroid method)**   \n",
    "    = 군집의 중심점 간 거리 (중심 간 거리)   \n",
    "    - 두 군집 결합 시, 새로운 군집 평균은 가중 평균으로 구함   \n",
    "    - 군집의 중심을 계산하기 때문에 시간 ↑    \n",
    "    - 덴드로그램에서 거리 축이 위에서 아래로 줄어드는 inversion 발생 가능\n",
    "\n",
    "(5) **중앙 연결법 (Median)**    \n",
    "    = 두 군집을 합친 후 중심점과 기존 군집의 중심점 간 거리    \n",
    "    - 군집 간 거리 = 두 군집 내 모든 샘플의 중앙값 이므로 극단값 영향 ↓       \n",
    "    - 기하학적 구조 파악 어려움   \n",
    "\n",
    "(6) **Ward 연결법 (Ward's procedure)**   \n",
    "    - 군집 병합 후 SSE 증가분이 최소인 것 선택   \n",
    "    - SSE = $\\sum_{k}(y_k - \\hat{y_k})^2$    \n",
    "\n",
    "\n",
    "**<u>덴드로그램</u>   \n",
    "- 군집 간 계층 구조를 트리 형태 그래프로 시각화한 것   \n",
    "- 덴드로그램 상 inversion 문제: 두 클러스터가 각각의 클러스터보다 낮은 높이에서 합쳐지는 현상    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65faf6e9",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 4. 차원 축소</span>   \n",
    "**차원의 저주**    \n",
    "= 피처 공간이 너무 커 알고리즘이 데이터를 효과적 / 효율적으로 훈련할 수 없는 현상 (고차원일수록 모든 점이 비슷하게 멀어져 거리 기반 알고리즘의 분별력 ↓)    \n",
    "- 군집화 알고리즘이 거리 기반 알고리즘이 경우가 많고, 차원이 높을수록 차원의 저주 발생 ↑      \n",
    "=> 차원 축소가 꼭 필요하다   \n",
    "\n",
    "**차원 축소**    \n",
    "- 차원을 축소한다 = 변수의 개수를 줄인다    \n",
    "- 목표: <u>고차원 데이터를 저차원 공간에 투영하되, 핵심 정보는 보존</u> => 노이즈 감소시켜 패턴 학습에 유리    \n",
    "(1) 선형 투영: 선형적 데이터 투영 ex. PCA, SVD, 랜덤 투영 등    \n",
    "(2) 비선형 차원 축소 = 매니폴드 학습: 데이터 포인트 사이의 곡선 거리를 고려해 학습 ex. t-SNE, UMAP, Isomap 등    \n",
    "\n",
    "**PCA(Principal Component Analysis)**   \n",
    "- 다차원 데이터에서 정보량이 가장 큰 방향을 기준으로 데이터 투영해 차원 축소    \n",
    "- 상관관계가 높은 피처를 결합해 선형 상관관계가 없는 더 적은 피처로 데이터 표현    \n",
    "- 주성분: 원본 고차원 데이터에서 최대 분산 방향을 찾아 저차원 공간에 투영하는 과정에서 파생된 성분     \n",
    "- 주의사항: 표준화 필수, 결측값 처리, 해석 어려움, 정보 손실 가능성   \n",
    "\n",
    "(1) 주요 파라미터: n_components [주성분 개수], whiten [데이터 정규화], random_state     \n",
    "(2) 주요 메서드: fit(), transform(), fit_transform(), inverse_transform()     \n",
    "(3) 주요 속성: explained_variance_ratio_ [각 주성분이 설명하는 분산 비율], explained_variance [주성분의 실제 분산값], components_, n_components_    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689df55e",
   "metadata": {},
   "source": [
    "## 군집화 알고리즘<br>\n",
    "#### <span style='background-color: #fff5b1'> 1. 계층적 군집화</span>   \n",
    "= 데이터 간 유사성 기반으로 트리 구조를 형성하며 상향식 / 하향식 방식으로 군집 형성하는 방법    \n",
    "- 데이터셋의 관측치로 덴드로그램 만듦     \n",
    "    = 가까운 두 군집 병합 과정을 시각적으로 표현한 트리 구조    \n",
    "    - 수직축 (높이) = 병합 시 군집 간 거리 = 유사성   \n",
    "- 군집 개수 사전 설정 X, 클러스터링 종료 후 선택 가능    \n",
    "- 계산량이 많아, 소규모 데이터의 군집 수 탐색 / 클러스터링 시각적 분석에 유용    \n",
    "- 상향식: 응집형 계층적 군집화 = 병합 클러스터링  \n",
    "    - 각 샘플이 독립적 클러스터이며, 하나의 클러스터만 남을 때까지 가장 가까운 클러스터 병합   \n",
    "    => 가장 가까운 클러스터 정의에 따라 알고리즘 다양    \n",
    "    - 단일 연결 (가장 가까운 두 샘플 간 거리) | 완전 연결 (가장 먼 두 샘플 간 거리)   \n",
    "        - 완전 연결: 거리 행렬 계산 → 단일 클러스터 시작 → 가장 가까운 클러스터 병합 → 거리 행렬 업데이터 → 반복    \n",
    "- 하향식: 분리형 계층적 군집화 = 분할 클러스터링    \n",
    "    - 전체 샘플을 포함하는 클러스터에서 시작해 유사성이 낮은 데이터를 더 작은 클러스터로 분할   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a04bd",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 2. k-means</span>   \n",
    "= 데이터를 정해진 개수의 그룹으로 나누되, 각 그룹의 중심점과 가장 가까운 데이터끼리 묶는 알고리즘    \n",
    "- 프로토타입 군집화에 속함: 각 클러스터의 중심을 프로토타입으로 삼아 데이터 묶음   \n",
    "\n",
    "**k-means 단계**    \n",
    "(1) 데이터 표본 중 랜덤으로 k개의 중심점을 초기 클러스터 중심으로 선택    \n",
    "(2) 각 표본을 가장 가까운 중심점에 할당 (이때, 유클리디안 거리 제곱 사용)     \n",
    "(3) 클러스터에 할당된 표본들의 데이터 평균을 계산해 중심점 이동   \n",
    "(4) 클러스터 할당이 변하지 않거나, 허용 오차나 최대 반복 횟수에 도달할 때까지 2-3번 반복   \n",
    "\n",
    "- 유클리디안 거리 지표를 사용하면 큰 수치를 가진 피처에 너무 큰 영향을 받기 때문에, 실제 데이터에 ㅈㄱ용할 때는 특성이 같은 스케일로 측정되었는지 확인해야 함    \n",
    "<br>\n",
    "\n",
    "**k-means 장단점**    \n",
    "\n",
    "(+)   \n",
    "- 직관적이고 구현이 쉬움    \n",
    "- 대용량 데이터에 적용 가능    \n",
    "\n",
    "(-)\n",
    "- 초기 중심점에 민감: 랜덤하게 할당하는데, 이에 따라 최종 군집화 품질 / 알고리즘 성능에 영향      \n",
    "- 군집 수 결정 어려움: 사용자가 k값 지정하는데, 결과에 큰 영향 => 엘보우 방법, 실루엣 계수로 k값 탐색   \n",
    "- 이상치에 민감: 평균 중심으로 군집을 구성해, 이상치가 중심을 왜곡할 가능성   \n",
    "- 기하학적 모양의 군집 파악 어려움   \n",
    "- 비어있는 클러스터 존재 가능 => scikitlearn의 k-means에서는 이미 해결책 마련함   \n",
    "\n",
    "**k-means++ 알고리즘**    \n",
    "= k-means에서 초기 중심점을 서로 멀리 떨어진 곳에 위치시켜 일관되고 좋은 결과를 도출하는, 개선된 초기화 방법   \n",
    "- 초기 중심점이 서로 멀리 떨어진 곳에 위치   \n",
    "\n",
    "**엘보우 방법**    \n",
    "= 클래스 내 SSE로 그래프를 활용해 최적 클러스터 개수인 k 추정하는 방법    \n",
    "- 왜곡이 빠르게 감소하는 지점[그래프의 기울기가 급격히 변하는 지점]의 k 값을 찾아 최적 클러스터 개수로 추정    \n",
    "<br>\n",
    "\n",
    "(+)   \n",
    "- 시각적으로 k값 파악 가능   \n",
    "- 반복문과 그래프만으로 간단하게 구현 가능   \n",
    "\n",
    "(-)   \n",
    "- 주관 개입: 그래프가 완만해지는 지점을 결정할 때    \n",
    "- 모호한 겨우: 데이터에 따라 엘보우 지점이 명확하지 않을 가능성    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942af00",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 3. DBSCAN</span>   \n",
    "= 밀도가 높은 지역의 데이터를 하나의 군집으로 묶고, 밀도 기준을 만족하지 못하면 군집에 포함하지 않는 알고리즘    \n",
    "=> '특정 거리에 존재해야 하는 데이터 포인트의 최소 개수' 인 밀집도에 따라 군집화 진행   \n",
    "- 명시적으로 이상치 지정해 클러스터링에서 제외   \n",
    "- 다른 알고리즘 대비 이상치에서 자유로움   \n",
    "- 어떤 데이터가 여러 군집의 특정 거리 안에 있으면 가장 밀도가 높은 군집과 그룹화함   \n",
    "\n",
    "**주요 단계**   \n",
    "(1) 샘플 분류: 핵심 샘플 / 경계 샘플 / 잡음 샘플    \n",
    "(2) 클러스터 생성: 개별 핵심 샘플 / epsilon 이내에 있는 핵심 샘플을 연결한 그룹을 클러스터로 생성   \n",
    "(3) 경계 샘플 할당: 해당 핵심 샘플 클러스터에 할당    \n",
    "\n",
    "**장단점**   \n",
    "(+)   \n",
    "- 밀도에 따라 클러스터를 할당해, 기하학적 분포를 가지는 데이터도 잘 처리 가능   \n",
    "- 이상치에 둔감: 노이즈를 통해 이상치 검출 가능하기 때문    \n",
    "- 잡음 샘플 구분    \n",
    "\n",
    "(-)   \n",
    "- epsilon[이웃이 되기 위한 포인트 사이의 최대 거리]과 min_samples [한 포인트가 군집이 되기 위한 eps 거리 내 최소 포인트 개수] 설정에 많은 영향 받음    \n",
    "- epsilon 조절이 쉽지 않고, 조절을 위한 데이터셋에 대한 도메인 지식 필요   \n",
    "- 연산량이 많아 속도 느림   \n",
    "- 다른 밀도 분포를 가진 데이터의 군집 분석은 잘 못함   \n",
    "\n",
    "**HDBSCAN**   \n",
    "= 계층적 DBSCAN   \n",
    "- 밀도 기반으로 1차 군집화 후, 거리 기준으로 밀도 기반 군집 반복 연결    \n",
    "- 주요 파라미터: min_cluster_size [클러스터로 인정받기 위한 최소 샘플 수], min_samples     \n",
    "\n",
    "(+) 여러 밀도 기준으로 반복 수행하며, DBSCAN의 단일 밀도 기준 한계를 보완함    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e8060",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 4. GMM (Gaussian Mixture Model)</span>   \n",
    "= 데이터가 여러 다른 모양의 가우시안 분포로 구성되었다고 가정한 후, 각 분포를 클러스터로 인식하는 방법    \n",
    "=> 단일 분포로 표현하기 어려운 복잡한 확률 분포를 여러 개의 가우시안 분포로 합쳐서 표현   \n",
    "- 모델 기반 군집화 방식   \n",
    "    - 데이터를 생성하는 통계적 모델을 가정하고, 데이터가 그 모델에서 새성됐다는 전제 아래에서 군집화 수행   \n",
    "- 각 군집은 확률 분포로 간주   \n",
    "- 전체 데이터 분포를 여러 확률 분포의 혼합으로 모델링   \n",
    "- 데이터 생성 메커니즘 모델링 가능   \n",
    "\n",
    "**가정**    \n",
    "- 관측된 데이터는 특정 가우시안 확률 분포에 의해 생성   \n",
    "- 전체 데이터셋은 여러 개의 다변량 가우시안 분포가 섞여 있는 것   \n",
    "- 개별 데이터는 우도에 따라 k개의 가우시안 분포 중 하나     \n",
    "    - 우도 = 고정된 관측값이 어떤 확률 분포에서 어느 정도의 확률로 나타나는지에 대한 확률    \n",
    "\n",
    "**GMM 진행 과정**    \n",
    "(1) 전체 데이터셋 분포 확인   \n",
    "(2) 전체 데이터셋이 서로 다른 정규 분포 형태의 확률 분포 곡선으로 구성 되어 있다고 가정   \n",
    "(3) 전체 데이터셋을 구성하는 여러 정규분포 곡선 추출하고, 개별 데이터가 이 중 어디에 속하는지 결정 => 각 분포 = 하나의 군집  \n",
    "\n",
    "**모델의 파라미터 추정**   \n",
    "- GMM은 기댓값-최대화 알고리즘으로 모델 파라미터 추정    \n",
    "\n",
    "**장단접**     \n",
    "(+)   \n",
    "- k-means보다 유연하게 다양한 데이터셋에 적용 가능   \n",
    "- k-means가 잘 작동하지 않는 타원형 분포 / 중첩된 군집 구조에서도 성능 굿   \n",
    "\n",
    "(-)  \n",
    "- 군집화 수행 시간 ↑   \n",
    "- 가우시안 분포가 아닌 데이터일 경우, 계산 복잡도 ↑ & 성능 ↓ 가능    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5560a62b",
   "metadata": {},
   "source": [
    "## 군집화 평가 방법<br>\n",
    "#### <span style='background-color: #fff5b1'> 1. 외부 평가와 내부 평가</span>   \n",
    "**외부 평가**    \n",
    "- 데이터셋에 정답 레이블이 존재하는 경우 사용 => 군집화 결과와 레이블이 유사한지 평가    \n",
    "- ARI: RI를 보완해 우연에 의한 가능성 배제   \n",
    "\n",
    "**내부 평가**    \n",
    "- 정답 레이블이 없는 경우 => 군집 내부 응집도와 군집 간 분리도를 기반으로 품질을 정량적으로 측정   \n",
    "- 알고리즘 자체의 내부 평가 지표와 범용 내부 평가 지표 존재     \n",
    "    - k-means: SSE 최소화    \n",
    "    - GMM: 로그 우도 최대화   \n",
    "- 실루엣 계수, 던 지수   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa0b4e",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 2. 실루엣 계수</span>   \n",
    "= 클러스터 내 샘플들이 조밀하게 모여 있는 정도를 측정 => 군집의 품질 확인   \n",
    "\n",
    "조건 | 실루엣 계수 | 의미 | 평가   \n",
    "------ | ------ | ------ | ------    \n",
    "분리도>응집도 | 1에 가까움 | 샘플이 자신의 클러스터에 잘 속해 있고, 다른 클러스터와는 멀리 떨어짐 | 군집화 굿    \n",
    "분리도 = 응집도 | 0에 가까움 | 샘플이 클러스터 경계에 위치해, 군집 간 거리 모호 | 군집 간 분리 명확X    \n",
    "분리도<응집도 | -1에 가까움 | 샘플이 잘못된 클러스터에 할당되어 군집이 섞여 있음 | 군집화 잘못함   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b0ef2",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'> 3. Dunn Index</span>   \n",
    "= 클러스터 간 최소 거리 [분리도] 와 클러스터 내 최대 거리 [응집도] 비율로 클러스터링 품질을 평가하는 지표    \n",
    "= 분리도 / 응집도   \n",
    "- 인덱스 값이 클수록 군집화 결과 굿 (일반적으로 양수 값이며 무한대까지 가능)   \n",
    "- 실루엣 계수와 종합 평가 추천   \n",
    "(-)    \n",
    "- 군집 수가 많을 수록 계산 비용 증가   \n",
    "- 극단값 영향 많이 받음   \n",
    "- 이상치에 민감    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
