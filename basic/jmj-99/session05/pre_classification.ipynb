{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c7b21f",
   "metadata": {},
   "source": [
    "# 분류 사전과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac20ef",
   "metadata": {},
   "source": [
    "## 분류 <br>\n",
    "#### <span style='background-color: #fff5b1'>1. 지도 학습: <u>회귀 vs 분류</u></span>\n",
    " 구분 | 분류 | 회귀    \n",
    "------ | ------ | ------   \n",
    "예측 대상 | 범주형 변수 | 연속형 변수    \n",
    "개념 | 주어진 데이터를 활용해 정답을 잘 맞추는 함수를 찾는 문제 | 알고리즘을 활용해 기존 데이터가 어떤 레이블에 속하는지 판별하는 문제    \n",
    "예시 | 스팸 / 비스팸, 양성 / 음성 | 주택 가격 예측, 기온 예측   \n",
    "결과의 형태 | 이산적 | 연속적 (수치)   \n",
    "\n",
    "<br>\n",
    "\n",
    "#### <span style='background-color: #fff5b1'>2. 이진 분류 vs 다중 분류</span>\n",
    "\n",
    "- **이진 분류**: 예측하고자 하는 변수가 어떤 기준에 대해 T/F 값만을 가질 때   \n",
    "- **다중 분류**: 예측하고자 하는 변수가 가질 수 있는 값이 3개 이상일 때   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ccf9b",
   "metadata": {},
   "source": [
    "## <u>분류 모델</u> <br>\n",
    "#### <span style='background-color: #fff5b1'>1. 로지스틱 회귀</span>\n",
    "- 이진 분류 문제 ex. 스팸/비스팸, 양성/음성     \n",
    "- 샘플이 특정 클래스에 속할 확률을 추정해 기준에 따라 분류하는 것이 목표   \n",
    "=> 독립 변수의 선형 조합에 로지스틱 함수를 적용해 출력값을 0에서 1 사이로 변환<br>   \n",
    "**[필요성] 출력이 0과 1 사이면서 s자 형태로 그려지는 함수 이용해야 함**\n",
    "- 단순 선형 회귀가 이진 분류에 적합하지 않음. x와 y 관계 표현을 위해 직선을 사용하면 분류 작업이 제대로 동작하지 않기 때문에 s자 형태로 표현하는 함수가 필요.   \n",
    "    - **시그모이드 함수**: 출력이 0~1 사이의 값을 가지며 s자 형태로 그려지는 함수. 입력 값이 작거나 크면 천천히 변화, 중간 값에서는 급격히 변화 -> 급격한 변화 방지 & 현실적 확률 값 제공    \n",
    "    - 입력값이 커지면 1에 수렴 / 입력값이 작아지면 0에 수렴     \n",
    "    - 출력값 >= 특정값 이면 True, 이하면 False    \n",
    "    - 시그모이드 함수가 H(x) = sigmoid (wx+b) = \\sigma (wx+b) 일 때, 적합한 가중치인 w와 b를 계산해서 찾음   \n",
    "    <br>\n",
    "    - **승산** = 특정 사건이 일어날 확률 / 특정 사건이 일어나지 않을 확률 => 사건 발생 확률이 발생하지 않을 확률보다 몇 배 높은지 알 수 있음    \n",
    "    - 비선형적 결과를 선형적 결과로 변환하기 위해 로지스틱 회귀에서 승산 사용 (로지스틱 회귀는 선형 회귀 모델에 시그모이드 함수를 적용한 비선형적 모델이기 때문에 승산 사용)       \n",
    "    - 모델에 시그모이드 함수 적용 시 예측 확률 출력 -> logit 함수 (로그 변환) 적용 시 선형 관계로 변환    \n",
    "- 직선으로 분류할 경우, 값이 -(무한대) or +(무한대)와 같이 범위(0~1)를 벗어나는 값들도 가질 수 있기 때문     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b22851",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style='background-color: #fff5b1'>2. 의사결정나무 (Decision Tree)</span>\n",
    "- 조건에 따라 데이터를 분류해 최종적으로 데이터가 순수한 레이블의 집합으로 구성될 때까지 분류 반복    \n",
    "\n",
    "**구성**     \n",
    "- root node: 가장 상위에 위치한 노드     \n",
    "- parent node: 주어진 노드의 상위 노드      \n",
    "- child node: 하나의 노드에서 분리된 2개 이상의 노드      \n",
    "- binary tree: child가 최대 2개인 tree    \n",
    "- leaf nodes: child가 없는 노드 = label     \n",
    "- edge: parent와 child의 연결선     \n",
    "- height: 특정 노드에서 가장 먼 leaf node까지 경로에 있는 edge의 개수     \n",
    "- depth: root node에서 특정 노드에 도달하기 위한 edge의 개수    \n",
    "\n",
    "----    \n",
    "\n",
    "**CART 알고리즘**    \n",
    "- Classifiacation And Regression Tree = 이진분할    \n",
    "- 데이터셋을 임계값 기준으로 두 child로 나누는 알고리즘     \n",
    "    - 불순도가 낮아지는 방향으로 임계값 나눔    \n",
    "- 불순도: 분류하려는 데이터 집합에서 서로 다른 범주가 섞여 있는 정도    \n",
    "    - **지니 지수**: 불순도. 통계적 분산 정도를 정량화해 표현한 값.      \n",
    "- 당장의 지니 계수를 낮추는 판단 => 가장 효율적인 대안을 제시하지는 않음    \n",
    "\n",
    "(1) 임계값 설정       \n",
    "    - 임계값을 기준으로 데이터를 그룹화하여 나눔    \n",
    "(2) 불순도 감소 알고리즘     \n",
    "    - 클래스를 정확하게 구분하는 분류 기준을 찾아, 불순도가 낮은 쪽으로 가지 형성      \n",
    "\n",
    "----   \n",
    "**학습 시 고려사항**     \n",
    "    (1) 하이퍼파라미터 설정   \n",
    "    - min_samples_split: 분할을 위해 노드가 가져야 하는 최소 샘플 수     \n",
    "    - min_samples_leaf: 리프 노드가 가지는 최소 샘플 수    \n",
    "    - min_weight_fraction_leaf: 가중치가 부여된 전체 샘플 수에서의 비율    \n",
    "    - max_leaf_nodes: 리프 노드의 최대 개수    \n",
    "    - max_features: 각 노드에서 분할에 사용할 특성의 최대 개수    \n",
    "\n",
    "    (2) 시각화: min_samples_leaf 개수를 제어해 적절히 분류되도록 하고, 분류가 잘 이루어졌는지 시각화 수행         \n",
    "\n",
    "    (3) prunning = 불필요한 노드 지우기    \n",
    "        - 노드가 많은 경우, 과적합 확률 있음    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e3cbe",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style='background-color: #fff5b1'>3. 서포터 벡터 머신 (SVM)</span>\n",
    "- 클래스를 분류하는 다양한 경계선 중 최적의 라인을 찾아내는 알고리즘    \n",
    "- 데이터를 분리하는 초평면 중 데이터와 가장 거리가 먼 초평면을 선택해 분리하는 지도 학습 기반의 이진 선형 분류 모델     \n",
    "- 명확하게 분류 가능한 데이터 집단에서 성능이 뛰어남 => 패턴 인식, 이미지 분석, 바이오인포매틱스 등 다양한 머신러닝 응용 분야에서 활용    \n",
    "\n",
    "----    \n",
    "**구성**    \n",
    "- 서포트 벡터: 데이터 중 결정 경계와 가장 가까이 있는 데이터의 집합   \n",
    "- 결정 경계: 데이터 분류의 기준이 되는 경계    \n",
    "    - 데이터에서 가장 멀리 떨어져 있는 경우가 베스트     \n",
    "- 마진: 결정 경계 ~ 서포트 벡터 거리 = 여유 공간    \n",
    "    - 마진이 가장 큰 경우가 최적의 선     \n",
    "- 초평면: n차원 공간의 n-1 차원 평면   \n",
    "    - n차원 공간을 두 부분으로 나누는 한 차원 낮은 부분 공간 = Subspce    \n",
    "- 슬랙 변수 / 여유 변수: 완벽한 분리가 불가능할 때 분류를 위해 허용된 오차에 대한 변수     \n",
    "\n",
    "----     \n",
    "\n",
    "**장점**    \n",
    "    (1) 서포트 벡터를 이용해 결정 경계를 생성 -> 데이터가 적을 때 효과적     \n",
    "    (2) 새로운 데이터 입력 시, 전체 데이터 포인트와 거리를 계산하지 않고 서포트 벡터와의 거리만 계산하기 때문에 연산량 최소화    \n",
    "    (3) 정확성 굿 & 비선형 모델 분류 가능    \n",
    "    (4) 과대 적합 가능성 ↓ & 노이즈 영향 ↓    \n",
    "\n",
    "**단점**    \n",
    "    (1) 데이터 전처리 과정 중요    \n",
    "    (2) 데이터가 많을수록 최적화를 위한 평가 과정이 많아 속도가 느려짐    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767741d1",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style='background-color: #fff5b1'>4. 최소 근접 알고리즘 (KNN)</span>\n",
    "- 데이터에서 거리가 가까운 k개의 다른 데이터 label을 참조해 분류하는 알고리즘 => 비슷한 특성을 가진 데이터끼리 가까이 있다는 것을 가정     \n",
    "\n",
    "**KNN 알고리즘 계산 순서**      \n",
    "(1) 데이터 준비   \n",
    "    - 미리 학습 X   \n",
    "    - 각 데이터는 특징 벡터와 label로 구성   \n",
    "\n",
    "(2) K 값 설정    \n",
    "    - K = 가장 가까운 이웃의 개수 (동점 방지를 위해 보통 홀수로 설정)    \n",
    "    - 일반적으로, 학습용 데이터 개수의 제곱근으로 설정     \n",
    "    - K가 너무 크면 주변의 점과 근접성이 떨어져 과소적합 / K가 너무 작으면 이상값과 이웃이 될 가능성    \n",
    "\n",
    "(3) 거리 계산    \n",
    "    - 새로운 예측 데이터가 주어지는 경우, 해당 데이터와 기존 모든 데이터 간의 거리 계산    \n",
    "    - 유클리드 거리, 맨해튼 거리 사용    \n",
    "\n",
    "(4) 가장 가까운 K개의 이웃 선택   \n",
    "    - 계산된 거리 중 가장 작은 거리 값을 가진 K개의 데이터 선택 => 가장 가까운 이웃       \n",
    "\n",
    "(5) 분류    \n",
    "    - K개의 이웃 중 가장 많이 등장하는 클래스가 예측 결과    \n",
    " \n",
    "----    \n",
    "\n",
    "**장점**   \n",
    "    (1) 나누는 기준을 몰라도 데이터 분류 가능   \n",
    "    (2) 입력 데이터만 주어지면 바로 예측값을 구할 수 있어 학습 과정 필요 X    \n",
    "    (3) 이해가 쉽고 간단히 구현 가능    \n",
    "    (4) 추가된 데이터 처리 용이     \n",
    "    (5) 훈련 데이터 요약 X 일반화 X 원본 데이터 전체 그대로 저장 O      \n",
    "\n",
    "**단점**   \n",
    "    (1) 테스트 데이터 개수에 따라 시간이 오래 걸리기도 함        \n",
    "    (2) 학습 데이터를 모두 거리 계산에 사용해, 하급 데이터의 양도 계산 시간에 영향을 미침      \n",
    "    (3) K 값 결정 어려움     \n",
    "    (4) 수치형 데이터가 아닌 경우 유사도 정의 어려움     \n",
    "    (5) 데이터 내 이상치 존재할 경우, 분류 성능에 큰 영향을 받음     \n",
    "\n",
    "----\n",
    "\n",
    "**앙상블**    \n",
    "- 여러 분류 모델을 결합해 하나의 분류 모델보다 더 좋은 성능을 내는 머신러닝 기법 => 여러 모델을 학습시키고 그 예측 결과를 종합해 최종 예측   \n",
    "    - **보팅**: 여러 알고리즘 모델을 병렬로 사용   \n",
    "    - **배깅**: 깉은 알고리즘 모델을 병렬로 사용    \n",
    "    - **부스팅**: 같은 알고리즘 모델을 직렬로 사용     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5a0c3",
   "metadata": {},
   "source": [
    "## <u>분류 평가 지표</u> <br>\n",
    "#### <span style='background-color: #fff5b1'>1. 혼동 행렬</span>\n",
    "- 분류 모델의 예측 결과를 정확한 예측과 잘못된 예측으로 구분한 표 (행렬)     \n",
    "\n",
    "구분 | 분류 값 | 설명   \n",
    "---- | ---- | ----   \n",
    "예측 정확 | TP | 정답이 참 & 참으로 예측    \n",
    "예측 정확 | TN | 정답이 거짓 & 거짓으로 예측   \n",
    "예측 틀림 | FP | 정답이 거짓 & 참으로 예측   \n",
    "예측 틀림 | FN | 정답이 참 & 거짓으로 예측    \n",
    "---  \n",
    "**혼동 행렬을 이용한 분류 모델 평가 지표 (0~1 사이의 값)**   \n",
    "\n",
    "**(1) 정확도**   \n",
    "- 모델이 입력된 데이터를 얼마나 정확하게 예측하는지   \n",
    "- 모든 가능한 예측 중 참인 비율 = (TP+TN) / (TP+TN+FP+FN)   \n",
    "\n",
    "**(2) 정밀도**   \n",
    "- 참이라고 예측한 경우 실제 참의 비율    \n",
    "- ex. 스팸이 아닌 메일을 스팸으로 분류하는 경우,  정밀도를 높여야 함 => Threshold 높여야 함    \n",
    "    - Threshold: 모델의 참 거짓을 가르는 분류 기준   \n",
    "\n",
    "**(3) 재현도**   \n",
    "- 실제로 참인 경우 참으로 예측하는 비율    \n",
    "- ex. 암 환자인데 암 환자가 아니라고 예측하는 경우, 즉 FN을 줄이기 위해 재현도를 높여야 함 => Threshold 낮춰야 함       \n",
    "<br>\n",
    "---   \n",
    "\n",
    "**정밀도 & 재현도 그래프**   \n",
    "- 두 값이 만나는 지점을 Threshold로 정하면 예측 오류 최소화 가능    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ff42e",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'>2. F1-Score</span>\n",
    "- 정밀도와 재현도의 조화 평균 => 정밀도와 재현도 간 균형을 효과적으로 평가       \n",
    "- F1 Score = 2 * Precision * Recall / (Precision+Recall)   \n",
    "- 머신러닝 모델의 성능을 평가하는 주요 지표 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c28b0a",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'>3. ROC / AUC Curve</span>\n",
    "**ROC Curve**   \n",
    "- 얼마나 잘 분류되었는지를 보여주는 그래프    \n",
    "    - TPR (True Positive Rate): 참 중 참으로 예측한 비율 = 재현도    \n",
    "    - FPR (False Positive Rate): 거짓 중 참으로 예측한 비율     \n",
    "- 현실적으로 완벽한 분류기가 나올 수 없기 때문에, 적절히 타협한 최선의 ROC Curve를 도출해야 함   \n",
    "\n",
    "**AUC Curve**     \n",
    "- ROC와 x축 사이의 면적 (적분값)    \n",
    "- 모델의 성능을 숫자로 나타낼 수 있음 -> 0.5 ~ 1 사이 값을 가지며 1에 가까울수록 분류 성능 굿     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5f826",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'>4. 다중 분류 평가 지표</span>\n",
    "**Macro Average**    \n",
    "- 클래스별로 구한 평가 지표 평균 => 모든 클래스에 동등한 가중치    \n",
    "\n",
    "**Weighted Average**    \n",
    "- 클래스별로 구한 평가 지표 가중 평균 => 빈도가 높은 (샘플이 많은) 클래스에 큰 가중치     \n",
    "\n",
    "**Micro Average**   \n",
    "- 모든 클래스의 예측 결과를 더해 전체 성능을 평가하는 지표 => 모든 클래스에 동등한 가중치        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b928b",
   "metadata": {},
   "source": [
    "## <u>하이퍼파라미터 최적화</u> <br>\n",
    "#### <span style='background-color: #fff5b1'>1. 하이퍼파라미터 최적화</span>\n",
    "**하이퍼파라미터**     \n",
    "- 학습 시작 전 사용자가 직접 설정하는 변수 -> 모델 학습 과정에 반영되는 값   \n",
    "\n",
    "**하이퍼파라미터 최적화**     \n",
    "- Tuning을 거쳐 적절한 하이퍼팔미터를 찾아 모델 성능을 향상시키는 것   \n",
    "    (1) 하이퍼파라미터 탐색 범위 설정: 최적 값을 찾고 싶은 하이퍼파라미터의 범위 설정     \n",
    "    (2) 평가 지표 계산 함수 정의: 탐색하고자 하는 하이퍼파라미터를 인수로 받아 평가지표 값을 계산하는 함수 정의    \n",
    "    (3) (1)에서 샘플링한 하이퍼파라미터 값을 사용해 검증 데이터로 정확도 평가    \n",
    "    (4) (1)~(3)을 반복하며 정확도 결과에 따라 하이퍼파라미터의 범위를 좁힘     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d8252d",
   "metadata": {},
   "source": [
    "#### <span style='background-color: #fff5b1'>2. 하이퍼파라미터 최적화 방법</span>\n",
    "**Grid Search**   \n",
    "- 정해진 범위에서 하이퍼파라미터 모두 순회    \n",
    "(+) 범위가 넓고 스텝이 작을수록 꼼꼼하게 전 범위 탐색 -> 최적 해를 정확히 찾을 수 있음     \n",
    "(-) 시간이 너무 오래 걸림   \n",
    "\n",
    "**Random Search**     \n",
    "- 정해진 범위에서 하이퍼파라미터 무작위 탐색    \n",
    "(+) 속도가 Grid Search보다 빠름     \n",
    "(-) 무작위 -> 정확도가 떨어짐    \n",
    "\n",
    "**Bayesian Optimization**     \n",
    "- 사전 정보를 바탕으로 하이퍼파라미터 값을 확률적으로 추정하며 탐색    \n",
    "- Gaussian Process라는 통계학을 기반으로 만들어진 모델     \n",
    "(1) 초기 값 몇 개를 무작위로 실험    \n",
    "(2) 실험 결과를 바탕으로 어떤 파라미터의 조합이 좋을지 확률 모델로 예측     \n",
    "(3) 다음 실험할 조합을 가장 좋을 것 같은 위치로 선택    \n",
    "(4) 결과 반영 후 반복    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
