{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c2f667",
   "metadata": {},
   "source": [
    "# 1. 군집화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e439d0b3",
   "metadata": {},
   "source": [
    "## 1.1 머신러닝 : 비지도 학습\n",
    "\n",
    "* 비지도 학습이란 : 정답을 알려주지 않고 학습을 진행하는 방법! (지도학습과의 차이)\n",
    "* 이에 따라 패턴 및 유사도를 스스로 확인해서 기계가 학습 -> 대표적으로 군집화!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb25c810",
   "metadata": {},
   "source": [
    "### 비지도학습의 특징\n",
    "\n",
    "* 데이터 자체에 있는 정보를 활용\n",
    "* 정답이 없으므로, 명확한 목표 및 성능을 알 수 없음 (정답지가 없으니까!)\n",
    "* 이에 따라, 목표가 명확하지 않으며 우리가 알지 못하는 데이터의 특성을 탐색하는 데에 적합!\n",
    "\n",
    "### 비지도학습이 적합한 케이스\n",
    "\n",
    "1. 패턴을 알 수 없음 (정답으로 제시할 게 없음)\n",
    "2. 패턴이 계속 변화 (정답을 믿을 수 없음)\n",
    "3. 열려있는 문제를 해결하고 지식을 일반화해야 하는 경우 (해석을 통해 정답을 도출해야 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763124bd",
   "metadata": {},
   "source": [
    "## 1.2 군집화란?\n",
    "\n",
    "* 군집화 : 데이터를 비슷한 피쳐를 가지는 그룹들로 나누는 비지도 학습\n",
    "    * 군집 : 유사한 데이터들을 묶어놓은 것으로, 데이터는 두 군데 이상의 군집에 포함될 수 없음\n",
    "* 데이터들이 유사한 모습을 보이는 것들끼리 묶어놓는다!\n",
    "* 비지도학습 : 군집화\n",
    "    * 종속 변수를 설정하지 않으므로, 데이터 내부에 있는 패턴을 이용해서 그룹화를 진행한다. 즉, 모든 변수를 이용해서 패턴 발견 및 분류를 하므로, 이는 비지도 학습임!\n",
    "\n",
    "### 군집화의 목표\n",
    "\n",
    "1. 응집도 최대화 : 같은 군집 내부는 유사하도록\n",
    "2. 분리도 최대화 : 다른 군집 끼리는 구분되도록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75293537",
   "metadata": {},
   "source": [
    "## 1.3 군집화 과정\n",
    "\n",
    "1. feature 선택 및 추출 : 군집화를 할 때 이용한 피처 고르기\n",
    "2. 군집화 알고리즘 선택 : 어떻게 나눌 것인가?\n",
    "3. 군집의 유효성 검증 : 잘 나눠졌는가?\n",
    "4. 결과 해석 : 인사이트 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c027935",
   "metadata": {},
   "source": [
    "# 2. 군집화의 주요 고려사항 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85363b74",
   "metadata": {},
   "source": [
    "## 2.1 군집화의 주요 고려사항 \n",
    "\n",
    "**덕목**\n",
    "* 한번 진행하고 땡이 아니라, 여러번 진행해보면서 최적의 결과를 찾아야 한다!\n",
    "* 이러한 과정에서 고려해야 할 3가지 사항이 존재\n",
    "\n",
    "1. 변수 유형 이해\n",
    "* 고려할 피처가 어떻게 이루어져 있는지 알아야, 어떻게 나눌 것인지 선택을 할 수 있음.\n",
    "\n",
    "2. 거리 / 유사도 정의와 측정 \n",
    "* 군집화의 목표를 잘 이루기 위해, 데이터 간의 거리 및 유사도를 기반으로 그룹을 만들어야 함\n",
    "* 데이터에 따라 거리, 유사도의 기준이 달라질 것이므로 이를 잘 책정해주어야 함\n",
    "\n",
    "3. 차원 축소\n",
    "* 유사한 성격의 변수들은 하나로 묶어서 생각 해 줄 필요가 있음\n",
    "* 필요 없는 변수는 제거해 주어야 함\n",
    "* 너무 많은 변수는 불필요한 요인을 만들 가능성을 높여주기 때문에, 차원 축소가 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0e954",
   "metadata": {},
   "source": [
    "## 2.2 변수 유형 이해\n",
    "\n",
    "1. 연속형 변수\n",
    "* 숫자가 의미를 가지며, 수치로 측정되는 변수\n",
    "* 유클리디안 거리, 맨하탄 거리 등 사용 가능\n",
    "* k-means, 계층적 군집화 가능\n",
    "* 스케일링이 필수다! (단위 잘 맞춰주기, 값들에 따라 다를 테니)\n",
    "\n",
    "2. 명목형 변수\n",
    "* 카테고리로 나누어지는 변수\n",
    "* 해밍 거리, 자카드 거리 등을 이용\n",
    "* K-modes, 계층적 군집화 가능\n",
    "* 원-핫 인코딩 및 더미 변수 변환으로 처리한다\n",
    "\n",
    "3. 혼합형 변수\n",
    "* 연속형 + 명목형\n",
    "* K-prototypes, Gower distacne 기반 계층적 군집화를 사용할 수 있음\n",
    "* 다양한 특징을 가질 것이므로, 상황 봐서 좋은 방법으로 처리하자\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dde71a",
   "metadata": {},
   "source": [
    "## 2.3 거리/유사도의 정의와 측정\n",
    "\n",
    "### 데이터의 거리 측정 방법\n",
    "\n",
    "-> 말 그대로 데이터간의 거리를 물리적. 의미적으로 측정한다. 가까울수록 유사한 성질, 멀수록 다른 성질을 가진다\n",
    "\n",
    "1. 유클리디안 거리\n",
    "* 피타고라스 기반 거리 측정 (직선거리)\n",
    "* K-means에서 주로 이용한다\n",
    "\n",
    "2. 맨하탄 거리\n",
    "* 절대값 기반 거리 (축방향으로만 이동해서 도달)\n",
    "* 이상치에 덜 민감하다\n",
    "\n",
    "3. 코사인 유사도\n",
    "* 내적 계산을 이용해서 데이터 값의 벡터 간 각도를 측정\n",
    "* 텍스트 및 고차원에 효과적 (선형적으로 확인하기에도 좋을듯? 선이 얼마나 기울어져있을지 볼 수 있으니)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d0581",
   "metadata": {},
   "source": [
    "### 6가지 연결 방법\n",
    "\n",
    "* 계층적 군집화에서 두 군집간의 거리를 구하는 방법. 방법별로 도출되는 거리가 다르므로, 다른 군집화 결과를 도출함.\n",
    "\n",
    "1. single linkage (단일 연결, 최단 연결법)\n",
    "* 서로 가장 가까운 거리를 찾는다\n",
    "* 고립된 군집을 찾는 데에 특화(일단 집히는 대로 가져오기)\n",
    "* 이상치에 취약 (하나만 슉 나가있어도 최단은 얘 기준이 되니...)\n",
    "\n",
    "2. complete linkage (완전 연결, 최장 연결법)\n",
    "* 싱글과 반대로, 가장 먼 거리를 찾는다\n",
    "* 군집들 내부 응집성에 초첨 (그나마 제일 먼 놈 가져오기)\n",
    "* 마찬가지로 이상치에 민감\n",
    "\n",
    "3. average method (평균 연결)\n",
    "* 모든 항목의 거리를 구하고 그 평균을 구함\n",
    "* 위 2개에 비해 이상치에 덜 민감\n",
    "* 그러나 매우 많은 계산이 필요할 수 있다\n",
    "\n",
    "4. centric method (중심연결)\n",
    "* 군집들 각각의 중심을 구하고, 그 거리를 측정한다\n",
    "* 두 군집이 결합 시엔 가중평균\n",
    "* 군집의 중심 구하는 데에 시간 소요\n",
    "* inversion 발생 가능\n",
    "\n",
    "5. median (중간 연결)\n",
    "* 중앙값 사이의 거리 구하기\n",
    "* 이상치에 강함\n",
    "* 기하학적 구조는 파악 어렵\n",
    "\n",
    "6. ward 연결법\n",
    "* 군집을 병합하고 오차제곱합 증가분이 최소인 것을 선택\n",
    "* 군집 내 분산 최소화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f0afa",
   "metadata": {},
   "source": [
    "## 2.4 차원 축소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d50d60",
   "metadata": {},
   "source": [
    "### 차원의 저주\n",
    "\n",
    "* 차원의 저주 : 특성 공간이 매우 커 데이터 효과적 훈련이 불가, 특히 고차원일수록 거리 기반 분별이 어려움 (공간이 너무 커서)\n",
    "\n",
    "### 차원 축소\n",
    "\n",
    "* 차원의 저주를 피하고자, 차원을 줄이는 차원 축소를 진행 -> 독립 변수를 감소시키기!\n",
    "* how? \n",
    "    * 선형 투영 : 고차원 공간에서 저차원 공간으로 선형적 데이터 투영 (PCA, SVD 등)\n",
    "    * 매니폴드 학습 : 유클리드 거리(기하적 거리)가 아니라 데이터 포인터 사이 곡선 거리를 고려한다 (t-SNE,UMAP 등)\n",
    "\n",
    "### PCA\n",
    "\n",
    "* how PCA? : 다차원 데이터에서 분산이 가장 큰 방향을 찾고, 그 방향으로 데이터를 투영해서 새롭게 축을 만들어줌, 고차원 데이터를 저차원화!\n",
    "* 분산을 크게 잡는 이유? 분산을 최대한 보존하기 위해!\n",
    "* 이렇게 새롭게 만들어진 축(성분)을 주성분이라 한다! (한 선 위로 데이터가 축소됨)\n",
    "\n",
    "**유의할점!!**\n",
    "* 표준화를 꼭 진행한 데이터에 적용\n",
    "* 결측값이 있으면 사용 불가\n",
    "* 직관성이 떨어짐 (축에 의미를 어떻게...?)\n",
    "* 정보 손실 (거리만 남아버림...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794bd7f3",
   "metadata": {},
   "source": [
    "# 3. 군집화 알고리즘\n",
    "\n",
    "## 3.1 계층적 군집화\n",
    "* 계층적 군집화 : 데이터 간 유사성을 통해 트리 구조를 형성, 상향 및 하향 방식으로 군집 형성\n",
    "\n",
    "### 계층적 군집화의 특징\n",
    "* 데이터셋의 관측치를 통해 덴드로그램 생성\n",
    "* 군집의 개수가 초기에 정해져있지 않음\n",
    "* 상향식 : 응집형 , 하향식 : 분리형 으로 정해짐\n",
    "\n",
    "### 응집형/분리형 계층적 군집화\n",
    "* 응집형 군집화 : 샘플들(작은거)이 클러스터가 되고, 근처끼리 이를 병합해나가며 하나의 군집이 되도록 한다. 그리고 그 과정을 잘 확인한다!\n",
    "* 분할혈 군집화 : 전체(큰거)에서 유사성이 낮은 데이터들끼리 분할을 진행\n",
    "\n",
    "### 응집형 계층적 군집화\n",
    "* 작은 것들을 근처끼리 병합해 나가며 최종적으로 하나의 군집이 되는 방식\n",
    "* 근처를 어떻게 두냐에 따라 다양하다\n",
    "* 단일 연결과 완전 연결 방식이 대표적이다!(이상치에 민감한 애들?)\n",
    "\n",
    "과정\n",
    "1. 클러스터별 거리 계산\n",
    "2. 가까운 것들끼리 병합\n",
    "3. 거리 계산 업데이트\n",
    "4. 이를 반복\n",
    "\n",
    "### 계층적 군집화의 시각화\n",
    "* 시각화는 덴드로그램으로 가능하다!\n",
    "* 계층적 군집화는 계산량이 많아 소규모 데이터에 적합!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ddab8",
   "metadata": {},
   "source": [
    "## 3.2 k-means\n",
    "\n",
    "* k-means : 데이터를 k개의 그룹으로 나누되, 각 그룹의 중심점과의 거리가 가장 가까운 데이터를 묶어주는 방법 (kNN이랑 느낌이 비슷하다?)\n",
    "* 가장 일반적인 알고리즘으로, 중심점 선 선택 후 가까운 데이터 모으기!\n",
    "\n",
    "how?\n",
    "1. k개의 초기 클러스터 중심정을 설정\n",
    "2. 각 표본들을 가장 가까운 중심점에 속하게 (유클리디안 거리 : 실제 데이터에 적용할 때 같은 스케일인지 확인해서 맞춰야함!)\n",
    "3. 새로 속한 데이터를 포함 다시 중심점\n",
    "4. 이걸 지정한 만큼 반복\n",
    "\n",
    "* 장점 : 직관적, 쉬운 구현, 대용량도 문제없다\n",
    "* 단점 : 초기 중심점에 좌지우지, k 결정 너무 사용자에 따라 다름, 이상치에 민감, 기하학적 모양의 군집은 어려움(단순히 모여있는게 아닌 경우)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a92192",
   "metadata": {},
   "source": [
    "### k-means ++ \n",
    "\n",
    "* k-means를 보완해서 초기 중심점들을 멀리 떨어뜨려 놓고 일관된 결과를 도출시키는 방법 : 초기 중심점에 좌지우지 되는 단점을 해결!\n",
    "\n",
    "how?\n",
    "1. k개의 중심점 저장할 집합 M 초기화\n",
    "2. 첫 중심점을 랜덤하게 선택, M 할당\n",
    "3. M에 없는 각 샘플들 에 중심점까지 최소제곱거리 구함\n",
    "4. 가중치 적용 확률 분포를 이용해 다음 중심점 랜덤 선택\n",
    "5. k 개 구할 때까지 계속 중심점 구함\n",
    "6. 이후는 k-means로~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd8101",
   "metadata": {},
   "source": [
    "### 엘보우 방법\n",
    "\n",
    "* 엘보우 방법 : 클래스 내 SSE를 통해 그래프를 이용해 최적 클러스터의 수를 찾는다!\n",
    "* k값을 바꿔가면서 k-means를 하다가, SSE 감소 효과가 미미하기 시작하는 k값을 찾는다!\n",
    "\n",
    "* 장점 : 직관적이고, 구현하기 쉬움(k-means 따라 그래프만 그리면 되니)\n",
    "* 단점 : 주관적 판단이 필요한데, 모호할 때는 참 애매할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde3371",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "* DBSCAN : 밀도가 높은 지역의 데이터를 일단 군집으로 묶어두고, 속하지 않은 데이터들은 군집에 포함시키지 않는 알고리즘\n",
    "* 고밀도 영역들을 중심으로 클러스터 인식\n",
    "* 밀집도 : '특정한 거리 안에 존재해야 하는 데이터의 최소 개수'\n",
    "\n",
    "**특징**\n",
    "* 이상치를 과감하게 제거하므로, 영향을 안 받음\n",
    "* 데이터가 밀도 높은 군집 근처에 잇으면 편입될 가능성 업!\n",
    "\n",
    "how?\n",
    "1. 샘플 분류 : 조건에 따라 분류\n",
    "    * 핵심 : 근처에 있는 샘플이 지정된 값 이상일 때\n",
    "    * 경계 : 근처 샘플 수가 핵심은 안되는데, 핵심 샘플의 반경 안일 때\n",
    "    * 잡음 : 위 둘 모두 해당 X\n",
    "2. 클러스터 생성\n",
    "    * 개별 핵심 샘플 및 반경 내 샘플을 클러스터로 생성\n",
    "3. 경계 샘플을 클러스터에 각각 할당\n",
    "\n",
    "* 장점 : 임의의 기하학적 분포에 강함, 이상치 무시, 잡음 걸러내기 좋음\n",
    "* 단점 : 파라미터 설정에 민감, 연산량 많음, 다른 밀도 분포의 데이터 군집 분석에 취약\n",
    "\n",
    "### HDBSCAN\n",
    "\n",
    "* 계층적 DBSCAN으로, 계층적 군집화처럼 변환!\n",
    "* how?\n",
    "1. 밀도 기반 1차 클러스터링\n",
    "2. 이후 계층적으로 진행!\n",
    "\n",
    "장점 : 여러 밀도 기준으로 반복적 수행이 가능 -> 단일 밀도 기준 한계를 보완!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b707e7",
   "metadata": {},
   "source": [
    "## 3.4 GMM\n",
    "\n",
    "* GMM : 데이터가 다양한 가우시안 분포가 합쳐진 결과로 보고, 각 분포당 하나의 클러스터로 인식\n",
    "* 각 군집을 확률 분포로 간주하고 이를 혼합해서 모델링하는 모델 기반 군집화! 생성 메커니즘까지 구현할 수 있다. \n",
    "\n",
    "기본 가정\n",
    "* 관측된 데이터는 가우시안 확률 분포에 의해 생성됨\n",
    "* 전체 데이터셋은 다변량 가우시안 분포가 섞임\n",
    "* 우도에 따라 k개의 개별 가우시안 분포에 개별 데이터가 속한다.\n",
    "\n",
    "how?\n",
    "1. 전체 데이터 분포 확인\n",
    "2. 전체 분포가 서로 다른 정규분포들의 혼합 결과임을 가정\n",
    "3. 여러개의 정규 분포 도출 및 각각 분포 군집을 설정\n",
    "\n",
    "### 모델 파라미터 추정\n",
    "\n",
    "* GMM은 기댓값-최대화 알고리즘으로 모델 파라미터를 추정 (군집의 중심, 모야, 크기, 방향)\n",
    "\n",
    "how?\n",
    "1. 필요한 파라미터들의 초기값을 설정\n",
    "2. E step : 현재 세타를 통해 샘플이 군집에 속할 사후확률 계싼\n",
    "3. M step : 사후확률 기반으로 다시 파라미터 계산\n",
    "4. 수렴조건 만족까지 이를 반복\n",
    "\n",
    "* 장점 : kmeans에 비해 유연하고 타원형 및 중첩 군집에 효과적\n",
    "* 단점 : 군집화 시간이 오래걸리고, 각 클러스터가 가우시안이 아니라면....젠장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a5226",
   "metadata": {},
   "source": [
    "# 4. 군집화 평가 방법\n",
    "\n",
    "* 어??? 근데 정해진 답이 없는데 어떻게 평가를 진행하죠??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94131dfb",
   "metadata": {},
   "source": [
    "## 4.1 내부 평가와 외부 평가\n",
    "* 평가 방법(정답 레이블의 유무)에 따라 외부평가와 내부평가로 구분\n",
    "\n",
    "1. 외부 평가\n",
    "* 정답 레이블이 존재하는 경우! (외부 값에 정답이 있다고 생각하면 될까?)\n",
    "* 군집화 결과와 외부 정답 레이블을 이용해 유사도 평가\n",
    "* ...근데 답이 있는데 왜 비지도를 햇죠?\n",
    "    * 새로운 인사이트를 얻거나, 구조에 대해 잘 이해하려고!\n",
    "2. 내부평가\n",
    "* 정답이 없는 경우!\n",
    "* 군집이 얼마나 잘 응집되고, 분리되었는지 여부를 진단(애초에 이 둘이 목적이었음)\n",
    "* 예시\n",
    "    * K-means : SSE 최소화\n",
    "    * GMM : 로그 우도 최소화\n",
    "* 범용적 지수 : 실루엣 계수, 던 지수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dc327f",
   "metadata": {},
   "source": [
    "## 4.2 실루엣 계수\n",
    "\n",
    "* 실루엣 계수 : 클러스터 내 샘플들이 얼마나 조밀하게 모여있는가를 진단! (응집성과 분리도 모두 사용!) 이에 따라 \"군집 자체의 품질\"을 진단 가능!\n",
    "\n",
    "how?\n",
    "1. 샘플 x의 동일 군집 내 모든 다른 포인트 사이 거리를 평균해서 응집력을 구함\n",
    "2. x의 다른 가장 가까운 군집 포인터 사이 거리 평균해서 분리도 구함\n",
    "3. 두 값 차이를 두 값의 max로 나눔\n",
    "\n",
    "판단\n",
    "* 실루엣 계수가 1에 가까움 : 분리도가 응집도보다 크다 : 잘 속해있고, 잘 분리되어 있따!\n",
    "* 실루엣 계수가 0 : 분리도 = 응집도 : 샘플이 경계에 위치....군집간 거리가 애매하다\n",
    "* 실루엣 계수가 -1 에 가까움 : 분리도가 응집도보다 작다 : 잘못들어감, 군집화 잘 안됨!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec87a85",
   "metadata": {},
   "source": [
    "## 4.3 던 지수\n",
    "\n",
    "* 던 지수 : 클러스터 간 최소 거리와 클러스터 내 최대 거리의 비율을 계산해서 클러스터링 품질을 평가\n",
    "\n",
    "how?\n",
    "1. 가장 가까운 두 클러스터 사이의  거리를 구함 (중심 거리 및 최소 거리 기준으로, 분리도)\n",
    "2. 한 클러스터 안에서 최대 거리를 구함 (응집도)\n",
    "3. 1에서 구한 값을 2에서 구한 값으로 나눔\n",
    "\n",
    "해석\n",
    "* 클수록 좋은 군집화! (분리가 잘되고 응집도 잘 되어있다는 의미니)\n",
    "* 0보다 크며, 무한대까지 가능하다!\n",
    "\n",
    "* 한계 : 군집이 많아질수록 많은 계산 요구, 극단값에 취약(최대.최소값), 이에 따른 이상치 면역 없음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4874c4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
