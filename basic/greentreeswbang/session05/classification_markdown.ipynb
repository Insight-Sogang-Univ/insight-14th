{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ccfa6c",
   "metadata": {},
   "source": [
    "다음 내용이 필수적으로 들어가야 합니다.\n",
    "\n",
    "- 회귀와 분류의 차이에 대해 설명해주세요.\n",
    "- 분류 모델의 네 가지 종류와, 각 모델이 무엇인지 간단하게 정리해주세요.\n",
    "- 분류 평가 지표에는 무엇이 있는지 작성해주세요.\n",
    "- 하이퍼파라미터 최적화가 무엇인지 쓰세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf595b",
   "metadata": {},
   "source": [
    "# 1 분류란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff3533",
   "metadata": {},
   "source": [
    "## 1 지도 학습 : 회귀 vs 분류\n",
    "\n",
    "**회귀**\n",
    "* 주어진 데이터를 기반으로 원하는 결과 값을 오차가 없이 fit 하는 것. (다른 값을 기반으로 원하는 것을 예측한다)\n",
    "\n",
    "**분류**\n",
    "* 데이터가 어떤 **범주**에 속하는지 알고리즘으로 판별 후 새로운 값 받았을 때 이전 값 토대로 어디에 속하는지 분류하는 것\n",
    "\n",
    "**회귀 vs 분류**\n",
    "* 회귀는 **연속형 변수**를 예측 (가격 등), 분류는 **범주형 변수**를 예측 (남/여, 스팸메일 여부 등등)\n",
    "* 분류에서는 로지스틱 회귀, 결정 트리, 서포트 벡터 머신, 최소 근접의 알고리즘을 주로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a39711",
   "metadata": {},
   "source": [
    "## 2. 이진 분류와 다중 분류\n",
    "\n",
    "**이진 분류**\n",
    "* A/B 문제일 때\n",
    "\n",
    "**다중 분류**\n",
    "* A/B 문제가 아닌, 이후 더 많은 변수 c d 등이 존재한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c790750b",
   "metadata": {},
   "source": [
    "# 2. 분류 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb3a3f",
   "metadata": {},
   "source": [
    "### 1. 로지스틱 회귀\n",
    "\n",
    "*  이진 문제 해결을 위한 대표적인 알고리즘 : 샘플이 어떤 클래스에 속할 확률을 추정하는 것!\n",
    "* 독립 변수의 선형적인 조합에 로지스틱 함수를 적용 -> 출력값을 0과 1 사이로 변환 (이진 문제 해결이기 때문!)\n",
    "\n",
    "**motivaiton**\n",
    "* 선혁 회귀로는 이진 문제를 푸는 것이 안되는가? yes!\n",
    "    * 왜? \"기준\"에 따른 차이인데, 이는 선형적으로 사이에 값이 의미를 가지는 문제가 아니기 때문!\n",
    "    * 따라서, 0과 1 사이의 s자 모양의 함수가 필요 -> 시그모이드 함수!(로지스틱 함수)\n",
    "\n",
    "**시그모이드 함수**\n",
    "* 0과 1사이의 값을 가지면서 s자 모양인 함수\n",
    "* 특정 값 기준으로 A/B가 구분될 때 유용하다 (모양부터 그렇게 생겼다)\n",
    "* $H(x)=\\frac{1}{1+e^{-(wx+b)}}=sigmoid(wx+b)=\\sigma(wx+b)$\n",
    "* 위 형태에서 w와 b를 예측하는 것이 목표인데, **승산**을 이용하면 변수 축소 + 이에 따른 선형적 계산이 가능해져서 편리!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21730942",
   "metadata": {},
   "source": [
    "### 2. 의사결정나무\n",
    "\n",
    "* 조건에 따라 데이터를 분류, 최종적으로 순수한 label들의 집합으로 데이터들을 나누는 것이 목표 (순수한의 의미는 뭘까...싶긴 한데)\n",
    "* a/b로 나누고, 나눈 것을 또 나누구,,,, 이것을 반복\n",
    "\n",
    "**구성 요소**\n",
    "* Root : 가장 상위 노드\n",
    "* Parent, Chiledren : 기준에서 상위, 하위\n",
    "* bianry : a/b\n",
    "* leaf : 말단\n",
    "* edge : p-c 연결\n",
    "* 뎁스, 하이츠 : 뎁스는 루트에서 목표까지 엣지, 하이츠는 다른 기준에서 목표까지 엣지\n",
    "\n",
    "**CART(이진 분할) 알고리즘**\n",
    "* 데이터셋을 임계값(기준) 을 가지고 2개로만 하위를 연결시키는 것 (a/b)\n",
    "    * 임계값은 어떻게? 불순도가 낮아지는 방법으로!\n",
    "        * 불순도? 말그대로 순수하지 않은 정도(의도한 것 보다)\n",
    "            * 어떻게? 지니 계수(불순도)로!  \n",
    "*  주요 단계\n",
    "    1. 임계값 설정 (기준잡아주기)\n",
    "    2. 불순도 감소 알고리즘 : 불순도를 이용해서, 얼마나 분류가 잘 되었는지 확인하고 불순도가 낮은 쪽으로 가지를 형성!\n",
    "\n",
    "**실제 학습 시 고려해야 할 점**\n",
    "1. 하이퍼파라미터 설정\n",
    "2. 시각화를 통해 분류 확인 (오버피팅 및 언더피팅 유무)\n",
    "3. 가지치기 : 과도한 노드는 오버피팅을 초래합니다\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6a8e3",
   "metadata": {},
   "source": [
    "### 3. SVM(Support Vector Machine)\n",
    "\n",
    "**motivation**\n",
    "* 어떻게 하면 땅나누기를 하는데 가장 깔끔하게 선을 그을 수 있을까?? -> 즉, 데이터들이 막 요리조리 모여있을 텐데 최대한 분류가 명확하게 될 평면을 어떻게 만들까??\n",
    "* n차원의 데이터를 나누면, 나누는 평면은 n-1 차원이겠죠?\n",
    "\n",
    "**구성 요소**\n",
    "* 서포트 벡터 : 나누는 선과 가장 가까이 있는 데이터들의 집합\n",
    "* 결정 경계 : 선\n",
    "* 마진 : SV와 선 사이 거리\n",
    "* 초평면 : n차원에서 n-1 평면\n",
    "* 슬랙 변수 : 완전한 분리가 어려울 때 허용 가능한 범주\n",
    "\n",
    "**방법**\n",
    "* 간단하다. 마진이 가장 큰 경우를 선택하면 됨!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f526fe7",
   "metadata": {},
   "source": [
    "### 4. KNN\n",
    "\n",
    "**motivation**\n",
    "* 비슷한 놈들은 비슷한 곳에, 끼리끼리 모여있을 것이라 생각!\n",
    "* 이에 따라 거리가 가까운 k개의 데이터를 기반으로 분류를 진행한다\n",
    "\n",
    "**방법**\n",
    "1. 데이터 생으로 준비 (학습 필요 없음)\n",
    "2. 가장 가까운 이웃의 개수 k를 설정 (동률 방지를 위해 홀수개로)\n",
    "3. 거리 계산\n",
    "4. k개 만큼 가까운 애들을 선정\n",
    "5. k개 중에 가장 많은 클래스를 예측 결과로 선정\n",
    "\n",
    "**kick**\n",
    "* KNN은 학습 없이, 단지 그 근처에 k개의 데이터 기반으로 쪽수를 센 뒤에 많은 것을 채택한다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c345b",
   "metadata": {},
   "source": [
    "# 3. 분류 평가 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f118f0e",
   "metadata": {},
   "source": [
    "### 1. 혼동 행렬\n",
    "\n",
    "* 예측 결과를 예측과 결과의 측면으로 2x2 구분해서 나타낸 행렬\n",
    "* 진짜같은데 진짜, 가짜같은데 가짜, 진짜같은데 가짜, 가짜같은데 진짜. 이렇게4개\n",
    "* 어떻게 구분? 정확도, 정밀도, 재현도 기준!\n",
    "\n",
    "**정확도, 정밀도, 재현도**\n",
    "* 정확도 : (맞춘거)/(전체)\n",
    "* 정밀도 : (진짜같은데 진짜였던거)/(진짜라고 예측한 모든것)\n",
    "* 재현도 : (진짜같은데 진짜였던거)/(실제 진짜였던 것)\n",
    "    * 각 상황별로 정밀도, 재현도 중에 중요 지표가 다를 수 있다\n",
    "-> 정확도만으로는 완벽하게 알 수가 없다 (위의 행렬에 있는 4가지 범주의 값들이 고루 있지 않을 때)\n",
    "-> 정밀도와 재현도 그래프에서 두 값이 만나는 지점을 기준으로 두면, 예측 오류 최소화 가능!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d98fe",
   "metadata": {},
   "source": [
    "### 2. F1- Score\n",
    "\n",
    "* F1 - score : 정밀도와 재현율 사이의 조화평균.\n",
    "    * 왜 조화평균을? : 정밀도와 재현율 사이에 비율에 대한 차이까지 보정해주기 위해서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6dbc0d",
   "metadata": {},
   "source": [
    "### 3. ROC, AUV curve\n",
    "\n",
    "**ROC curve**\n",
    "* 분류가 얼마나 잘 되었는가를 확인해보자\n",
    "* x축에는 FPR(실제로 거짓인데 진짜로 잘못 예측한 비율), y축에는 TPR(실제로 참인데 잘 예측해서 참이라고 예측한 비율)로 둔 곡선\n",
    "* 좌상단으로 치우쳐 있을 때 \n",
    "\n",
    "**AUC curve**\n",
    "* ROC curve와 x축 사이 적분 (ROC 커브를 적분한 그래프) -> 1에 가까울수록 좋은 분류 성능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b321a",
   "metadata": {},
   "source": [
    "# 4. 하이퍼파라미터 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223fa588",
   "metadata": {},
   "source": [
    "### 1. 하이퍼파라미터 최적화\n",
    "\n",
    "**하이퍼파라미터?**\n",
    "* 변수 중에서, 학습자가 설정해주는 변수! (그니까 변수인데 변하지 않는 변수라고 보면 됨)\n",
    "* 그렇다면 최적화는? : 하이퍼파라미터를 잘 설정해주어서 모델의 성능을 업그레이드!\n",
    "\n",
    "**순서**\n",
    "1. 지정할 하이퍼파라미터의 범위를 설정\n",
    "2. 평가 지표 계산 함수 정의 (하이퍼파라미터 별로 성능이 달라질텐데, 이를 나타낼 수)\n",
    "3. 하이퍼파라미터 지정하고 정확도 평가\n",
    "4. 반복해서, 하이퍼파라미터의 범위 결정\n",
    "\n",
    "**how?**\n",
    "1. Grid search : 하이퍼파라미터를 죄다 넣어본다!\n",
    "*  장점 : 정확하다!\n",
    "*  단점 : 오래걸린다...\n",
    "\n",
    "2. Random search : 정해진 범위 내에서 무작위로 넣어서 찾아본다!\n",
    "* 장점 : 빠르다!\n",
    "* 단점 : 정확하지 않을 수 있따...\n",
    "\n",
    "3. 베이시안 최적화 : **사전 정보**를 바탕으로 최적 하이퍼파라미터를 확률적으로 추정!\n",
    "* 초기 값을 무작위로 실험해보고, 어떤 조합이 좋을 지 확률적으로 예측. 이후 이에 맞춰서 가장 좋을 것 같은 위치로 조합을 이동. 이를 반복!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa5bca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
