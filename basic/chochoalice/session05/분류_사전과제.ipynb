{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f111bb",
   "metadata": {},
   "source": [
    "# **1)분류**  \n",
    "기계학습>>지도학습>>회귀,**분류**  \n",
    "\n",
    "### 1-1) 지도 학습  \n",
    "\n",
    "##### **연속형 변수 예측**\n",
    "- 회귀: **연속적인 숫자**값 예측  \n",
    "\n",
    "##### **범주형 변수 예측**\n",
    "- 분류: 입력된 데이터 주어진 **항목별**로 분류   \n",
    "      기존 데이터가 속한 레이블 알고리즘으로 인지 -> 새로운 데이터 레이블 판별   \n",
    "    (1)이진 분류: 예측하고자 하는 변수가 가질 수 있는 값이 참/거짓   \n",
    "    (2)다중 분류: 예측하고자 하는 변수가 가질 수 있는 값이 3개 이상  \n",
    "\n",
    "- 분류 모형의 이해:  \n",
    "  **입력**(변수1 변수2 변수3 변수4...) -> **분류 모형** -> **확률** -> **그룹1/그룹2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812b31d",
   "metadata": {},
   "source": [
    "# **2)분류 모델**  \n",
    "\n",
    "### 2-1) 로지스틱 회귀  \n",
    "- **이진 분류** 문제를 푸는 알고리즘, 샘플이 특정 클라스에 속할 **확률 추정**  \n",
    "- 로지스틱 회귀의 필요성: 로지스틱 회귀의 종속변수의 결과는 0,1만 가져야 하는데 선형회귀를    적용할 경우 해당 범위를 벗어날 수 있음   \n",
    "로지스틱 회귀 모델의 함수는 시그모이드 함수를 따름(출력범위 0~1, S자 형태)  \n",
    "- 확률-예측변수(독립변수)는 비선형 관계이므로 비선형 회귀모형\n",
    "\n",
    "#### 2-1-1) 시그모이드 함수\n",
    "- 입력값이 커지면 1에 수렴, 입력값이 작아지면 0에 수렴\n",
    "- 입력값($x$)이 $(-\\infin, \\infin)$일 때, 출력값($y$)은 0~1의 값을 가짐  \n",
    "- 측정값이 특정값 이상이면 1, 이하면 0으로 설정하여 **이진 분류**에 사용  \n",
    "- 시그모이드 함수식: $H(x)=\\frac{1}{1+e^{-(wx+b)}}=sigmoid(wx+b)=\\sigma(wx+b)$  \n",
    "                   w,b: 가중치   \n",
    "                   가중치w에 따른 변화-기울기   \n",
    "                   가중치b에 따른 변화-그래프 위치\n",
    "\n",
    "#### 2-1-2) Odds(승산)  \n",
    "${사건 A가 일어날 확률}\\over{사건 A가 일어나지 않을 확률}$  \n",
    "\n",
    "-사건이 발생할 확률이 발생하지 않을 확률보다 몇 배 높은지 알려줌  \n",
    "-**오즈를 사용하는 이유**: 확률은 0~1 사이의 값만 가짐, 회귀모형은 실수 전체 값에 적용됨  \n",
    "-> 0~$\\infin$ 까지의 값 가지는 오즈 사용  \n",
    "-오즈 사용시 문제점: 로그는 음수 값을 가질 수 없음, **1을 중심으로 비대칭**   \n",
    "-> **로그오즈** 사용   \n",
    "\n",
    "#### 2-1-3) 로그오즈  \n",
    "logit(p)=log( $\\frac{p(A)}{1-p(A)}$)  \n",
    "$p(A)=\\frac{1}{1+e^{-(wx+b)}}$ 의 양변에 logit transform(logit 함수)을 적용 -> wx+b  \n",
    "**확률이 0.5일 때를 중심으로 대칭**  \n",
    "exp(로그오즈)=오즈  \n",
    "\n",
    "### 2-2) 의사결정나무  \n",
    "- 조건에 따라 데이터 분류, 데이터가 순수한 라벨의 집합으로 구성될 때까지 분류 반복  \n",
    "- 의사결정나무의 구성 요소:\n",
    "  1. Root Node: 가장 상위에 위치한 노드  \n",
    "  2. Parent Node: 주어진 노드의 상위에 있는 노드  \n",
    "  3. Child Node: 하나의 노드로부터 분리된 2개 이상의 노드들  \n",
    "  4. Leaf Nodes: 자식마디가 없는 노드, 모델에서 label에 해당  \n",
    "  5. Edge: Parent node와 child node를 연결하는 연결선  \n",
    "\n",
    "  - Binary Tree(이진 트리): Tree 중에서 children이 최대 2개인 tree  \n",
    "  - Height: 특정 노드에서 가장 멀리 있는 leaf node까지의 경로에 있는 edge 개수  \n",
    "  - Depth: Root node에서 특정 노드에 도달하기 위한 edge의 수  \n",
    "\n",
    "- 실제 학습 시 고려해야 할 것들  \n",
    "  - Hyperparameter 설정  \n",
    "  ```python\n",
    "  min_samples_split #분할되기 위해 노드가 가져야 하는 최소 샘플 수  \n",
    "  min_samples_leaf  #리프 노드가 가지고 있어야 하는 최소 샘플 수  \n",
    "  min_weight_fraction_leaf #min_samples_leaf와 같, 가중치가 부여된 전체 샘플 수에서의 비율  \n",
    "  max_leaf_nodes  #리프 노드의 최대 개수  \n",
    "  max_features  #각 노드에서 분할에 사용할 특성의 최대 수  \n",
    "  ```  \n",
    "  - 시각화  \n",
    "  시각화로 분류가 잘 이루어졌는지 확인  \n",
    "  - Prunning(가지치기)   \n",
    "  과적합 방지, 일반화 성능을 높이기 위해서 결과의 개수를 줄임   \n",
    "\n",
    "#### 2-2-1) CART(이진분할)알고리즘  \n",
    "- 임계값을 기준으로 데이터셋 두 child로 나누는 알고리즘  \n",
    "- 임계값: 불순도(지니계수)가 낮아지는 방향으로 나눔  \n",
    "- 지니지수: 불순도를 나타냄, 통계적 분산 정도를 정량화해서 표현 (0~1 사이)  \n",
    "           최솟값: 0, 데이터가 하나의 class에만 속함  \n",
    "           최댓값: 모든 class가 동일한 비율로 존재  \n",
    "- 근시안적인 알고리즘으로 가장 효율적인 대안을 제시할 수 없음  \n",
    "\n",
    "### 2-3) SVM\n",
    "- 클래스를 분류할 수 있는 다양한 경계선 중 최적의 라인을 찾아내는 알고리즘  \n",
    "- 데이터를 분리하는 초평면(Hyperplane) 중에서 데이터들과 가장 거리가 먼(두 클래스 간의 거리를 최대화하는) 초평면을 선택하여 분리하는 지도 학습 기반의 이진 선형 분류 모델  \n",
    "  - **초평면**: n차원 공간을 두 부분으로 나누는 한 차원 낮은 부분 공간  \n",
    "           2차원의 상황은 두 개의 범주가 선으로 분리 가능, 독립변수의 개수가 많아짐에 따라 고차원이 되면 평면으로 분리  \n",
    "- 로지스틱 회귀나 판별분석에 비해 비선형 데이터에서 높은 정확도, 과적합 경향이 적음  \n",
    "- 데이터가 적을 때 효과적\n",
    "- SVM의 구성  \n",
    "  - Support vector (서포트 벡터): 데이터 중에서 결정 경계와 가장 가까이에 있는 데이터들의 집합 \n",
    "  - Decision Boundary (결정 경계): 데이터 분류의 기준이 되는 경계  \n",
    "  - Margin (마진): 결정 경계에서 서포트 벡터까지의 거리(여유 공간)  \n",
    "  - Hyperplane (초평면): $n$차원 공간의 ($n-1$)차원 평면  \n",
    "  - Slack Variables (슬랙 변수/여유변수): 완벽한 분리가 불가능할 때 분류를 위해 허용된 오차에 대한 변수\n",
    "\n",
    "### 2-4) KNN  \n",
    "- 비슷한 특성을 가진 데이터끼리 가까이 있다는 점 이용\n",
    "- 데이터로부터 거리가 가까운 k개의 다른 데이터 레이블을 참조하여 분류  \n",
    "- 범주를 나눈 기준을 알지 못해도 분류 가능  \n",
    "- 학습 데이터 모두를 거리 계산에 사용하기 때문에 학습 데이터의 양도 계산 시간에 영향을 미침  \n",
    "- 데이터 내 이상치 존재 시 분류 성능에 큰 영향  \n",
    "- KNN 알고리즘 계산 순서  \n",
    "  1. 데이터 준비  \n",
    "     **미리 학습 X**, 특징 벡터와 레이블로 이루어진 데이터  \n",
    "  2. K 값 설정  \n",
    "     **K: 가장 가까운 이웃의 개수**  \n",
    "     일반적으로 학습용 데이터 개수의 제곱근, 홀수 개  \n",
    "     **K값 큼: 과소 적합, K값 작음: 이상값과 이웃이 될 가능성**   \n",
    "  3. 거리 계산  \n",
    "     새로운 데이터가 주어지면 이 값과 기존 모든 데이터 간의 거리 계산  \n",
    "  4. 가장 가까운 k개의 이웃 선택  \n",
    "  5. 분류하기  \n",
    "     k개 중 가장 많이 등장하는 클래스가 예측 결과   \n",
    "     동점 발생시 (1) 동점이 발생한 클래스들 중, 더 가까운 이웃에 해당하는 클래스로 분류  \n",
    "                (2) 동점이 발생한 클래스들 중, 무작위로 하나를 선택  \n",
    "\n",
    "### 2-5) 앙상블  \n",
    "- 여러 개의 개별 분류 모델들을 *결합*해 하나의 분류 모델보다 더 좋은 성능을 내는 머신러닝 기법  \n",
    "- 앙상블 종류:  \n",
    " 1. 보팅: 다른 알고리즘의 모델을 병렬로 사용  \n",
    " 2. 배깅: 동일 알고리즘의 모델을 병렬로 사용  \n",
    " 3. 부스팅: 동일 알고리즘의 모델을 직렬(순차적)로 사용  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf21b3",
   "metadata": {},
   "source": [
    "# **3)(이진)분류 평가 지표**  \n",
    "### 3-1) 혼동 행렬\n",
    "- 분류 모델의 예측 결과를 정확한 예측과 잘못된 예측으로 구분하여 나타낸 표(행렬)  \n",
    "\n",
    "| 구분 | 분류 값 | 설명 |\n",
    "| --- | --- | --- |\n",
    "| 예측이 정확한 경우 | **True Positive (TP)** | 정답이 참, 참으로 예측 |\n",
    "|  | **True Negative (TN)** | 정답이 거짓, 거짓으로 예측 |\n",
    "| 예측이 틀린 경우 | **False Positive (FP)** | 정답이 거짓, 참으로 예측 (예측이 틀림)  |\n",
    "|  | **False Negative (FN)** | 정답이 참, 거짓으로 예측 (예측이 틀림)  |  \n",
    "\n",
    "- 정확도 (Accuracy): 모든 가능한 예측 중 참인 비율 (TP+TN) / (TP+TN+FP+FN)  \n",
    "                **정답 레이블의 비율이 불균형하면 모델의 정확도를 신뢰할 수 없음**\n",
    "- 정밀도 (precision): 참이라고 예측한 경우 실제 참의 비율  TP/(TP+FP)     \n",
    "                **거짓을 참으로 판단한 정도**를 알 수 있음  \n",
    "- 재현도 (recall): 실제로 참인 경우 중 참으로 예측하는 비율  TP/(TP+FN)  \n",
    "                **참을 거짓으로 판단한 정도**를 알 수 있음  \n",
    "- 정밀도와 재현도는 trade-off 관계  \n",
    "  경계값(threshold)을 기반으로 넘으면 참, 미만이면 거짓으로 판단  \n",
    "  경곗값(Threshold)를 낮춤 → Positive 예측이 늘어남 → recall ⬆, precision ⬇ \n",
    "  경곗값(Threshold)를 높임 → Positive 예측이 감소함 → recall ⬇, precision ⬆  \n",
    "  **정밀도와 재현도 그래프가 만나는 지점**을 임계값으로 정하면 예측 오류 최소화 가능   \n",
    "- ex) 암 진단: (양성 환자의 음성 예측)인 경우 더 치명적->FN 줄이기 위해 재현도 ⬆\n",
    "      스팸메일: (일반메일의 스팸메일 예측)인 경우 더 치명적->FP 줄이기 위해 정밀도 ⬆  \n",
    "\n",
    "### 3-2) F1-Score  \n",
    "- 정밀도와 재현도의 조화평균  \n",
    "- 2 *정밀도 *재현도/ (정밀도+재현도)  \n",
    "- 두 값 중 하나라도 낮으면 F1 Score 크게 낮아짐  \n",
    "- 클래스 비율이 크게 차이나는 **불균형 데이터셋**에서 정확도만으로는 모델의 성능 판별 어려움  \n",
    "\n",
    "### 3-3) ROC/AUC curve  \n",
    "#### 3-3-1) ROC curve  \n",
    "- 얼마나 분류가 잘 되었는지를 보여주는 그래프  \n",
    "- True Positive Rate (TPR) : 참인 것들 중에 참이라고 예측한 비율 (=Recall)  \n",
    "- False Positive Rate (FPR) : 거짓인 것들 중에 참이라고 잘못 예측한 비율  \n",
    "- x축이 FPR, y축이 TPR일 때 좌상단으로 붙어있을수록 좋은 이진 분류기  \n",
    "#### 3-3-2) AUC curve  \n",
    "- ROC와 x축 사이의 면적(적분값)  \n",
    "- 0.5~1사이의 값, **1에 가까울수록 성능이 좋음**  \n",
    "\n",
    "### 3-4) 다중 분류 평가 지표  \n",
    "- 이진 분류 평가 지표를 이용해 클래스별로 점수를 구한 뒤 적절히 평균을 내림  \n",
    "- Macro average: 클래스별로 구한 평가 지표 평균(모든 클래스에 동등한 가중치)\n",
    "- Weighted average: 클래스별로 구한 평가 지표 가중 평균(샘플 수대로 가중 평균)\n",
    "- Micro average: 모든 클래스의 예측 결과를 더하여  전체적인 성능을 평가하는 지표(모든 클래스에 동등한 가중치)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d3bd6",
   "metadata": {},
   "source": [
    "# 4)하이퍼파라미터 최적화  \n",
    "### 4-1) 하이퍼파라미터  \n",
    "- 하이퍼파라미터: 모델 학습 과정에 반영되는 값, 학습 시작 전에 사용자가 직접 설정하는 변수  \n",
    "- 하이퍼파라미터 최적화: tunning을 거쳐 적절한 하이퍼파라미터를 찾아 모델 성능을 향상시키는 것  \n",
    "- 하이퍼파라미터 최적화 과정:  \n",
    "  1. Hyperparameter 탐색 범위 설정 (최적 값을 찾고 싶은 하이퍼파라미터 범위 설정)  \n",
    "  2. 평가 지표 계산 함수(성능 평가 함수) 정의(탐색하려는 Hyperparameter를 인수로 받아 평가지표 값 계산해주는 함수 정의)  \n",
    "  3. 1단계에서 샘플링한 Hyperparameter 값을 사용하여 검증 데이터로 정확도 평가   \n",
    "  4. 위 단계를 특정 횟수 반복하며, 정확도 결과를 보고 하이퍼파라미터의 범위 좁힘  \n",
    "\n",
    "### 4-2) 하이퍼파라미터 최적화 방법  \n",
    "#### 4-2-1) Grid Search  \n",
    "- 정해진 범위에서 하이퍼파라미터를 모두 순회  \n",
    "- GridSearchCV (scikit-learn)  \n",
    "- 장점: 범위가 넓고 step이 작을수록 꼼꼼하게 전 범위를 탐색하여 최적해를 정확히 찾을 수 있음  \n",
    "- 단점: 시간이 너무 오래 걸림  \n",
    "- 적용: 넓은 범위, 큰 step을 활용해 범위를 좁힘  \n",
    "#### 4-2-2) Random Search  \n",
    "- 정해진 범위에서 하이퍼파라미터를 무작위로 탐색  \n",
    "- RandomSearchCV (scikit-learn)  \n",
    "- 장점: 속도가 Grid Search보다 빠름  \n",
    "- 단점: 무작위라는 한계 때문에 정확도가 떨어짐 -> Grid Search나 Bayesian Optimization에 비해 사용 빈도가 적음    \n",
    "#### 4-2-3) Bayesian Optimization   \n",
    "- 사전 정보를 바탕으로 최적 하이퍼파라미터 값을 확률적으로 추정하여 탐색  \n",
    "- bayes_opt   \n",
    "- 방법:  \n",
    "    - 초기 값 몇 개를 무작위로 실험  \n",
    "    - 그 결과를 바탕으로 어떤 파라미터의 조합이 좋을 지 확률 모델로 예측  \n",
    "    - 다음 실험할 조합을 가장 좋을 것 같은 위치로 선택  \n",
    "    - 결과를 반영하고 다시 반복  \n",
    "- Surrogate Model: 기존 입력값(x1, f(x1)), (x2, f(x2)), ... , (xt, f(xt))들을 바탕으로, 미지의 목적 함수 f의 형태에 대한 확률적인 추정을 하는 모델  \n",
    "- Acquisition Function: Surrogate Model이 목적 함수에 대해 확률적으로 추정한 결과를 바탕으로, 바로 다음 번에 탐색할 입력값 후보를 추천해 주는 함수  \n",
    "\n",
    "- 하이퍼파라미터 최적화 자동화 라이브러리: Optuna"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
