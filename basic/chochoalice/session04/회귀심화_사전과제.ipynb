{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497aaeed",
   "metadata": {},
   "source": [
    "# **0)회귀**  \n",
    "### 0-1) 회귀 복습  \n",
    "- 인공지능 > 머신러닝 > 지도학습,비지도학습,강화학습  \n",
    "- 연속적인 데이터 간의 함수관계를 찾아내기 위한 방법          \n",
    "- **MSE**: 모형의 분산에 대한 추정량 SSE/(n-k-1)  \n",
    "- **MAE**: 절댓값 사용-이상치에 둔감\n",
    "- **R-squared**: SSR/SST  \n",
    "- **AIC**: 작을수록 좋음   \n",
    "- 최소제곱법(OLS): 잔차 제곱합을 최소화하는 회귀선 찾는 방법  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3997b6",
   "metadata": {},
   "source": [
    "# **1)선형회귀**  \n",
    "- 선형회귀 프로세스:  \n",
    "사전검증-모델 생성 및 모델 fit-모델 평가-모델 성능 개선  \n",
    "\n",
    "### 1-1)다중선형회귀  \n",
    "- 종속변수 = 회귀계수*설명변수 + 절편 + 오차항  \n",
    "  절편과 오차항을 제외하고 보면 **종속변수에 대한 설명변수의 가중평균**  \n",
    "\n",
    "### 1-2)다중선형회귀의 기본가정  \n",
    "\n",
    "**다중선형회귀 적용 가능한 가정 6가지**  \n",
    "1. **선형성**  \n",
    "   종속변수와 설명변수의 관계가 선형적이어야 함  \n",
    "   종속변수에 영향을 미치는 모든 독립변수가 고려되었으며 함수의 형태가 적절해야 함\n",
    "\n",
    "2. **독립성**    \n",
    "   설명변수끼리 선형독립적이어야 함  \n",
    "   측정오차는 독립적, 종속변수의 값도 독립변수에만 의존하는 독립적인 측정값  \n",
    "   선형독립적이지 X -> 다중공선성 (VIF/상관계수 확인)\n",
    "\n",
    "3. **오차항의 평균은 0(0에 가까움)**    \n",
    "   평균값이 0과 다르다면 측정값은 바이어스가 발생\n",
    "\n",
    "4. **등분산성**    \n",
    "   오차항의 분산은 일정해야 함  \n",
    "   전체적인 분포에서 오차의 차이가 균등하지 않으면, 특정 구간에서 정확도가 떨어질 수 있음(정확도가 일정하지 않음)  \n",
    "   *등분산성에 대한 가정이 위배되더라도 추정된 모형은 크게 왜곡되지는 않음\n",
    "    - 분산이 큰 변수에 가중치 적게 주는 가중최소제곱법\n",
    "    - 변수 변환(주로 log 사용)\n",
    "   종속변수 Y가 어떤 수준의 값을 가져도 일정한 $\\sigma^2$ 값을 가짐  \n",
    "\n",
    "5. **오차항은 자기상관되어 있지 않음**    \n",
    "   오차항의 공분산은 항상 0이어야 함  \n",
    "   시계열 자료에서 많이 나타남.  \n",
    "   과거 값이 현재 값과 상관관계를 갖게 됨(이전 데이터의 패턴 학습)  \n",
    "\n",
    "6. **정규성(선택)**    \n",
    "   오차항이 정규분포를 따름  \n",
    "   F검정이나 신뢰구간 계산시 요구되는 가정  \n",
    "   잔차에 대한 분포가 종모양과 비슷한 형채를 나타내면 됨  \n",
    "\n",
    "- 언제 분석 가능한지  \n",
    "\n",
    "| 기본 가정 | 회귀 분석 전에 검증 가능? | 회귀 분석 후에 검증 가능? | 검증 방법 |  \n",
    "| ---: | :---: | :---: | :--- |  \n",
    "| 선형성 | ✅ | ✅ | scattor plot 찍어보기 |   \n",
    "| 독립성 | ✅ | ✅ | VIF 지수, 상관계수 |  \n",
    "| 오차항 평균=0 | ❌ | ✅ | np.mean(residuals) |  \n",
    "| 등분산성 | ❌ | ✅ | 잔차의 도표화 |  \n",
    "| 오차항 자기상관 X | ✅ (회귀 분석 전에는  간접적으로 검증 가능, 분석 후에 최종 검증) |✅ | Durbin-Watson 검정(분석 후) |  \n",
    "| 정규성 | ❌ | ✅ | Shapiro-Wilk 검증, Q-Q plot |  \n",
    "\n",
    "### 1-3)회귀분석 평가방법\n",
    "1. 시각화  \n",
    "회귀선과 데이터를 함계 시각화  \n",
    "\n",
    "2. 통계지표  \n",
    "##### 2-1) 모델의 유의성 검정\n",
    "\n",
    " - F 검정  \n",
    " -검정대상: 전체 모델  \n",
    " -목적: 회귀 모델이 유의미한지 검정  \n",
    " -**귀무가설**: 모든 회귀계수=0 (독립변수들이 종속변수들과 관계 없음)  \n",
    " -**대립가설**: 적어도 하나의 회귀계수는 0이 아님 (독립변수 중 하나 이상이 종속변수에 영향 줌)  \n",
    " -F 값이 클수록 모델이 통계적으로 유의하다는 것을 의미  \n",
    " -**Prob(F statistic의 p값)**이 0.05보다 작으면 귀무가설 기각 = **적어도 하나의 독립변수가 종속변수와 유의미한 관계를 갖고 있다** = **회귀 모델이 유의미하다**  \n",
    "\n",
    " - T 검정  \n",
    " -검정대상: 개별 변수   \n",
    " -목적: 특정 독립변수가 유의미한지 검정  \n",
    " -**귀무가설**: 해당 변수의 회귀계수=0 (이 변수는 종속변수에 영향 미치지 않음)  \n",
    " -**대립가설**: 해당 변수의 회귀계수는 0이 아님 (이 변수는 종속변수에 유의미한 영햘 줌)  \n",
    " -**p-value(P>|t|)**이 0.05보다 작으면 귀무가설 기각  \n",
    "\n",
    " ##### 2-2) 모델의 성능 평가  \n",
    " - 결정계수 (R-squared)  \n",
    " -모델이 설명하는 데이터의 총변동(평균과의 차이) 중에서 회귀모델로 설명된 비율  \n",
    " -R^2 = SSR/SST = 1-(SSE/SST)  \n",
    " -1에 가까울수록 모델이 데이터를 잘 설명한다고 볼 수 있음  \n",
    " -adjusted R-squared: R^2값은 독립 변수의 개수가 증가할수록 자연스럽게 증가 \n",
    "                      R-squared 값에 (n-1/n-k) 곱해서 구함 (변수개수 k에 따른 패널티 부여)  \n",
    "\n",
    "- AIC,BIC(SC)  \n",
    "둘 다 값이 낮을수록 좋다고 평가\n",
    "-AIC = Akaike information criterion  \n",
    "-BIC(SC): SC = Schwarz Criterion, BIC = Bayesian Information Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc0261",
   "metadata": {},
   "source": [
    "# **2)비선형회귀**  \n",
    "### 2-1) 다항식 회귀모델  \n",
    "- 다항식 회귀모델 형태: Y=a+bX+cX^2+dX^3+...  \n",
    "- 사용 방법: \n",
    "  기존의 선형 회귀에 거듭제곱 항을 추가해 모델 확장  \n",
    "  확장한 상태에 선형회귀 적용  \n",
    "  ```python\n",
    "  #기존의 변수들 다항식으로 만들기  \n",
    "  from sklearn.preprocessing import PolynomialFeatures\n",
    "  poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "  X_poly = poly.fit_transform(X)  \n",
    "  #선형회귀 클래스 이용 #statsmodel의 sm\n",
    "  import statsmodels.api as sm\n",
    "  X_poly = sm.add_constant(X_poly)\n",
    "  model = sm.OLS(Y, X_poly).fit()\n",
    "  Y_pred = model.predict(X_poly)  \n",
    "  #sklearn의 LinearRegression  \n",
    "  from sklearn.linear_model import LinearRegression\n",
    "  model = LinearRegression()\n",
    "  model.fit(X_poly, Y)\n",
    "  Y_pred = model.predict(X_poly)  \n",
    "\n",
    "- **sm.OLS**와 **LinearRegression** 차이  \n",
    "-sm.OLS  \n",
    "통계분석 및 회귀 계수 해석에 이용 \n",
    "회귀계수 유의성 분석이나 통계적 검정 필요할 때 사용   \n",
    "통계적 해석/결정계수 모두 제공  \n",
    "-LinearRegression  \n",
    "머신러닝 예측 모델  \n",
    "정규화 적용하고 싶을 때나 비선형 회귀 적용하고 싶을 때 사용  \n",
    "통계적 해석 미제공/결정계수 제공  \n",
    "\n",
    "### 2-2) 지수 회귀모델  \n",
    "- 종속변수 Y에 로그를 취한 후 선형 회귀 적용 \n",
    "- **원래의 데이터 형태로 복구하기 위해 예측값에 지수함수 다시 적용!**  \n",
    "```python  \n",
    "# 지수함수 형태의 임의의 데이터 생성 \n",
    "np.random.seed(42)\n",
    "X = np.linspace(1, 10, 20).reshape(-1, 1)\n",
    "Y = 2 * np.exp(0.8 * X).flatten() + np.random.normal(0, 10, X.shape[0]) \n",
    "\n",
    "# 종속변수 Y에 자연 로그 변환\n",
    "log_Y = np.log(Y)\n",
    "\n",
    "# 회귀 모델 학습 (log(Y) = a + bX)\n",
    "X_const = sm.add_constant(X)  \n",
    "model = sm.OLS(log_Y, X_const).fit()\n",
    "\n",
    "# 예측값 변환 (exp를 적용해 원래 스케일로 변환)\n",
    "log_Y_pred = model.predict(X_const)\n",
    "Y_pred = np.exp(log_Y_pred) # exp(지수 함수) 적용해서 로그 변환 되기 전의 Y로\n",
    "```\n",
    "\n",
    "### 2-3) 로그 회귀모델  \n",
    "- 독립변수 X에 로그 연산 취한 후, 나머지는 선형 회귀와 동일  \n",
    "- 예측값(Y)에 별도의 처리를 하지 않아도 됨  \n",
    "```python\n",
    "# 로그함수 형태의 임의의 데이터 생성 \n",
    "np.random.seed(42)\n",
    "X = np.linspace(1, 10, 20).reshape(-1, 1)\n",
    "Y = 5 + 3 * np.log(X).flatten() + np.random.normal(0, 0.5, X.shape[0]) \n",
    "\n",
    "# 독립 변수 X에 자연 로그 변환\n",
    "log_X = np.log(X)\n",
    "\n",
    "# 회귀 모델 학습 (Y = a + b * log(X))\n",
    "log_X_const = sm.add_constant(log_X) \n",
    "model = sm.OLS(Y, log_X_const).fit()\n",
    "\n",
    "# 예측값 계산\n",
    "Y_pred = model.predict(log_X_const)\n",
    "```\n",
    "\n",
    "### 2-4) 스플라인 회귀  \n",
    "- 전체 데이터 범위를 여러 구간으로 나눈 후 구간에 대해 별도의 회귀 모델(선형 또는 다항 등)을 적용  \n",
    "- 구간 경계에서의 **연속성 유지** 중요  \n",
    "```python\n",
    "X_spline = dmatrix(\"bs(X, df=5, degree=3, include_intercept=False)\", \n",
    "{\"X\": X}, return_type='dataframe')\n",
    "# X: 데이터\n",
    "# df: 나눌 구간의 수+1 (df=5라면 4개의구간으로 구분합니다)\n",
    "# degree: 사용할 회귀 모델의 차수 (degree=1이면 선형회귀, degree=2면 2차 다항 회귀)\n",
    "# include_intercept=False: 절편 제거(필요하면 True로)\n",
    "# return_type='dataframe': dataframe 형식으로 반환합니다.\n",
    "\n",
    "model_spline = sm.OLS(Y, X_spline).fit()\n",
    "Y_pred_spline = model_spline.predict(X_spline)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
