{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f278ab",
   "metadata": {},
   "source": [
    "# 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233c4be",
   "metadata": {},
   "source": [
    "## 분류란?\n",
    "지도학습: 회귀 vs 분류 <br>\n",
    "1. 회귀 : 연속적인 숫자 값을 예측 <br>\n",
    "x를 기반으로 y를 잘 fit 하는 함수를 찾기 <br>\n",
    "연속형 변수를 예측하기 위해 사용\n",
    "2. 분류: 입력된 데이터를 주어진 항목들로 나누기 <br>\n",
    "기존 데이터가 **어떤 레이블에 속하는지** 패턴을 알고리즘으로 인지한 뒤에 <br>\n",
    "새롭게 관측된 데이터에 대한 레이블을 판별하는 것 <br>\n",
    "범주형 변수를 예측하기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0388c64",
   "metadata": {},
   "source": [
    "대표적인 머신러닝 알고리즘\n",
    "1. **로지스틱 회귀(Logistic Regression)** : 독립변수와 종속변수의 **선형 관계성**에 기반해 분류\n",
    "2. **결정 트리 (Decision Tree)** : 데이터 **균일도**에 따른 규칙 기반의 분류\n",
    "3. **서포트 벡터 머신 (SVM)** : 개별 클래스 간의 **최대 마진**을 효과적으로 찾아 분류\n",
    "4. **최소 근접 (Nearest Neighbor)** 알고리즘 : **근접 거리** 기준으로 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07892352",
   "metadata": {},
   "source": [
    "1. 이진 분류: 예측하고자 하는 변수 어떤 기준에 대하여 참 또는 거짓의 값만을 가질 때\n",
    "2. 다중 분류: 예측하고자 하는 변수가 가질 수 있는 값이 3개 이상일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a2a8b",
   "metadata": {},
   "source": [
    "## 분류 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2e8b2",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀\n",
    "**이진 분류** 문제를 푸는 대표적인 알고리즘 <br>\n",
    "샘플이 특정 클래스에 속할 확률을 추정하여 기준치에 따라 분류하는 것이 목표이다. <br>\n",
    "*간단히 말하면, 독립변수의 선형회귀에 로지스틱 함수를 적용하여 출력값을 0에서 1사이로 변환해주는 것을 의미함*\n",
    "<br>\n",
    "왜 필요한데요? <br>\n",
    "합격/불합격처럼 '분류'의 문제에서는 직선이 아니라 S자 형태의 함수가 필요함! <br>\n",
    "또한 예측값이 0과 1사이의 값을 가져야 하는데, 음의 무한대부터 양의 무한대까지 등 범위를 벗어나는 값들도 가질 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e099f90",
   "metadata": {},
   "source": [
    "로지스틱 함수와 비슷한 ... <br>\n",
    "**시그모이드 함수** : 출력이 0과 1사이의 값을 가지면서, S자 형태로 그려지는 함수\n",
    "<br>\n",
    "어떻게 사용하나요? <br>\n",
    "적합한 가중치를 계산해서 찾습니다! -> 살짝 어려우니 교육 잘 듣기!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e77df4",
   "metadata": {},
   "source": [
    "### 결정 트리 = 의사결정 나무 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4a462",
   "metadata": {},
   "source": [
    "의사결정 나무: 조건에 따라 데이터를 분류하며, 최종적인 데이터가 순수한 label의 집합으로 구성될 때까지 분류를 반복하는 분석 방법 = 스무고개 방법이랑 비슷하다...!\n",
    "<br>\n",
    "**CART (이진분할) 알고리즘** <br>\n",
    ": 데이터셋을 임계값을 기준으로 두 child로 나누는 알고리즘 <br>\n",
    "<br>\n",
    "그렇다면 임계값은 어떻게 나누나요? **불순도(지니계수)가 낮아지는 방향으로**\n",
    "<br>\n",
    "불순도는 뭐지?: 분류하려고 하는데, 다른 class가 섞여있는 정도를 의미함.<br>\n",
    "이를 확인하기 위해 **지니계수**를 사용함\n",
    "<br>\n",
    "**지니계수**\n",
    "통계적 분산 정도를 정량화해서 표현한 값 <br>\n",
    "지니계수 = 0 ; 가장 순수도가 높음 <br>\n",
    "지니계수 = 0.5(최대) ; 불순물이 반반 섞여있는 상태 <br>\n",
    "단, 클래스가 3개라면 33%일 때가 지니계수 최대!\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d952448",
   "metadata": {},
   "source": [
    "*CART 알고리즘의 주요 단계*\n",
    "1. 임계값 설정\n",
    "2. 불순도 감소 알고리즘 <br>\n",
    "어떤 기준으로 분류했을 때 동일한 객체들로만 잘 모아지게 할 수 있을까?에 대한 분류 기준 찾기! <br>\n",
    "보통 불순도가 낮은 쪽으로 !! 가지를 형성하게 된다\n",
    "\n",
    "<br>\n",
    "그러나... <br>\n",
    "근시안적인 알고리즘이라 tree의 height를 고려하지 않은 분류법이 나오기도 한다는 한계가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b0968",
   "metadata": {},
   "source": [
    "실제 학습시 고려해야 할 것\n",
    "1. 하이퍼파라미터 설정\n",
    "* min_samples_split : 분할되기 위해 노드가 가져야 하는 최소 데이터 수  <br>\n",
    "* min_samples_leaf: 리프 노드가 가져야 하는 최소 샘플 수 <br>\n",
    "* min_weight_fraction_leaf : 2와 같지만 가중치가 부여된 전체 샘플 수에서의 비율 <br>\n",
    "* max_leaf_nodes: 리프 노드의 최대 개수 <br>\n",
    "* max_features: 각 노드에서 분할에 사용할 특성의 최대 수 <br>\n",
    "<br>\n",
    "2. 시각화 <br>\n",
    "시각화를 수행하면서 분류가 잘 이루어졌는지 확인 가능\n",
    "<br>\n",
    "3. 가지치기 = 불필요한 노드 지우기 <br>\n",
    "왜? -> 과적합이 될 확률이 높아지기 때문에, 가지쳐서 일반화 성능을 높인다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a8286",
   "metadata": {},
   "source": [
    "### 서프트 백터 머신 = SVM\n",
    ": 클래스를 분류할 수 있는 다양한 경계선 중 최적을 라인을 찾아내는 알고리즘 <br>\n",
    "= 데이터를 분리하는 초평면 중에서 데이터들과 가장 거리가 먼 초평면을 선택하여 분리하는 **이진 선형 분류 모델**\n",
    "<br>\n",
    "장점 : 명확하게 분류 가능할 때 뛰어난 성능, 고차원 공간에서 효과적 사용 가능 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb9f4f",
   "metadata": {},
   "source": [
    "SVM의 구성\n",
    "- Support vector (서포트 벡터): 데이터 중에서 결정 경계와 가장 가까이에 있는 데이터들의 집합\n",
    "- Decision Boundary (결정 경계): 데이터 분류의 기준이 되는 경계\n",
    "- Margin (마진): 결정 경계에서 서포트 벡터까지의 거리(여유 공간)\n",
    "- Hyperplane (초평면): $n$차원 공간의 ($n-1$)차원 평면\n",
    "- Slack Variables (슬랙 변수/여유변수): 완벽한 분리가 불가능할 때 분류를 위해 허용된 오차에 대한 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9da66",
   "metadata": {},
   "source": [
    "최적의 선을 찾는 방법 <br>\n",
    "한 집단에 너무 치우지지 않은... 최대한 데이터로부터 멀리 떨어져 있는 것이 좋음! <br>\n",
    "**Margin, 즉 거리가 가장 큰 경우를 선택함으로써 최적을 선을 찾음**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494685a",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7de0f5",
   "metadata": {},
   "source": [
    "가정: *유유상종* | 비슷한 특성을 가진 데이터끼리 서로 가까이 있다는 점을 이용\n",
    "**KNN?: 데이터로부터 거리가 가까운 k개의 다른 데이터 레이블을 참조하여 분류하는 알고리즘**\n",
    "<br>\n",
    "계산 순서\n",
    "1. 데이터 준비 <br>\n",
    "미리 학습하는 과정이 없어서, 데이터를 준비해야 한다!\n",
    "2. K 값 설정 <br>\n",
    "K는 가장 가까운 이웃의 개수를 나타내며, 보통 *홀수*로 설정 <br>\n",
    "동점이 나올까봐 홀수로 하는건데, 그래도 동점이 나오면, 더 가까운 이웃에 해당하는 클래스로 분류하거나 그냥 무작위로 하나를 선택한다!\n",
    "3. 거리 계산 <br>\n",
    "새로운 데이터= 예측하려는 데이터가 주어지면, 이 값과 기존 모든 데이터 간의 거리를 계산한다. \n",
    "4. 가장 가까운 K개의 이웃 선택\n",
    "계산된 거리 중에서 가장 작은 거리 값을 가진 K개의 데이터를 선택한다\n",
    "5. 분류하기\n",
    "K개의 이웃 중 *가장 많이 등장하는 클래스가 예측 결과가 된다*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c962a0",
   "metadata": {},
   "source": [
    "최적의 K를 선택하는 법\n",
    "* 일반적으로는 학습용 데이터 개수의 제곱근으로 설정\n",
    "* K를 너무 크게 설정하면 주변 점과의 근접성이 떨어져서 분류가 잘 이뤄지지 않음 + 과소 적합\n",
    "* K를 너무 작게 설정하면 이상값, 잡음 데이터와 이웃이 될 가능성이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c761b",
   "metadata": {},
   "source": [
    "참고 : 앙상블 = 여러 개의 개별 분류 모델들을 '결합'해 하나의 분류모델보다 더 좋은 성능을 내는 머신러닝법 기법\n",
    "<br>\n",
    "앙상블의 종류 <br>\n",
    "1. **보팅(Voting) : 다른 알고리즘**의 모델을 **병렬**로 사용\n",
    "2. **배깅(Bagging): 동일 알고리즘**의 모델을 **병렬**로 사용\n",
    "3. **부스팅(Boosting): 동일 알고리즘**의 모델을 **직렬**(순차적/Sequential)로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febaa8fc",
   "metadata": {},
   "source": [
    "## 분류 평가 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef555173",
   "metadata": {},
   "source": [
    "### 혼동 행렬 <br>\n",
    ": 분류 모델의 예측 결과를 정확한 예측과 잘못된 예측으로 구분하여 나타낸 행렬 표 <br>\n",
    "1. TP: 정답 참인데, 참으로 예측\n",
    "2. TN: 정답 거짓인데, 거짓으로 예측\n",
    "3. FP: 정답 거짓인데, 참으로 예측 (틀린 경우)\n",
    "4. FN: 정답 참인데 거짓으로 예측 (틀린 경우)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204af36",
   "metadata": {},
   "source": [
    "혼동 행렬을 이용한 분류 모델 평가 지표 <br>\n",
    "1. 정확도: 모든 가능한 예측 중 참인 지표 <br>\n",
    "모델이 얼마나 정확하게 예측하는지 나타냄  -> 모든 가능한 예측 중 참인 비율 <br>\n",
    "단점: 정답 레이블의 비율이 불균형하면 모델의 정확도를 신뢰할 수 없음\n",
    "* 정밀도: TP/TP+FP | 관건은 FP | 거짓으로 참으로 판단한 정도를 알 수 있음 <br>\n",
    "* 재현도: TP/TP+FN | 관건은 FN | 참을 거짓으로 판단한 정도를 알 수 있음 <br>\n",
    "<br>\n",
    "분류를 할 때, 확률에 기반해서 Threshold**가 넘으면 참**,<br>\n",
    " Threshold **미만이면 거짓**으로 판단한다.<br>\n",
    "따라서 이 **경곗값(Threshold)를 조정하면 Precision과 Recall을 조정**할 수 있다.\n",
    "\n",
    "- 경곗값(Threshold)를 낮춤 → Positive 예측이 늘어남 → recall ⬆, precision ⬇\n",
    "- 경곗값(Threshold)를 높임 → Positive 예측이 감소함 → recall ⬇, precision ⬆"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66801ed3",
   "metadata": {},
   "source": [
    "따라서 두 value 값이 만나는 지점은 Threshold로 정하면 예측 오류를 최소화 할 수 있다!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625647d",
   "metadata": {},
   "source": [
    "### F1-score\n",
    ": 정밀도와 재현율의 조화 평균"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bc4f42",
   "metadata": {},
   "source": [
    "### ROC / AUC Curve\n",
    "1. ROC Curve : 얼마나 분류가 잘 되었는가를 보여주는 그래프 <br>\n",
    "True Positive Rate (TPR) : 참인 것들 중에 참이라고 예측한 비율 <br>\n",
    "False Positive Rate (FPR) : 거짓인 것들 중에 참이라고 잘못 예측한 비율 <br>\n",
    "2. AUC Curve : ROC와 x축 사이의 면적(적분값) <br>\n",
    "모델의 성능을 숫자로 표현 가능하다! *1에 가까울수록 분류 성능이 좋은 것에 해당*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fc709",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 최적화\n",
    "하이퍼파라미터: 학습 시작 전에 사용자가 직접 설정하는 변수 <br>\n",
    "**하이퍼파라미터 최적화**: 적절한 하이퍼파라미터를 찾아 모델 성능을 향상시키는 것<br>\n",
    "<br>\n",
    "하이퍼 파라미터 최적화 과정\n",
    "1. 하이퍼파라미터 탐색 범위 설정\n",
    "2. 평가 지표 계산 함수 정의 -> 인수로 받을 함수를 정의\n",
    "3. 1단계에서 샘플링한 하이퍼 파라미터 값을 사용하여 검증 데이터로 정확도 평가\n",
    "4. 위의 세 단계를 특정 횟수 반복하며, 정확도 결과를 보고 하이퍼파라미터의 범위를 좁힘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507eb280",
   "metadata": {},
   "source": [
    "하이퍼파라미터 최적화방법\n",
    "- Grid Search: 정해진 범위에서 Hyperparameter를 모두 순회 <br>\n",
    "장점: 범위가 넓고 스텝이 작을수록 정확한 최적해를 찾을 수 없다 <br>\n",
    "단점: 시간이 너무 오래 걸린다 <br>\n",
    "적용: 넓은 범위, 큰 스텝을 활용해 범위 줄인다 <br>\n",
    "- Random Search: 정해진 범위에서 Hyperparameter를 무작위로 탐색 <br>\n",
    "장점: 속도가 Grid보다 빠르다 <br>\n",
    "단점: 무작위라서 *정확도가 떨어진다* <br>\n",
    "- Bayesian Optimization: 사전 정보를 바탕으로 Hyperparameter 값을 확률적으로 추정하며 탐색 <br>\n",
    "특징:  \"Aquisition Fucntion\"을 적용했을 때, \"가장 큰 값\"이 나올 확률이 높은 지점을 찾아냄 <br>\n",
    "방법 : 초기 값 몇개 무작위 실험 -> 어떤 파라미터 조합이 좋을 지 확률 모델로 예측 -> 실험할 조합을 가장 좋을 것 같은 위치로 선택 -> 결과 반영하고 다시 반복 "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
