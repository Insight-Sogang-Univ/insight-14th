{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10840ca",
   "metadata": {},
   "source": [
    "# 군집화 \n",
    "## 1. 비지도 학습\n",
    "### (1) 비지도학습이란?\n",
    "\n",
    "        * 답을 가르쳐주지 않고 공부시키는 방법. 데이터속의 패턴 또는 각 데이터 간의 유사도를 기계가 학습하도록 한다!\n",
    "### (2) 비지도학습의 특징\n",
    "        * 데이터 자체 내제된 구조를 파악하는 것이 핵심\n",
    "\n",
    "        * 정답값이 없다보니 모델의 성능을 명확히 측정하기 어려움\n",
    "\n",
    "        * 피처학습을 통해 데이터의 고유 패턴을 식별할 수 있다\n",
    "\n",
    "        * 결과적으로 목표가 명확한 문제는 지도학습이 좋지만, 목표가 불명확하고 데이터 안의 특징을 찾고싶을땐 비지도학습이 적합\n",
    "\n",
    "        * 예시 \n",
    "                a. 고객 세그먼트 분석: 패턴이 알려지지 않고, 계속 변할 가능성 높음\n",
    "                b. 이상 거래 탐지 : 이상거래는 언제 발생할지 모르기때문에 적합\n",
    "                c. 자연어 처리 : 열린 문제를 해결하고 일반화하는 경우\n",
    "## 2. 군집화란\n",
    "### (1) 군집화와 목표\n",
    "\n",
    "        * 군집화: 데이터를 비슷한 특성을 가진 그룹으로 나누는 비지도학습\n",
    "\n",
    "        * 종속변수를 따로 설정하지 않음 >> 데이터 내부 특징만으로 고유패턴을 추출!\n",
    "\n",
    "        * 군집화의 목표 : 데이터간 유사성을 최대로 유지하며 다른그룹과 최대로 분리되도록!\n",
    "\n",
    "                a. 응집도 최대화 : 같은 군집에 속한 애들은 최대한 비슷\n",
    "                b. 분리도 최대화 : 서로 다른 군집인 애들은 최대한 분리\n",
    "### (2) 군집화 기본 과정\n",
    "        * 피처 선택 및 추출 : 군집화에 사용할 데이터의 피처를 선택\n",
    "\n",
    "        * 군집화 알고리즘 선택 : 데이터의 특성, 목표에 맞는 알고리즘 선택\n",
    "\n",
    "        * 군집 유효성 검증 : 군집화가 얼마나 잘 됐는지 평가\n",
    "\n",
    "        * 결과해석 : 군집이 지닌 특성 분석, 해석\n",
    "\n",
    "## 3. 군집화의 데이터\n",
    "### (1) 군집화의 고려사항\n",
    "\n",
    "        * 변수 유형 이해: 피처의 종류와 특성을 명확히 이해. 연속형, 명목형 변수 개수 등에 따라 알고리즘이 달라짐\n",
    "\n",
    "        * 거리/유사도 정의와 측정: 거리 측정 방법에 대한 정의가 중요\n",
    "\n",
    "        * 차원 축소 : 유사한 변수들을 묶어 차원축소를 해줘야함. 변수가 많으면 모델 효율성, 성능 모두 떨어짐. \n",
    "                \n",
    "### (2) 변수 유형 이해\n",
    "\n",
    "        * 연속형 변수   \n",
    "                a. 거리 측정: 유클리디안, 맨해탄 거리 \n",
    "                b. 알고리즘 : K-means, 계층적 군집화\n",
    "                c. 주의사항 : 스케일링 필수\n",
    "\n",
    "        * 명목형 변수\n",
    "                a. 거리측정: 해밍거리, 자카드 거리\n",
    "                b. 알고리즘 : K-modes, 계층적 군집화\n",
    "                c. 처리방법: 원핫인코딩, 더미변수 변환\n",
    "\n",
    "        * 혼합형 변수\n",
    "\n",
    "                a. 연속형과 명목형이 섞인 데이터\n",
    "                b. 알고리즘: K-prototypes, Gower distance 기반 군집화\n",
    "                c. 처리방법: 각 변수 유형에 맞는 거리함수 사용\n",
    "\n",
    "### (3) 거리/ 유사도 정의와 측정\n",
    "\n",
    "        * 데이터 거리 측정 방법 : 유사성/비유사성을 수치로 계산. 가까울수록 유사\n",
    "                a. 유클리디안 거리 : 가장 일반적 거리 측정 방법. 직선거리\n",
    "                b. 맨하탄 거리 : 차이의 절대값 합. 이상치에 덜 민감\n",
    "                c. 코사인 유사도 : 벡터간의 각도 측정. 텍스트데이터, 고차원 데이터에 효과적\n",
    "\n",
    "        * 6가지 연결방법\n",
    "\n",
    "                a. 최단 연결법\n",
    "                        ㄱ. 두 군집 사이의 가장 가까운 두점 간의 거리\n",
    "                        ㄴ. 고립된 군집을 찾는데 중점. 이상치에 취약\n",
    "                b. 최장 연결법\n",
    "                        ㄱ. 두 군집 사이의 가장 먼 두점 간의 거리\n",
    "                        ㄴ. 내부 응집성에 중점. 이상치에 취약\n",
    "                c. 평균 연결법\n",
    "                        ㄱ. 두 군집의 모든 점 쌍 사이의 평균\n",
    "                        ㄴ. 이상치에 덜 민감, 계산량은 많아짐\n",
    "                d. 중심 연결법\n",
    "                        ㄱ. 두 군집의 각각의 중심전간 거리\n",
    "                        ㄴ. 중심점은 가중평균을 이용해 구함\n",
    "                        ㄷ. 계산에 시간이 걸림, 덴드로그램 인버션이 발생할 가능성이 있음. \n",
    "                        ㄹ. 덴드로그램이 뭔가? 군집화 결과를 시각화하기 위한 그래프. \n",
    "                e. 중앙 연결법\n",
    "                        ㄱ. 두 군집을 합친 전체 중심점과 합치기 전 각각의 중심점 간 거리\n",
    "                        ㄴ. 모든 샘플의 중앙값으로 정의\n",
    "                        ㄷ. 데이터의 중앙값에 기반해 군집 간 거리를 정의>> 극단값에 영향 덜받음\n",
    "                f. 와드 연결법\n",
    "                        ㄱ. 군집내 제곱합이 증가하지 않게 병합\n",
    "                        ㄴ. 군집의 병합 후 군집내 제곱합의 증가분이 최소인 것을 선택\n",
    "                        ㄷ. 군집 내 분산 최소화라고 생각\n",
    "\n",
    "### (4) 차원 축소\n",
    "\n",
    "        * 거리기반 알고리즘은 차원이 높아질수록 차원의 저주가 발생\n",
    "\n",
    "        * 차원의 저주란 고차원일수록 모든 점이 비슷하게 멀어져 분별력이 감소하는 현상\n",
    "\n",
    "        * 차원축소의 목표는 변수를 제거해 차원을 낮춰 고차원 공간을 저차원에 투영하는 것\n",
    "\n",
    "        * 선형 투영 : 고차원 공간에서 저차원으로 선형 투영 Ex) PCA, SVD, 랜덤 투영 등\n",
    "\n",
    "        * 매니폴드 학습: 비선형이며, 유클리드 거리가 아닌 데이터 포인트들 사이의 곡선거리를 고려 Ex) t-SNE, UMAP, Isomap 등\n",
    "\n",
    "        * PCA\n",
    "                a. 핵심목적 : 가장 정보량이 큰 방향을 기준으로 데이터를 투영\n",
    "                b. 특징 \n",
    "                        ㄱ. 데이터 분산을 최대한 보존하면서 차원을 줄임\n",
    "                        ㄴ. 상관관계가 높은 피처들을 결합해 더 적은 수의 피처로 표현\n",
    "                        ㄷ. 원본에서 최대분산 방향을 찾아 피처를 감소시키면 상관관계가 감소됨.\n",
    "                c. 코드\n",
    "                        ㄱ. # 코드 \n",
    "                                * from sklearn.decomposition import PCA\n",
    "                                * pca = PCA(n_components=주성분개수)\n",
    "                        ㄴ. 주요 파라미터\n",
    "                                * n_components: 주성분 개수 \n",
    "                                * whiten: True면 단위분산으로 정규화\n",
    "                                * random_state: 랜덤시드 고정값 설정\n",
    "                        ㄷ. 주요 매서드\n",
    "                                * fix(x): PCA 모델 학습\n",
    "                                * transform(x): 데이터를 주성분 공간으로 변형\n",
    "                                * fit_transform(x): 학습과 변환을 한번에 수행\n",
    "                                * inverse_transform() : 원본 공간으로 역변환\n",
    "                        ㄹ. 주요 속성들\n",
    "                                * explained_variance_ratio_: 각 주성분이 설명하는 분산 비율\n",
    "                                * explained_variance_:각 주성분의 실제 분산값\n",
    "                                * components_ : 주성분벡터\n",
    "                                * n_components: 실제 선택된 주성분 개수\n",
    "                d. 주의사항\n",
    "                        ㄱ. 표준화필수 : pca사용전에 실시\n",
    "                        ㄴ. 결측값 처리: 결측값모두 처리\n",
    "                        ㄷ. 해석의 어려움: 직관적 해석이 어렵\n",
    "                        ㄹ. 정보손실: 차원축소과정에서 일부 정보 손실됨\n",
    "        \n",
    "## 4. 군집화 알고리즘\n",
    "\n",
    "### (1) 계층적 군집화 : 데이터 유사성 기반의 트리구조. 상향시그 하향식 방식이 있음\n",
    "\n",
    "        * 특징\n",
    "                a. 덴드로 그램\n",
    "                        ㄱ. 군집을 점차적으로 병합해가는 과정을 시각화한 트리구조\n",
    "                        ㄴ. 수직축은 두 군집 병합시의 유사성을 나타냄\n",
    "                b. 군집의 개수를 사전에 설정하지 않음. 군집화 종료 후에 군집 개수 선택 가능\n",
    "                c. 상향식 트리구조 - 응집형 계층적 군집화. 하향식 트리구조 - 분리형 계층적 군집화\n",
    "\n",
    "        * 응집형 군집화 : 작은 군집들을 병합해서 더 큰 군집을 만듦\n",
    "\n",
    "        * 분리형 군집화 : 하나의 군집을 분할해가며 여러 군집을 만듦\n",
    "\n",
    "        * 두 방법 모두 proximity matrix에 기반해 조직화함. \n",
    "\n",
    "        * 응집형 계층적 군집화\n",
    "\n",
    "                a. 모든 데이터 포인트를 각각 하나의 클러스터로 간주. 점차 가까운 클러스들을 병합해 가는 방식\n",
    "                b. '가장 가까운 클러스터' 정의 방식에따라 알고리즘이 달라짐\n",
    "                c. 단일연결, 완전연결이 대표적\n",
    "                d. 완전연결 응집형 군집화 과정\n",
    "                        ㄱ. 거리행렬계산 : 모든 데이터 포인트 간 거리 계산 후 거리 행렬 작성\n",
    "                        ㄴ. 단일 클러스터 시작: 각 데이터 포인트를 개별 클러스터로 초기화\n",
    "                        ㄷ. 가장 가까운 클러스터 병합: 정의된 기준에 따라, 새로운 클러스터 제작\n",
    "                        ㄹ. 거리 행렬 업데이트: 생성된 클러스터와 나머지 간의 거리를 계산 후 거리행렬 업데이트\n",
    "                        ㅁ. 반복: 모든 데이터 포인트가 하나의 클러스터에 속할때까지 반복\n",
    "        \n",
    "### (2) k-means\n",
    "\n",
    "        * 데이터를 정해진 K개의 그룹으로 나누되, 그룹 중심점과의 거리가 가장 가까운 데이터끼리 묶는 알고리즘\n",
    "\n",
    "        * 연속형 데이터중 군집 중심점을 선택 >> 해당 중심에 가까운 데이터들을 묶어 나감\n",
    "\n",
    "        * 프로토 타입 기반임. 각 클러스터의 중심을 프로토타입으로 데이터를 묶는방식\n",
    "\n",
    "        * 주요 단계 \n",
    "                a. 랜덤하게 k개의 중심점을 초기 클러스터 중심으로 선택\n",
    "                b. 각 표본들을 가장 가까운 중심점에 할당\n",
    "                c. 각 클러스터의 표본 평균을 계산하여 중심점을 이동\n",
    "                d. 클러스터 할당이 변하지 않거나 허용오차, 최대이터레이션 도달까지 2,3번 반복\n",
    "\n",
    "        * 특성의 스케일: 유클리디안 거리지표를 사용. 실제 데이터에 적용할 때 특성이 같은 스케일로 측정되었는지 확인이 필수다. 왜냐하면 큰 수치의 피처는 영향도 크게 받기 때문. z-score 표준화나 min-max scale, standard scale 등을 사용한다.\n",
    "\n",
    "        * 장점: 직관적이고 구현이 쉽다. 대용량 데이터에도 적용 가능\n",
    "\n",
    "        * 단점\n",
    "                a. 초기 centroid값에 민감\n",
    "                b. 군집 결정이 어려움\n",
    "                c. 이상치에 민감함\n",
    "                d. 기하학적 모양의 군집은 파악에 어려움\n",
    "\n",
    "        * K-means ++알고리즘\n",
    "                a. 초기 중심점들을 서로 멀리 떨어지게 위치시킴\n",
    "                b. 기본 k-means보다 일관되고 좋은 결과를 도출할 수 있음!\n",
    "\n",
    "### (3) 엘보우 방법\n",
    "\n",
    "        * 클래스 내 SSE를 바탕으로 최적 클러스터 k를 추정하는 방식\n",
    "\n",
    "        * k값을 바꿔가며 왜곡이 빠르게 감소하는 지점을 k로 추정한닷\n",
    "\n",
    "        * 직관적이고 구현이 간단하다는 장점이 있다.\n",
    "\n",
    "        * 하지만 주관적판단이 들어가고 모호한 경우가 있어 단점도 존재한다.\n",
    "\n",
    "### (4) DBSCAN\n",
    "\n",
    "        * 밀도가 높은 지역의 데이터를 하나의 군집으로 묶어 밀도기준을 만족하지 못하는 점은 군집에 포함시키지 않는 알고리즘\n",
    "\n",
    "        * 데이터 고밀도 영역을 클러스터로 인식한다.\n",
    "\n",
    "        * 핵심샘플을 연결한 그룹을 클러스터로 생성한 후 경계 샘플을 해당 핵심 샘플에 할당한다.\n",
    "\n",
    "        * 기하학적 분포를 갖는 데이터에도 강하고, 이상치에 민감하지 않다. \n",
    "\n",
    "        * 단점으로는 epsilon, min_samples 설정에 영향을 많이 받는다. Epsilon을 조절하기 어렵고, 연산량이 많아 속도가 느리다\n",
    "\n",
    "### (5) GMM\n",
    "\n",
    "        * 데이터가 각각의 가우시안 분포로 구성되었다는 가정하에 각 분포를 클러스터로 인식하는 방법.\n",
    "\n",
    "        * 모델기반 군집화 방법이다. 모델기반 군집화란 데이터를 통계적 모델을 가정하고 그 모델로부터 데이터가 생성됐다는 전제하에 군집화를 수행하는 것이다. \n",
    "\n",
    "        * 따라서 각 군집을 확률분포로 간주하며 전체 데이터 분포를 확률분포의 혼합으로 모델링한다.\n",
    "\n",
    "        * EM 알고리즘을 사용해 모델 파라미터를 측정한다.\n",
    "\n",
    "                a. initialization: 필요한 파라미터 u,시그마,파이에 대해 초기값을 설정\n",
    "                b. E step : 현재 세타를 통해 x가 특정분포에 속할 사후확률을 구한다\n",
    "                c. M step : 계산된 사후확률을 통해 u,시그마,파이를 다시 추정한다.\n",
    "                d. 수렴조건이 만족할때까지 스텝 반복\n",
    "\n",
    "        * k-means보다 유연하고, 타원형분포나 중첩군집구조도 잘 예측할 수 있다.\n",
    "\n",
    "        * 하지만 수행시간이 오래걸리고, 가정분포가 틀린 데이터엔 성능이 저하될 수 있다. \n",
    "\n",
    "## 5. 군집화 평가방법\n",
    "\n",
    "### (1) 외부평가, 내부평가\n",
    "\n",
    "        * 정답레이블이 존재 - 외부평가\n",
    "\n",
    "                a. 군집화결과와 정답을 비교해 얼마나 잘 맞췄는지 평가\n",
    "                b. 대표적 지표로 ARI가 있다. 1에 가까울수록 완벽하게 일치함을 의미한다.\n",
    "\n",
    "        * 정답레이블이 존재x - 내부평가\n",
    "\n",
    "                a. 군집 내부 응집도, 분리도를 기반으로 정량 평가한다\n",
    "                b. 각 알고리즘 자체의 내부 평가 지표와 범용 평가 지표가 있다.\n",
    "                c. K-means : SSE최소화하기 GMM: 로그우도 최대화하기\n",
    "                d. 대표적 범용지표: 실루엣계수, 던지수\n",
    "\n",
    "### (2) 실루엣 계수\n",
    "\n",
    "        * 클러스터 내 샘플들이 얼마나 '조밀하게' 모여있는가\n",
    "\n",
    "        * 실루엣 계수 계산 과정\n",
    "                a. 샘플과 동일한 클러스터 내 다른 포인트 사이 거리 평균으로 응집력 계산\n",
    "                b. 샘플과 가장 가까운 클러스터내 모든 샘플 평균거리로 분리도 계산\n",
    "                c. 응집력과 분리도 차이를 둘 중 큰값으로 나누어 실루엣으로 계산\n",
    "\n",
    "        * 계수 해석\n",
    "                a. 분리도가 응집도보다 큰 경우 1에 가까워지며 군집화가 잘됬다고 판단!\n",
    "                b. 분리도와 응집도가 같으면 계수는 0이되고 모호한 상태임\n",
    "                c. 분리도가 응집도보다 낮으면 -1에 가까워지며 군집화 실패\n",
    "\n",
    "        * 파이썬에서 돌리기\n",
    "                a. silhouette_samples함수를 이용한다\n",
    "                b. 군집화가 잘되면 실루엣계수 값 분포가 고르고 0.7이상이 나온다\n",
    "                c. 만약 잘 안되면 실루엣계수 값 분포가 불균형하고, 0.6보다 낮은 값이 나온다\n",
    "\n",
    "### (3) Dunn Index\n",
    "\n",
    "        * 클러스터 간 분리도를 응집도로 나눠서 비율을 계산한다\n",
    "\n",
    "        * 던 인덱스 값이 클수록 좋은 군집화 결과를 의미한다.\n",
    "\n",
    "        * 일반적으로 0이상이고, 무한대까지 가능하다. 실루엣계수와 병행해야 한다.\n",
    "\n",
    "        * 하지만 군집수가 많을수록 비용 증가, 이상치에 취약함 등의 단점도 존재한다.\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4719cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
