{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700f95e5",
   "metadata": {},
   "source": [
    "- 회귀와 분류의 차이에 대해 설명해주세요.\n",
    "- 분류 모델의 네 가지 종류와, 각 모델이 무엇인지 간단하게 정리해주세요.\n",
    "- 분류 평가 지표에는 무엇이 있는지 작성해주세요.\n",
    "- 하이퍼파라미터 최적화가 무엇인지 쓰세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1480b4",
   "metadata": {},
   "source": [
    "# 분류!\n",
    "\n",
    "## 분류란?\n",
    "    1. 기존 데이터가 어떤 레이블에 속하는지 패턴을 학습하고, 새 데이터에 대해 레이블을 예측하는 것!\n",
    "    2. 분류와 회귀의 차이는 무엇인가?\n",
    "        (1) 둘다 지도학습이지만, 회귀는 연속데이터의 숫자 예측이고 분류는 범주데이터의 레이블 예측이다.\n",
    "        (2) 즉, 종속변수가 연속형이면 회귀, 범주형이면 분류를 사용해야 한다!\n",
    "    3. 분류에는 두가지가 있다!\n",
    "        (1) 이진분류 : 0과 1처럼 참이냐 거짓이냐의 두가지 값을 분류해내는것\n",
    "        (2) 다중분류 : 종속변수가 3개이상일 때 분류해내는 것\n",
    "\n",
    "## 분류의 네가지 종류!\n",
    "    1. 로지스틱 회귀\n",
    "        \n",
    "        (1) 이진분류문제에 특화덴 알고리즘이며, 샘플이 특정 클래스에 속할 '확률'을 추정하여 분류하는걸 목표로 한다!\n",
    "        (2) 로지스틱함수 = 시그모이드 함수\n",
    "            1. 출력이 0과 1사이 값을 가지면서 s자 형태로 그려지는 함수이다!\n",
    "            2. 입력값이 커지면 1에, 작아지면 0에 수렴하는 특징이 있다. \n",
    "            3. 다라서 출력값이 특정값 이상이면 1, 이하면 0으로 정해 이진분류문제에 적용시킬 수 있는 것이었다!\n",
    "        (3) 왜 로지스틱함수를 쓰는가? 그건 바로 선형만으로는 분류하기엔 부족하기 때문! \n",
    "            1. 단순 직선은 너무 터프하다\n",
    "            2. 시그모이드 함수는 값이 극단적일수록 천천히 변화하는데, 이는 모델이 예측할때 급격한 변화를 방지해 보다 안정적인 예측을 가능케 한다.\n",
    "            3. 예측값이 0과 1사이에 존재해야하는데 직선일 경우 무한대로 넘어가버리면 모델을 쓸수가 없다. 이때 로지스틱함수가 그 대안이 된 것이다!\n",
    "        (4) 시그모이드 함수의 가중치 \n",
    "            1. W와 b를 인공지능은 주어진 데이터안에서 찾는다!\n",
    "            2. 가중치 W가 올라갈수록 그래프의 기울기도 가팔라진다.\n",
    "            3. 가중치 b가 올라갈수록 그래프는 위로 올라간다?\n",
    "\n",
    "    2. 의사결정 나무\n",
    "        (1) 조건에 따라 데이터를 분류하며 최종적으로 데이터가 순수한 라벨의 집합으로 구성될때까지 분류를 반복하는 방법\n",
    "        (2) 가장 널리 쓰이는 알고리즘은 CART로 데이터셋을 임계값 기준으로 두 Child로 나누는 알고리즘이다. \n",
    "        (3) 임계값은 불순도(지니계수)가 낮아지는 방향으로 나눠준다!\n",
    "        (4) 불순도란 분류하려는 데이터 집합에서 서로 다른 클래가 섞여있는 정도이다.\n",
    "        (5) 실제 학습시에는 하이퍼 파라미터를 설정해주고, 시각화 후 가지치기를 해준다. \n",
    "        (6) 과적합을 줄이기 위해선 min_samples_leaf 개수를 제어하는 방법이 있다!\n",
    "    3. SVM ( Support Vector Machine )\n",
    "        (1) 클래스를 분류할 수 있는 다양한 경계선 중 최적의 라인을 찾아내는 알고리즘이다!\n",
    "        (2) 데이터가 명확하게 분류할 수 있을수록 성능이 좋게 나온다.\n",
    "        (3) 초평면이란 n차원을 나누는 n-1차원의 공간이다.\n",
    "        (4) SVM은 데이터로부터 가장 거리가 큰 경우를 선택해 최적의 선을 찾는다.\n",
    "        (5) 장점으로는 \n",
    "            1. 서포트벡터만을 이용해서 데이터가 적을때 효과적\n",
    "            2. 새로운 데이터가 입력되도 금방 분류 가능\n",
    "            3. 정확성이 좋고 비선형모델도 가능하며 오버피팅 가능성이 적다\n",
    "        (6) 단점으로는\n",
    "            1. 아무래도 데이터가 명확할수록 성능이 잘나오다보니 전처리 과정이 중요\n",
    "            2. 데이터가 많아질수록 속도가 느려진다.\n",
    "    4. KNN ( K-Nearest Neighbor )\n",
    "        (1) 유유상종을 가정! 즉 비슷한 특성을 가질수록 가까이 있다고 가정한다.\n",
    "        (2) 데이터로부터 거리가 가까운 k개의 다른 데이터 레이블을 참조하여 분류한다.\n",
    "        (3) 알고리즘 계산 순서 요약\n",
    "            1. 데이터준비 \n",
    "            2. K값 설정 :원하는 K값을 설정하는데 보통 홀수개로 설정한다\n",
    "            3. 거리 계산 : 새로운 데이터를 기존 모든 데이터간의 거리를 계산한다. 유클리드, 맨해튼 거리등을 사용한다.\n",
    "            4. 가장 가까운 K개의 이웃 선택 :계산결과중 가장 작은 거리를 선택하고 이들이 가장가까운 이웃이 된다.\n",
    "            5. 분류하기:K개의 이웃중 가장 많이 등장하는 클래스가 예측결과가 된다.\n",
    "        (4) 중요한 특징: 어떠한 학습이 필요하지 않다는 것. 별도의 모델없이 데이터만을 이용해 분류해낸다!\n",
    "        (5) 하지만 데이터 개수가 많을수록 계산량이 많아져 느려지고, K값 결정도 어렵다. \n",
    "    5. 앙상블\n",
    "        (1) 여러개의 개별 분류 모델들을 결합시켜 더 좋은 성능을 내게 한다. 여러 개의 약 분류기를 결합해 강 분류기로 만들어 내는 것!\n",
    "        (2) 종류\n",
    "            1. 보팅 : 다른 알고리즘을 병렬로\n",
    "            2. 배깅 : 동일 알고리즘을 병렬로\n",
    "            3. 부스팅 : 동일 알고리즘을 직렬로\n",
    "## 분류의 평가 지표 대표적 3가지\n",
    "    1. 혼동 행렬(컨퓨전 매트릭스)\n",
    "        (1) 분류 모델의 예측 결과를 정확한 예측과 잘못된 예측으로 구분하는 표 \n",
    "        (2) 가장중요한건 3가지다\n",
    "            1. Accuracy : 모든 가능한 예측 중 참인 비율\n",
    "            2. Precision: TN/(FN+TN) 으로 계산할 수 있다.\n",
    "            3. recall : FP/ (FP+TN) 으로 계산할 수 있다.\n",
    "        (3) p와r중 뭐가 더 중요한가? 상황마다 다르다!\n",
    "            1. precision이 더 중요한 경우 : 스팸메일 분류기\n",
    "            2. recall이 더 중요한경우 : 암 환자 판단\n",
    "        (4) P와 R은 trade-off가 있어서 둘다 올릴순 없고 상황에 맞게 하나를 선택해야한다. 임계값을 선택하는것도 좋은 방법\n",
    "    2. F1-Score\n",
    "        (1) 정밀도(Precision)와 재현율(Recall)의 조화평균\n",
    "    3. ROC/ AUC Curve\n",
    "        (1) Roc곡선\n",
    "            1. 얼마나 분류가 잘 되었는가를 보여준다\n",
    "            2. TPR이 1인 좌상단으로 붙을 수록 좋은 모델이다! \n",
    "            3. 하지만 완벽한 분류기는 나올 수 없으므로 타협한 ROC곡선을 도출해야한다\n",
    "        (2) AUC곡선\n",
    "            1. ROC와 X축 사이의 면적\n",
    "            2. 당연히 면적이 넓어질수록 분류성능이 좋은것으로 판단\n",
    "    4. 다중 분류 평가 지표\n",
    "        (1) 이제까진 모두 이진 분류 평가 지표였음.\n",
    "        (2) 다중 분류평가는 이진평가들을 사용해 클래스별 점수를 적절히 평균내는 것\n",
    "        (3) 종류\n",
    "            1. Macro average: 클래스별 지표 평균, 모든라벨이 유사한 중요도일 때\n",
    "            2. Weighted average:클래스별 지표 가중평균, 샘플이 많은 라벨에 중요도를 부여할 때\n",
    "            3. Micro average: 모든 클래스의 예측 결과 합, 라벨에 상관없이 전체를 보고플 때\n",
    "\n",
    "## 하이퍼파라미터 최적화와 그 방법 그리고 검증과 자동화\n",
    "    1. 하이퍼파라미터 최적화란?\n",
    "        (1) 하이퍼파라미터란 : 학습 시작 전에 사용자가 직접 설정하는 변수 ex) max_depth, n_estimators\n",
    "        (2) 최적화란 : 튜닝을 거쳐 적절한 하이퍼파라미터를 찾아 모델성능을 향상시키는 것\n",
    "        (3) 과정 : 파라미터 탐색 범위 설정 > 평가지표 계산 함수 정의 > 검증데이터로 정확도 평가 > 앞 단계를 반복후 정확도 재평가 및 파라미터 범위 제한\n",
    "    3. 하이퍼파라미터 최적화 검증\n",
    "        (1) Grid Search: 정해진 범위에서 모든 하이퍼파라미터를 순회\n",
    "        (2) Random Search: 정해진 범위에서 무작위로 하이퍼파라미터를탐색\n",
    "    4. 하이퍼파라미터 최적화 자동화"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
