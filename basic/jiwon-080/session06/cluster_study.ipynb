{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0abde096",
   "metadata": {},
   "source": [
    "> ### 1. 비지도학습이란?\n",
    "\n",
    "데이터에 정답라벨이 없고, 데이터 속 패턴과 데이터 간 유사도를 학습  \n",
    "정답라벨이 없기 때문에, (지도학습에서 쓰는) 테스트셋과 비교한 평가지표를 통한 평가는 어렵다.  \n",
    "\n",
    "- 비지도학습이 적합한 경우\n",
    "    - 패턴이 알려지지 않았거나 계속 변하는 경우  \n",
    "    - 패턴을 찾기보다 데이터 자체의 특성을 연구하고 일반화하는 문제\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66067e4",
   "metadata": {},
   "source": [
    "> ### 2. 군집화란?\n",
    "\n",
    "비슷한 특성을 가진 데이터를 군집(클러스터)으로 묶는 비지도학습  \n",
    "-> 각각 다른 군집 안의 데이터들은 비슷하지 않아야 함\n",
    "\n",
    "- 군집화의 목표  \n",
    "    - 응집도 최대화: 같은 군집 안의 데이터끼리의 유사한 정도를 최대화  \n",
    "    - 분리도 최대화: 각 군집들을 최대한 분리하도록 함\n",
    "\n",
    "- 군집화의 과정  \n",
    "    1. 군집화에 사용할 **피처의 선택, 추출**  \n",
    "    2. 데이터 특성, 목표에 맞는 **군집화 알고리즘** 선택  \n",
    "    3. 군집 유효성 검증  \n",
    "    4. 결과 해석: 군집들이 지닌 특성을 해석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943cf4b",
   "metadata": {},
   "source": [
    "> ### 데이터 준비 - 과정 1\n",
    "\n",
    "- 군집화의 주요 고려 사항  \n",
    "    1. 변수 유형 이해  \n",
    "    피처의 종류(연속형 or 범주형), 변수 개수와 같은 변수 특징을 파악  \n",
    "    2. 거리 or 유사도의 정의, 측정 방법에 대한 정의  \n",
    "    1에서 파악한 변수 특징에 따라 적합한 거리 측정 방법(유클리드, 맨해튼, 자카드 등)을 결정  \n",
    "    3. 차원 축소  \n",
    "    PCA에서 차원을 축소시킨 것처럼, 유사한 변수들을 묶어 처리하는 차원 축소를 고려해야 함  \n",
    "    유사한 변수 통합, 불필요한 변수 제거와 같은 전처리 과정  \n",
    "\n",
    "> 1. 변수 유형 이해  \n",
    "- 연속형  \n",
    "    - 적합한 거리: 유클라디안, 맨하탄 거리  \n",
    "    - 적합한 알고리즘: K-means, 계층적 군집화  \n",
    "    - **스케일링이 필수**(단위크기의 차이에 영향이 큼)  \n",
    "\n",
    "- 범주형  \n",
    "    - 적합한 거리: 해밍, 자카드 거리  \n",
    "    - 적합한 알고리즘: K-modes, 계층적 군집화(특별한 거리함수)  \n",
    "    - 범주형 처리는 OneHotEncoding이나 더미 변수를 생성하여 처리  \n",
    "\n",
    "![다양한거리측정](https://blog.kakaocdn.net/dna/7PyTd/btsy9eivA5l/AAAAAAAAAAAAAAAAAAAAAFBfqaQ-lE6AR9pVVMVgGLEUyYjjFm-Lb5TX9ou-nsia/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=7uaR3kqHB1VcAbOdeJl7xtwWD04%3D)  \n",
    "\n",
    "- 혼합형(연속형+명목형)  \n",
    "    - 적합한 알고리즘: K-prototypes, 계층적 군집화(Gower 거리 기반)  \n",
    "    - 각각 변수의 유형에 맞는 거리함수를 조합  \n",
    "\n",
    "> 2. 거리 or 유사도의 정의와 측정  \n",
    "- 유클라디안 거리  \n",
    "삼각형의 사선 변을 구하는 것처럼 구한다.  \n",
    "\n",
    "- 맨하탄 거리  \n",
    "거리를 지나갈 때 블럭의 수를 계산하며 지나가는 것처럼 구한다.  \n",
    "이상치에 비교적 강한 것이 특징  \n",
    "![거리그림](https://cdn.imweb.me/upload/S202101041a4e45576971e/a49d90fc8d690.png)  \n",
    "\n",
    "- 코사인 유사도  \n",
    "벡터 간의 각도를 측정 -> 텍스트데이터, 고차원 데이터에 활용  \n",
    "\n",
    "![코사인유사도1](https://blog.kakaocdn.net/dna/bz5z8H/btsHPNLPPpF/AAAAAAAAAAAAAAAAAAAAAKXY2JypjgFjlP79LgskkH-9BHkdejoAz1Y_upHk1xki/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=8gAkezgqsoWw2BBZsspA7G5swzc%3D)  \n",
    "![코사인유사도2](https://wikidocs.net/images/page/24603/%EC%BD%94%EC%82%AC%EC%9D%B8%EC%9C%A0%EC%82%AC%EB%8F%84.PNG)  \n",
    "\n",
    "> 2. +연결 방법의 정의  \n",
    "\n",
    "각 방법은 군집 간의 거리를 측정하는 기준이 달라, 다른 군집화 결과가 발생한다.  \n",
    "![연결법](https://velog.velcdn.com/images/dnddl9368/post/7204c381-f0c2-4b62-b590-0a7588510d08/image.png)  \n",
    "- 단일/최단 연결법(Single linkage)  \n",
    "    - 각 군집 안의 점을 골라 **가장 가까운 거리**가 되도록 측정  \n",
    "    - 고립된 군집을 찾는데 중점  \n",
    "    - 이상치에 약함 \n",
    "\n",
    "- 완전/최장 연결법(Complete linkage)  \n",
    "    - 각 군집 안의 점을 골라 **가장 먼 거리**가 되도록 측정\n",
    "    - 군집의 응집성에 중점  \n",
    "    - 이상치에 약함  \n",
    "\n",
    "- 평균 연결법(Average linkage)  \n",
    "    - 각 군집 안의 모든 점들에 대한 거리 평균을 구함  \n",
    "    - 이상치에는 강해지지만, 계산량이 지나치게 많아짐  \n",
    "\n",
    "- 중심 연결법(Centroid Method)  \n",
    "    - 군집의 중심점(데이터포인트들의 중간)을 측정하고, 이 중심점끼리의 거리를 측정  \n",
    "    - 중심점을 측정할때 계산량이 많아짐  \n",
    "    - 두 군집을 결합할 때 가중평균으로 새 군집의 평균을 구함\n",
    "    -> inversion 문제: 합친 후 군집과 다른 군집의 거리가 합치기 전보다 더 가까워질 수 있음  \n",
    "    -> 단일, 완전 연결법에서는 발생 x  \n",
    "\n",
    "- 중앙 연결법(Median)  \n",
    "    - 두 군집들의 포인트들의 중앙값으로 거리를 정의  \n",
    "    - 극단값, 이상치에 강해짐  \n",
    "    - 기하학적 구조를 파악하기는 어려움  \n",
    "\n",
    "- 와드 연결법(Ward's Procedure)  \n",
    "    - 군집을 병합할 때 SSE(오차제곱합)의 증가분이 최소인 것을 선택,  \n",
    "    즉, 군집 내 분산을 최소화시키는 방향으로 병합해감"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047350d",
   "metadata": {},
   "source": [
    "> 3. 차원의 축소  \n",
    "\n",
    "- 차원의 저주: 데이터가 고차원이 될수록(1차원->2차원->3차원->...)  \n",
    "피쳐 공간이 너무 커져서, 데이터 포인트들의 거리들이 다같이 멀어짐,  \n",
    "거리 기반 알고리즘의 분별력이 감소  \n",
    "![차원의저주](https://blog.kakaocdn.net/dna/6g8f5/btqv0yFthYE/AAAAAAAAAAAAAAAAAAAAAEc3sr1skzGHkfPhl_x-GIFC8LRECT7_MN76a5y6KAkt/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=7T5IWZzFgMmcZ2W31%2FJxWKG5oUc%3D)  \n",
    "-> **차원을 축소**해서 분석하는 것이 필요  \n",
    "\n",
    "- 차원 축소의 유형  \n",
    "    - 선형 투영: 고차원 공간->저차원 공간  \n",
    "    예) **주성분 분석(PCA)**, 특잇값 분해(SVD), 랜덤 투영  \n",
    "    - 매니폴드 학습(비선형 차원 축소): 유클리드 거리가 아닌, 데이터 포인트들 사이의 곡선 거리를 고려하며 학습    \n",
    "    예) t-SNE, UMAP, Isomap  \n",
    "\n",
    "- PCA  \n",
    "1. 고차원 데이터에서  \n",
    "2. 가장 정보량(분산)이 큰 방향을 찾음  \n",
    "3. 그 방향을 기준으로 차원을 줄임  \n",
    "이렇게 저차원 공간에 투영된 파생 성분 - 주성분  \n",
    "![pca](https://blog.skby.net/wp-content/uploads/2024/12/PCA_%EA%B0%9C%EB%85%90%EB%8F%84.png)  \n",
    "3차원을 2차원 평면으로 줄인 예시\n",
    "\n",
    "- 파이썬에서 PCA 사용\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=주성분개수, whiten=bool, random_state=고정값)\n",
    "```  \n",
    "n_components(주성분 개수): 정수(그 수만큼), 실수(그 비율의 분산을 설명하는 개수), None(모든 주성분 사용)  \n",
    "whiten(데이터 정규화): True(정규화), False(정규화 X)  \n",
    "<br>\n",
    ".fit(X): PCA 모델 학습  \n",
    ".transform(X): 주성분 공간으로 변환  \n",
    ".fit_transform(X): 학습+변환  \n",
    ".inverse_transform(X): 원본 공간으로 (근사해서)역반환  \n",
    "<br>\n",
    "explained_variance_ratio_: 각 주성분이 설명하는 분산 비율  \n",
    "explain_variance_: 각 주성분의 분산 자체  \n",
    "components_: **주성분 벡터**(원본 특성들의 가중합)  \n",
    "n_components_: 실제 선택된 주성분 개수\n",
    "\n",
    "- PCA의 주의점  \n",
    "표준화, 결측값 처리 필수  \n",
    "직관적 해석이 어려워짐  \n",
    "정보 손실 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe4ec3",
   "metadata": {},
   "source": [
    "> ### 군집화 알고리즘 - 과정 2\n",
    "\n",
    "> 1. 계층적 군집화  \n",
    "트리 구조(덴드로그램)을 형성하여, 상향 or 하향식으로 군집을 형성  \n",
    "\n",
    "- 덴드로그램의 수직축(높이): 가지들 사이의 유사성(실제 분포에서 가까이 있는 정도)  \n",
    "- 군집 개수를 사전 설정하지 않고 나중에 선택하는 방법으로 사용할 수 있음  \n",
    "\n",
    "- 덴드로그램의 종류  \n",
    "    - 상향식 - 응집형 계층적 군집화(병합 클러스터링)  \n",
    "    개별데이터들을 가지로 병합 -> 최종적으로 하나의 클러스터  \n",
    "    - 하향식 - 분리형 계층적 군집화(분할 클러스터링)  \n",
    "    전체 데이터에서 여러 가지로 뻗어나감  \n",
    "\n",
    "- 응집형 계층적 군집화  \n",
    "    - 모든 데이터 포인트를 각각 별개의 클러스터로 간주->점차 병합  \n",
    "    - 가장 가까운 클러스터의 기준을 정의  \n",
    "    - 단일 연결 or 완전 연결 방법  \n",
    "\n",
    "    - 과정  \n",
    "    1. 거리 행렬 계산  \n",
    "    2. 단일 클러스터로 초기화  \n",
    "    3. 가장 가까운 클러스터 병합하여 새 클러스터로 만듦  \n",
    "    4. 거리 행렬 업데이트  \n",
    "    5. 3~4를 반복  \n",
    "\n",
    "> 2. k-means  \n",
    "데이터를 정해진 개수의 군집(k개)으로 나누고,  \n",
    "각 그룹의 중심점과 거리가 가장 가까운 데이터 포인트들을 묶음  \n",
    "\n",
    "- 과정  \n",
    "1. 랜덤하게 k개의 중심점을 설정  \n",
    "2. 각 표본들을 가장 가까운 중심점에 할당  \n",
    "이때, 가장 가까운 거리를 계산할 때는 **유클라디안 거리의 제곱**을 사용  \n",
    "3. 각 클러스터에 할당된 데이터 포인트들의 평균 계산 -> 그 평균으로 중심점 이동  \n",
    "4. 2~3을 반복(할당이 변하지 않거나, 허용오차 또는 반복횟수까지)  \n",
    "\n",
    "- k-means의 주의점  \n",
    "    - 거리를 사용하기 때문에 스케일링이 중요!  \n",
    "    - 처음에 랜덤 설정되는 초기 중심점에 민감함  \n",
    "    - 군집 수의 결정에 유의(엘보우 방법, 실루엣 계수 등으로 판단 가능)  \n",
    "    - 이상치에 약함. 이상치가 중심을 왜곡할 수 있음  \n",
    "    - 기하학적 분포에 약함(클러스터를 원형으로 가정하기 때문에)\n",
    "\n",
    "- k-means++  \n",
    "가중치가 들어간\n",
    " 확률 분포를 통해, k-means에서 초기 중심점을 더 멀리 위치시켜서 k-means의 처음 중심점 배치문제를 해결  \n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=num, init='random' or 'k-means++', ..., random_state=42)\n",
    "#init 기본값이 ++긴 함.\n",
    "```\n",
    "\n",
    "- 엘보우 방법  \n",
    "![엘보우](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRC4yl81AacV5GuPM8940W5oFhWDOg8MZhdMbgqulIoqLGE1MvDqGk5wPOwz9QEfuC_oJY&usqp=CAU)  \n",
    "반복문으로 k를 증가시켜가며, 왜곡이 급격히 감소하는 지점을 k로 설정하는 방법  \n",
    "직관적이나, 주관이 개입하는 문제가 있다.\n",
    "\n",
    "> 3. DBSCAN  \n",
    "데이터 포인트들의 **밀집도**에 따라 군집화  \n",
    "- 밀집도 정의: 특정 거리 내에 존재해야 하는 데이터 포인트의 최소 개수\n",
    "\n",
    "- 특징  \n",
    "    - 이상치를 지정하여 제외할 수 있음-> 이상치에 강함  \n",
    "    - 어떤 데이터가 있으면 가장 밀도 높은 군집에 그룹화되는 특성  \n",
    "\n",
    "- 과정  \n",
    "    1. 샘플 분류\n",
    "    MinPts를 지정(이웃 샘플의 수)\n",
    "        - 핵심 샘플: 특정 반경 ε 안에 있는 이웃 샘플이 MinPts개 이상  \n",
    "        - 경계 샘플: MinPts보다 이웃이 적지만 다른 핵심 샘플의 반경 안에 있음  \n",
    "        - 잡음 샘플: 핵심, 경계에 해당되지 않음  \n",
    "    2. 클러스터 생성  \n",
    "    핵심 샘플, 또는 특정 반경 ε 안에 있는 핵심 샘플을 연결한 클러스터  \n",
    "    3. 경계 샘플을 2번의 클러스터에 할당\n",
    "    \n",
    "- 하이퍼파라미터  \n",
    "    - eps: 두 포인트가 이웃이 되기 위한 최대거리  \n",
    "    - min_samples: 한 포인트가 군집이 되기 위한, eps의 거리 내의 최대 포인트 개수  \n",
    "\n",
    "- 장점  \n",
    "    - k-means의 기하학적 분포에 약하다는 점을 해결  \n",
    "    - 이상치에 강함  \n",
    "    - 잡음을 구분해냄  \n",
    "- 단점  \n",
    "    - 파라미터 설정을 잘 해줘야 함  \n",
    "    - 연산량이 많음  \n",
    "    - 밀도가 약한 부분을 노이즈로 처리하는 경향  \n",
    "    - 차원의 저주의 영향을 받음  \n",
    "\n",
    "- HDBSCAN  \n",
    "DBSCAN을 계층적 군집화처럼 변환한 것  \n",
    "    - 과정  \n",
    "    1. 밀도 기반 군집화  \n",
    "    2. **거리 기준**으로 1번의 군집들을 반복적으로 연결  \n",
    "    - 파라미터  \n",
    "        - min_cluster_size: 클러스터 인정을 위한 최소 샘플 수  \n",
    "        - min_samples\n",
    "\n",
    "> 4. Gaussian Mixture Model(GMM)  \n",
    "데이터 분포를 여러가지 가우시안 분포로 생각하고 클러스터로 인식  \n",
    "M자 분포에서 각 언덕을 클러스터로 하는 느낌  \n",
    "\n",
    "- 특징  \n",
    "    - 모델 기반 군집화 방법  \n",
    "    - 정규성을 가정함, 각 군집은 확률 분포로 간주  \n",
    "\n",
    "- 과정  \n",
    "    1. 데이터 분포 확인  \n",
    "    2. 전체 데이터는 여러개의 정규분포라고 가정  \n",
    "    3. 2번에서 생각한 여러개의 정규분포를 추출-> 군집으로 삼음   \n",
    "    4. 개별 데이터가 3번의 정규분포들 중 어디에 속할지 결정  \n",
    "\n",
    "- 파라미터 추정  \n",
    "기댓값-최대화(EM) 알고리즘으로 파라미터를 추정함  \n",
    "-> 군집들의 중심, 모양, 크기, 방향  \n",
    "    - EM 알고리즘  \n",
    "    파라미터 설정 -> 특정 분포에 속할 사후확률 계산 -> 파라미터 추정 반복\n",
    "\n",
    "- 장점  \n",
    "    - k-means보다 유연하고, 기하학적 분포나 중첩 분포에도 강함  \n",
    "- 단점  \n",
    "    - 시간이 오래 걸림  \n",
    "    - 정규성 가정, 가우시안 가정에 어긋날 경우 계산 복잡, 성능 저하  \n",
    "\n",
    "```python\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, covariance_type = 'full', random_state=42)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aea97c",
   "metadata": {},
   "source": [
    "> ### 군집화 평가 - 과정 3\n",
    "\n",
    "- 외부 평가  \n",
    "사실은 정답 라벨이 존재했던 경우에 사용  \n",
    "군집화 결과와 정답 비교하여 유사도를 평가(ARI 지수 등)  \n",
    "\n",
    "- 내부 평가  \n",
    "정답 라벨이 없을 때 사용  \n",
    "**응집도**와 **분리도**를 기반으로 성능을 평가  \n",
    "<br>\n",
    "예) k-means에서는 SSE의 최소화를 기준으로 평가  \n",
    "GMM에는 로그 우도의 최대화를 기준으로 평가  \n",
    "범용적: 실루엣 계수, 던 지수  \n",
    "\n",
    "    - 실루엣 계수  \n",
    "![실루엣1](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbtxZrd%2Fbtq7nyWNazV%2FAAAAAAAAAAAAAAAAAAAAAAIwFUGGl3uDSJ3g7Vvoe-qwVDAKBkeV1kdjGDWOBRXz%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DZQnfkQ%252BTkG8Nv8gR%252BbaRBu5aOK0%253D)  \n",
    "![실루엣2](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2Fd8D4nr%2Fbtq7gq7njvX%2FAAAAAAAAAAAAAAAAAAAAANSxq5Ad6lc6KFWd-Gjuhr2RBSleK6sylyFP0iDYlKar%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DS8bIUQfsrkJao90HY%252FquqdPjPFs%253D)  \n",
    "![실루엣3](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbHdD6j%2Fbtq7jCloMOj%2FAAAAAAAAAAAAAAAAAAAAAHNlRtwphzr3Mk6Yei8J3zM0nsIeb98eG2JiIW9fPrW_%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DkbQaTYFp51igSgkoELeB3ETN2Hc%253D)  \n",
    "![실루엣4](https://blog.kakaocdn.net/dna/LBK27/btq7iQxH7iV/AAAAAAAAAAAAAAAAAAAAABye-RDKGYFIJ0QjrwvWO6v6x_CCE72Cou26yr_Z-MXO/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=SWnxjcCGkq7KV%2BTmWmTbR6aKHs4%3D)  \n",
    "-> 모든 데이터들의 실루엣 계수의 평균이 1에 가까울수록(분리도>응집도) 좋은 군집화  \n",
    "->  클러스터의 실루엣 계수의 평균이 1에 가까울수록 강한 군집  \n",
    "\n",
    "    - 던 지수  \n",
    "    클러스터 간 최소거리(분리도)와 클러스터 내 최대거리(응집도)의 비율로 군집화 평가  \n",
    "    분리도/응집도 이므로, 분리도가 높고 응집도가 작으면 던 지수는 크게 나타난다.\n",
    "    -> 클수록 좋은 군집화 결과,  \n",
    "    단 이상치에 약하므로 실루엣 지수와 복합적 사용해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de2ff2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
