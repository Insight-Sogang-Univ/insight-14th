{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d77184f",
   "metadata": {},
   "source": [
    "### 1. 머신러닝이란?\n",
    "\n",
    "- 직역하면 기계 학습으로, 데이터를 학습시켜서 패턴을 찾아내고 예측하는 방법\n",
    "    - 과거에는 알고리즘 전문가가 패턴을 만드는 일이 많았다면, 현재는 컴퓨팅파워의 향상과 데이터 양의 증가로 머신러닝을 사용하기 용이함.\n",
    "- 머신러닝의 종류  \n",
    "    **지도학습** : 데이터에 정답이 주어지는 학습  \n",
    "    예시) 고양이 강아지 사진 분류 문제에 사진별로 정답이 있는 것, 회귀분석에서 종속변수와 회귀값이 비슷해지도록 지도  \n",
    "\n",
    "    **비지도학습** : 데이터에 정답이 주어지지 않고, 패턴과 유사도를 학습  \n",
    "    예시) 고양이 강아지 사진 분류 문제에서 사진별로 정답이 없고, 기계가 고양이의 공통점 강아지의 공통점을 학습해서 분류  \n",
    "    ![지도학습비지도학습예시](.\\제목없음.png)  \n",
    "    (지금 아패가 없어서..)\n",
    "\n",
    "    **강화학습** : 여러 번 시행착오를 통해 상을 늘리고 벌을 줄이는 방법을 학습  \n",
    "    예시) 길찾기, 게임 봇  \n",
    "    ![https://marutitech.com/businesses-reinforcement-learning/](https://cdn.marutitech.com/RL_1_3932c27f5d.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2d6a45",
   "metadata": {},
   "source": [
    "### 2. 지도학습 - 회귀와 분류\n",
    "\n",
    "- 지도학습의 과정  \n",
    "    1. 테스트/트레인 셋 구분  \n",
    "    트레인 셋: 모델을 학습시킬 때 쓰는 데이터셋  \n",
    "    대략 0.6~0.8 정도를 사용, 과적합에 유의하며 설정  \n",
    "    테스트 셋: 학습에 포함시키지 않음, 학습시킨 모델을 평가시킬때 사용  \n",
    "    대략 0.2~0.4 정도를 사용  \n",
    "\n",
    "    2. 모델 만들기  \n",
    "    모델을 고르고, 파라미터를 설정하고, 트레인 셋으로 모델을 학습  \n",
    "\n",
    "    3. 모델 평가  \n",
    "    테스트 셋을 학습시킨 모델에 적용시켜서, 실제 테스트 셋의 정답과 모델이 낸 정답과 비교  \n",
    "    평가 지표(RMSE, R2 score, Confusion matrix를 통한 에러의 크기 등)를 통해서 평가  \n",
    "\n",
    "    4. 최종 모델\n",
    "\n",
    "- 회귀 지도학습  \n",
    "간단히 말하자면 **회귀식 Ax1+Bx2+...+=y 을 찾는 문제**  \n",
    "독립변수로부터 종속변수값이 도출되는 패턴(회귀식)을 찾는 문제\n",
    "\n",
    "- 분류 지도학습\n",
    "간단히 말하자면 종속변수가 0 또는 1인 문제  \n",
    "독립변수로부터 0인지 1인지 도출되는 패턴을 찾아서 분류하는 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9dc4bb",
   "metadata": {},
   "source": [
    "### 3. 선형회귀\n",
    "\n",
    "- 회귀식(Ax1+Bx2+...+=y)이 선형인 것  \n",
    "회귀식을 통해 현재 데이터를 요약하고, 미래 값을 예측할 수 있음\n",
    "\n",
    "- 선형회귀의 가정  \n",
    "1. 선형성: 종속변수와 독립변수가 선형적 관계  \n",
    "2. 독립성: 관측치끼리 영향이 없는 독립적인 관측치(시계열일 경우 자기상관에 유의)  \n",
    "3. 등분산성: 오차항의 분산이 일정  \n",
    "4. 정규성: 오차항이 정규분포를 따라야 함  \n",
    "\n",
    "- 단순선형회귀 (독립변수가 1개뿐인 선형회귀)  \n",
    "최적의 회귀식은 잔차의 제곱합이 가장 적은(데이터를 가장 잘 설명하는) Y=aX+b로 도출된다.  \n",
    "    - 오차와 잔차의 차이\n",
    "    오차: 실제값 - 실제 평균값의 차이  \n",
    "    잔차: 실제값 - 예측값의 차이\n",
    "\n",
    "    - 최소제곱법(LSM 또는 OLS 방법)  \n",
    "    잔차의 제곱 합(RSS)가 최소가 되는 회귀선을 구하는 방식  \n",
    "    ![https://novustat.com/statistik-blog/ols-regression-101.html](https://novustat.com/wp-content/uploads/2019/03/5-ols-regression-regression-line-and-residuals.png)  \n",
    "    사진에서 빨간선의 제곱합이 최소가 되면 된다.\n",
    "\n",
    "    - **경사하강법**?  \n",
    "    빨리 정상에 갈때는 경사가 가파르게 높아지는 부분을 찾고, 아래에 갈 때는 경사가 가파르게 낮아지는 부분을 찾는 방법?  \n",
    "    OLS를 통해 구한 함수가 복잡하여(닫힌 상태, 형태가 복잡, 데이터 양이 많음) 최소값을 구하기 어려울 때 주로 사용한다  \n",
    "    잔차제곱을 줄이려는 목적은 OLS와 같다.  \n",
    "      \n",
    "        - 목적함수(Objective func): 최적화를 통해 오차를 최소화하는 것이 목적인 함수  \n",
    "        - 손실함수(Loss func): 개별(한 개의) 데이터에 대한 오차(모델이 얼마나 틀렸는지)를 측정하는 함수  \n",
    "        예) MSE, RMSE, MAE(회귀)  Binary/Categorical Crossentropy(분류)  \n",
    "        - 비용함수(Cost func): 전체 데이터셋에서 평균적인 손실을 측정하는 함수  \n",
    "        대부분 손실함수를 평균 or 합산하여 사용 (예외: 강화학습, 생성 모델)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ba87d",
   "metadata": {},
   "source": [
    "-   \n",
    "    - 경사하강법의 손실함수  \n",
    "    -> 잔차 제곱의 합\n",
    "    E(W,b)=손실함수(y=Wx+b 에서 도출되는 잔차 제곱의 합)  \n",
    "    **손실함수 위에서 W와 b에 대해(이동시키면서) E가 변화하는 양상을 통해 E를 최소화하는 W와 b 찾기!**  \n",
    "    최솟값 도착하면 미분계수 0 -> 이동을 멈춘다\n",
    "    \n",
    "    - 경사하강법의 학습률(알파)\n",
    "    손실함수 위에서 이동시킬때 얼마나 이동시키는 정도  \n",
    "    U자형의 왼쪽에서 너무 많이 이동하면, 골짜기를 넘어서 오른쪽으로 가 있을 수 있다(최솟값에서 더 멀어진다)  \n",
    "    학습률이 너무 작다: 수렴할때까지가 오래 걸린다  \n",
    "    학습률이 너무 크다: 수렴이 안되고 점점 최솟값에서 멀어진다(발산한다)  \n",
    "    -> 적절한 학습률을 가지는게 중요\n",
    "\n",
    "    - Logical Minima 문제  \n",
    "    비용함수에 여러개의 골짜기가 있을때,  \n",
    "    한 골짜기 안에 갇혀서 최솟값이 있는 골짜기에 도달하지 못하는 현상  \n",
    "    학습률이 작을 때 발생하기 쉽다.  \n",
    "    - - 해결법: 모멘텀  \n",
    "    기존 경사하강법이 이전 기울기를 다 잊은 채로 이동  \n",
    "    -> 이전의 기울기와 이동방향을 참고하며 이동  \n",
    "    -> 골짜기와 언덕을 더 잘 이동\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404857a0",
   "metadata": {},
   "source": [
    "### 4. 다중회귀분석\n",
    "\n",
    "- 선형회귀지만, 독립변수의 수가 2개 이상인 것  \n",
    "선형회귀처럼 OLS를 사용가능, 3차원 공간이기 때문에 관측치와 평면 사이의 차이로 구함, 경사하강법도 사용 가능\n",
    "\n",
    "- **다중공선성**  \n",
    "독립변수들 간 상관관계가 있는 현상  \n",
    "예) 독립변수:어떤 식품의 소금사용량/설탕사용량/나트륨함량이 있다고 하면  \n",
    "소금사용량과 나트륨함량은 양의 상관관계가 있을 것이다. 두 가지를 같이 독립변수로 사용하면 다중공선성이 발생한다.  \n",
    "pairplot이나 heatmap으로 상관관계를 대략적으로 파악하자  \n",
    "또는 R2를 이용한 점수인 VIF(분산 팽창 인수)를 이용해 판단할 수 있다.  \n",
    "\n",
    "- 다중공선성 해결법 | 정확한 다중회귀를 위해선 반드시 해결해 주어야 한다  \n",
    "    - 특정 변수 제거\n",
    "    - 변수 변환: 합쳐서 새로운 컬럼으로 만들거나 라벨링하기  \n",
    "    - 규제 선형 모델 사용: 릿지, 라쏘, 엘라스틱넷 등  \n",
    "    - PCA(주성분분석): 데이터의 차원을 줄여서 간단화해나가는 과정  \n",
    "    소분류를 주성분으로 묶어서 대분류로 만든다는 느낌  \n",
    "    정규화(평균을 0으로 조정, 스케일링) -> 주성분 찾기 -> 주성분에 데이터 투영 -> 스케일링 복구(선택)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ed392",
   "metadata": {},
   "source": [
    "### 5. 규제선형모델\n",
    "\n",
    "- 과적합을 방지하기 위해 규제를 가하는 모델\n",
    "    - 과적합: 모델이 너무 트레인셋을 학습하여서 테스트셋+새로운 데이터에는 성능저하가 나타나는 것  \n",
    "    - 다중회귀에서 OLS 방법을 쓰면 과적합이 발생하기 쉽다. 회귀계수(Wx에서 W)가 커진다=특정 독립변수에 과하게 의존한다.  \n",
    "    -> 규제를 통해 W 크기를 감소시켜 과적합을 예방\n",
    "\n",
    "- L2 규제(릿지 회귀)  \n",
    "W의 제곱에 대해 패널티를 부여  \n",
    "계수에 따라 다른 패널티가 적용(제곱을 사용하기 때문에 큰 회귀계수일 경우 강한 패널티)  \n",
    "-> 예측 변수가 많을 때(고차원 데이터), 다중공선성일때 유용\n",
    "\n",
    "- L1 규제(라쏘 회귀)  \n",
    "W의 절댓값에 대해 패널티를 부여  \n",
    "절댓값을 사용하기 때문에, **각 계수들에게 주는 패널티가 같음**  \n",
    "-> 패널티보다 작은 계수는 0이 되기도 함(변수 선택 가능)  \n",
    "-> 변수 수가 많고 일부만 중요할 때, 해석을 간단화하고자 할 때 유용\n",
    "\n",
    "- L2+L1 규제(엘라스틱넷 회귀)  \n",
    "**L1의 변수 선택 기능**과 **L2의 큰 계수를 효과적으로 줄이는 기능**을 함께 활용  \n",
    "-> 변수 수가 많으면서 **다중공선성일 때** 유용  \n",
    "\n",
    "- 규제를 할 때는 스케일링이 꼭 필요(주로 MinMax 이용)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ece877",
   "metadata": {},
   "source": [
    "### 6. 회귀모델 평가하기\n",
    "\n",
    "- 평가 지표  \n",
    "MSE: 회귀 모델이 예측한 값과 실제 관측값 사이 **오차**의 **제곱**의 **평균**  \n",
    "RMSE: 직관성을 위해 MSE를 루트 한 값  \n",
    "MAE: 회귀 모델이 예측한 값과 실제 관측값 사이 **절댓값 오차**의 **평균**  \n",
    "\n",
    "    - MSE와 MAE 중 무엇을 쓸까?  \n",
    "    MSE를 사용한 모델은 제곱을 사용하기 때문에 이상치의 영향을 많이 받는 약점이 있다. 이 점을 고려하여 이상치가 있을 때 MAE 사용도 고려할 수 있다.\n",
    "\n",
    "- 변수 유의성 평가\n",
    "\n",
    "    - t검정  \n",
    "    귀무가설 H0: 회귀계수가 0이다 -> 종속변수에 영향을 주지 않는다  \n",
    "    대립가설 H1: 회귀계수가 0이 아니다 -> 종속변수에 영향을 준다  \n",
    "\n",
    "    1. 회귀계수별 t분포 기반 t value를 확인한다.  \n",
    "    2. 회귀계수별 p value를 확인한다. 0.05 아래면 H0을 기각한다(=해당 변수는 유의미하다.)  \n",
    "    3. 회귀계수가 0이면 그 독립변수는 모델에 영향을 주지 않는다.  \n",
    "    p-value가 작을수록 영향이 크다.  \n",
    "    t값이 크면 영향이 클 가능성이 높다.  \n",
    "    다음과 같이 변수들을 평가하면 된다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
