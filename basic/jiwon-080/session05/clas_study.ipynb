{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c1eb2e",
   "metadata": {},
   "source": [
    "> ### 1. 분류의 특징  \n",
    "\n",
    "> 지도학습의 일종  \n",
    "\n",
    "지도적(정답에 대한 라벨이 존재하고 학습시킴) 특성이 있다.  \n",
    "회귀는 연속형 변수와 회귀선을 예측한다면,  \n",
    "분류는 범주형 변수를 예측하는 지도학습\n",
    "\n",
    "> 이진 분류와 다중 분류  \n",
    "\n",
    "- 이진 분류: 종속변수가 참/거짓, 0/1인 것  \n",
    "- 다중 분류: 종속변수에 3개 이상의 범주형 변수가 있는 것\n",
    "\n",
    "> 대표적인 분류 모델\n",
    "\n",
    "- 로지스틱 회귀: 회귀 분석 기반으로, 이진 분류를 예측  \n",
    "- 결정 트리: 구분의 규칙을 만들어서 분류해 나가는 것  \n",
    "- 서포트 벡터 머신: 각각 클래스 간의 최대 마진을 찾아 분류해 나가는 것  \n",
    "- 최소 근접법: 데이터 간의 거리를 기준으로 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f1f63",
   "metadata": {},
   "source": [
    "> ### 2. 분류 모델의 종류\n",
    "\n",
    "1. 로지스틱 회귀  \n",
    "이진 분류의 값을 예측하는 회귀이고  \n",
    "**독립 변수의 선형 조합(선형회귀) -> 로지스틱 함수(시그모이드)를 적용 -> 출력값을 0과 1 사이로 변환**  \n",
    "하는 과정을 거친다.  \n",
    "![시그모이드함수](https://kjhov195.github.io/post_img/200107/image4.png)  \n",
    "- 입력값이 커지면 1에 수렴, 작아지면 0에 수렴, y의 범위는 0~1임\n",
    "\n",
    "커트라인으로 0/1이 문제에서, 커트라인 근처의 그래프는 급격하게 0에서 1로 증가할 것이다.  \n",
    "이 때 단순/다중 선형회귀처럼 직선의 회귀선을 그대로 사용한다면 정확하게 분류하기 어렵다.  \n",
    "따라서 중간값 근처에서 급격히 변하는 모양인 시그모이드 함수가 분류 문제에서는 설명을 더 잘 한다.\n",
    "\n",
    "- 시그모이드 함수의 가중치  \n",
    "w 가중치: 시그모이드 함수의 기울기(클수록 급격하게 오르는 모양, 작을수록 완만하게 오르는 모양)  \n",
    "b 가중치: 회귀선의 상수항처럼 클수록 높은 위치에 있는 모양\n",
    "\n",
    "- 오즈  \n",
    "$Odds=사건A가 일어날 확률/A가 일어나지 않을 확률$  \n",
    "오즈를 통해 비선형적 결과를 선형적으로 해석할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dbdb27",
   "metadata": {},
   "source": [
    "2. 의사결정 나무  \n",
    "Root Node: 가장 상위 노드  \n",
    "부모 노드/자식 노드: 부모 노드로부터 2개 이상의 자식 노드가 분리될 때  \n",
    "Binary Tree: 이진 트리, 자식 노드가 최대 2개인 트리  \n",
    "Leaf Nodes: 트리의 가장 아래라고 볼 수 있는 노드  \n",
    "Edge: 부모 노드와 자식 노드의 연결선  \n",
    "**Height**: 특정 노드에서 잎 노드까지 경로에 있는 edge 개수  \n",
    "**Depth**: 뿌리 노드에서 특정 노드에 도달하기 위한 edge 개수\n",
    "\n",
    "- CART(이진분할) 알고리즘  \n",
    ": 데이터셋을 임계값을 기준으로 두 child로 나누는 알고리즘  \n",
    "나누는 기준: 불순도(지니 계수)가 낮아지는가?  \n",
    "    - 불순도: 데이터 집합에 다른 범주가 섞여있는 정도  \n",
    "    - 지니 지수 계산 예시:  \n",
    "    (1) ★ ♧ ★ ☆ ★ ☆ ☆ ★ ♧ ★의 지니는?  \n",
    "    $Gini=1-(0.5^2+0.3^2+0.2^2)=0.62$  \n",
    "    (50% ★, 30% ☆, 20% ♧)  \n",
    "    (2) ★ ★ ★ ★ ★ ★ ★ ★ ★ ★의 지니는?  \n",
    "    $Gini=1-(1^2)=0$  \n",
    "    (3) ★ ★ ★ ♧ ★ ★ ☆ ★ ★ ★의 지니는?  \n",
    "    $Gini=1-(0.8^2+0.1^2+0.1^2)=0.34$  \n",
    "    (1) > (3) > (2) 순으로 불순도가 높다.\n",
    "\n",
    "- CART 알고리즘의 단계  \n",
    "    1. 임계값 설정  \n",
    "    부모 노드에서의 임계값을 기준으로 데이터를 그룹으로 나눈다.  \n",
    "    2. 불순도 감소 알고리즘  \n",
    "    불순도가 낮은 쪽으로 가지를 형성해나간다.  \n",
    "    한계) 그리디 알고리즘: 당장 분류할 때의 지니지수를 낮추는 것만 생각하기 때문에 전체적으로 효율적인 부분을 못 보는 경향  \n",
    "    3. 과적합 방지를 위한 조정  \n",
    "    3-1. 하이퍼파라미터 설정  \n",
    "    min_samples_spilt: 분할하기 위해 노드가 가져야하는 최소 데이터 수  \n",
    "    min_samples_leaf: 리프 노드가 가져야하는 최소 샘플 수  \n",
    "    min_weight_fraction_leaf: 가중치가 부여된, 전체 데이터 수에서 노드가 가져야 하는 데이터 비율  \n",
    "    max_leaf_nodes: 리프 노드의 최대 개수  \n",
    "    max_feature: 각 노드에서 분할에 사용할 특성의 최대 개수  \n",
    "    예) min_sample_split이 설정되어 있지 않다면, 너무 많은 분할이 생기고 그에 따라 과적합이 발생할 수 있다.  \n",
    "    3-2. 시각화  \n",
    "    시각화를 통해 트리를 확인해 보거나 규제 전후를 비교  \n",
    "    3-3. Prunning(가지치기)  \n",
    "    불필요한 노드와 그 노드 아래를 제거해 과적합을 방지\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be467bbe",
   "metadata": {},
   "source": [
    "3. SVM(서포트벡터머신)  \n",
    "\n",
    "데이터를 분류할 수 있는 **가장 좋은 경계선 or 경계면**을 찾아내는 알고리즘  \n",
    "-> 데이터를 분리하는 초평면 중, 데이터들과 가장 거리가 먼 초평면을 선택하여 분리하는  \n",
    "지도 학습, 이진 선형 분류 모델이다.  \n",
    "\n",
    "- Support Vector(서포트 벡터): 결정 경계와 가장 가까이 있는 데이터들의 집합  \n",
    "- Decision Boundary(결정 경계): 데이터를 분류하는 경계  \n",
    "- Margin(마진): 결정 경계<->서포트 벡터 의 거리  \n",
    "- Hyperplane(초평면): n차원 공간보다 1차원 낮은 n-1차원의 평면  \n",
    "- Slack Variables(슬랙 변수/여유변수): 완벽하게 분리가 불가할 때, 분류를 위해 허용된 오차에 대한 변수\n",
    "<br><br>\n",
    "- 가장 좋은 결정 경계는?  \n",
    "각각의 서포트 벡터와 결정 경계의 사이 거리(즉 마진)가 가장 큰 경우가 가장 좋은 결정 경계이다.  \n",
    "\n",
    "- SVM의 장점<->단점:  \n",
    "데이터가 적을 때 효과적 <-> 전처리를 잘 해서 데이터를 비교적 단순화시켜야 효과적  \n",
    "서포트 벡터(집합)와의 거리를 계산하기 때문에 연산량이 적음 <-> 데이터가 많아질수록 속도가 느려짐  \n",
    "정확성이 뛰어남, 비선형적 데이터 분류 가능  \n",
    "과적합이 잘 안 나타남, 노이즈 영향이 적음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b15022",
   "metadata": {},
   "source": [
    "4. KNN(K-Nearest Neighbor)\n",
    "\n",
    "데이터와 거리가 가까운 k개의 다른 데이터(이웃)를 참조하여 분류하는 알고리즘  \n",
    "\n",
    "- KNN 알고리즘의 단계  \n",
    "    1. 데이터 준비: 특징 벡터와 레이블(어떤 클래스인지)  \n",
    "    2. k값 설정: k개의 이웃을 참조할지, 보통 홀수로 설정(짝수면 동점 문제가 생길 수 있음)  \n",
    "    3. 거리 계산:\n",
    "    새 데이터가 주어짐 -> 이 데이터와 기존의 모든 데이터 간의 거리 계산  \n",
    "    ![거리계산](https://cdn.imweb.me/upload/S202101041a4e45576971e/a49d90fc8d690.png)  \n",
    "    4. 계산된 거리 중 가장 작은 거리인 k개의 데이터 선택  \n",
    "    5. 4번의 k개 이웃 둥 가장 많이 등장하는 클래스가 예측 결과\n",
    "\n",
    "- k의 선택법?  \n",
    "일반적으로는 학습 데이터의 제곱근으로 설정  \n",
    "너무 크면 과소적합 가능성, 너무 작으면 이상치나 잡음에 취약해진다.  \n",
    "\n",
    "- 5번 과정에서 동점이 발생한다면?  \n",
    "동점 클래스들 중 더 가까운 클래스로 분류  \n",
    "또는 그 중 무작위로 하나를 선택  \n",
    "\n",
    "- KNN의 장점:  \n",
    "범주의 기준을 알지 못해도 분류가 가능  \n",
    "기존 데이터를 통해 새 데이터를 즉석으로 예측하는 방식, 학습과정이 불필요, 데이터가 계속 추가될 때 유리  \n",
    "전체 데이터를 일반화한 모델로 만들지 않고 보존하는 방식  \n",
    "\n",
    "- KNN의 단점:  \n",
    "데이터가 늘어날수록 계산이 오래 걸림(모든 데이터 거리를 계산하기 때문)  \n",
    "K값의 결정이 어려움  \n",
    "범주형 데이터일 때 유사도를 정의하기 어려움  \n",
    "이상치, 잡음에 약함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa3ccd",
   "metadata": {},
   "source": [
    "5. 앙상블 기법  \n",
    "여러개의 분류 모델(약 분류기)들을 결합하여 강 분류기로 만드는 방법  \n",
    "예) 트리 기반의 앙상블 기법인 랜덤 포레스트, 그래디언트 부스팅, XGB  \n",
    "\n",
    "- 보팅: 여러가지 다른 알고리즘 모델, 병렬, 투표를 통해 모델을 선정  \n",
    "![보팅설명](https://blog.kakaocdn.net/dna/dyuTYx/btqZpUObch5/AAAAAAAAAAAAAAAAAAAAAKoJ9X4H0Fne2faGsvFUAisiFRAvECRsOy2BStqRWDu9/img.jpg?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=RkCUJRveFRyrNCjTHDsRElNYwkQ%3D)  \n",
    "- 배깅: 여러개의 트레인 셋을 만들고(붓스트랩 추출) 이들에 대해 동일 알고리즘 모델 사용, 병렬 (랜덤 포레스트 같은)  \n",
    "![배깅설명](https://blog.kakaocdn.net/dna/d2c03C/btqZqptGnmi/AAAAAAAAAAAAAAAAAAAAAJT_HACwK5doK8QBSkls--QypyvDP3Nd4DbfaJ_ktRj8/img.jpg?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=pKJkwQeKcJq6RXBpNLgPf2alqRM%3D)\n",
    "- 부스팅: 동일 알고리즘 모델, 직렬(순차적), 잘못 분류된 데이터에 가중치를 적용해서 개선된 모델을 만들어 나가는 방식  \n",
    "![부스팅설명](https://blog.kakaocdn.net/dna/vnVXB/btrNTf8ROQt/AAAAAAAAAAAAAAAAAAAAAG7R0qymY2_aNKMdIEPh01CHMCKMU7qygc4pd4u7LG5E/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=YQlwo3sQ8cDodEPNw2TyqQuXdTc%3D)  \n",
    "(https://bigdaheta.tistory.com/32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e153692",
   "metadata": {},
   "source": [
    "> ### 3. 분류 평가 지표  \n",
    "\n",
    "1. 혼동 행렬(Confusion Matrix)  \n",
    "\n",
    "|              | Pred True | Pred False |\n",
    "|--------------|-----------|------------|\n",
    "| Actual True  | TP        | FN(Type 2 Error) |\n",
    "| Actual False | FP(Type 1 Error)| TN         |\n",
    "\n",
    "- 혼동 행렬을 이용한 평가 지표  \n",
    "    - 정확도: 정확하게 예측하는 정도  \n",
    "    (TP+TN)/(TP+TN+FP+FN)  \n",
    "    단점은 불균형 데이터일 경우 신뢰성이 떨어진다.\n",
    "\n",
    "    - 정밀도: TP/(TP+FP)  \n",
    "    - 재현도: TP/(TP+FN)  \n",
    "\n",
    "    정밀도와 재현도는 상충관계에 있음(FP<->FN)  \n",
    "    정밀도는 타입1 에러와 관련, 재현도는 타입2 에러와 관련.  \n",
    "    (타입1과 2 에러를 동시에 줄이기 어렵다는 것을 생각하면)  \n",
    "    어떤 에러를 더 줄일까를 생각하며 Threshold(쉽게는 참거짓을 판단하는 기준치)를 조정할 수 있음.  \n",
    "    -> 임계값 별 정밀도와 재현도 그래프에서, 만나는 부분의 Threshold를 사용\n",
    "\n",
    "2. F1 score  \n",
    "정밀도와 재현도의 조화 평균  \n",
    "-> 정확도의 신뢰성이 떨어질 경우 대안으로 사용 가능\n",
    "\n",
    "3. ROC / AUC curve  \n",
    "x축: FP의 비율  \n",
    "y축: TP의 비율로 그린 그래프,  \n",
    "좌상단 끝에 가까울 수록(그래프 면적이 1에 가까울 수록) 좋은 성능이라고 평가한다.  \n",
    "\n",
    "4. 다중 분류 평가 기준  \n",
    "1~3은 모두 이진 분류 평가 기준이다.  \n",
    "- Macro: 클래스별로 구한 평가 지표 평균, 클래스별로 같은 가중치  \n",
    "- Weighted: 클래스별로 구한 평가 지표 가중 평균, 빈도가 큰 클래스에 높은 가중치  \n",
    "- Micro: 모든 클래스의 예측결과를 더함, 클래스별로 같은 가중치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a3ec9",
   "metadata": {},
   "source": [
    "> ### 4. 하이퍼파라미터 최적화(튜닝)  \n",
    "\n",
    "기계가 적절한 하이퍼파라미터를 찾아 성능을 향상시키는 것  \n",
    "\n",
    "- 과정  \n",
    "탐색 범위 설정 -> 성능 평가용 함수를 정의 -> 정확도 평가 -> 반복하며 하이퍼파라미터의 범위를 좁혀나감  \n",
    "\n",
    "- 방법  \n",
    "    - Grid Search: 사용자가 정한 범위 안에서 하이퍼파라미터를 모두 순회  \n",
    "    성능은 좋지만 계산시간이 오래 걸린다.    \n",
    "    - Random Search: 정한 범위 안에서 하이퍼파라미터를 무작위로 탐색  \n",
    "    계산속도는 빠르지만 정확도가 떨어진다.  \n",
    "    - Bayesian Optimization: 초기 값 몇 개를 무작위로 실험 후 -> 그 결과를 통해 파라미터의 조합을 확률 모델로 예측하고 -> 가장 좋은 위치에서 다시 실험하는 과정을 반복\n",
    "\n",
    "    사이킷런에서 파라미터 범위를 바꿔가면서 테스트 하는 방법도 있지만  \n",
    "    Optuna 라이브러리로 자동화하고 교차 검증하는 방법도 있다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
